\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}


\input{../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfic}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopficsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfaml}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Advanced Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}


\newcommand{\kopfdive}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Deep Dive\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #1}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}: #1}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}[1]
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}: #1}\\\noindent}
	{\bigskip}

\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-svm.tex}
\input{../../latex-math/ml-gp.tex}

\kopfaml{7}{Gaussian Processes}



\loesung{Bayesian Linear Model}{


%
The posterior distribution is obtained by Bayes' rule 
%
$$
\underbrace{p(\thetab | \Xmat, \yv)}_{\text{posterior}} = \frac{\overbrace{p(\yv | \Xmat, \thetab)}^{\text{likelihood}}\overbrace{q(\thetab)}^{\text{prior}}}{\underbrace{p(\yv|\Xmat)}_{\text{marginal}}}. 
$$
% 
In the Bayesian linear model we have a Gaussian likelihood: $\yv ~|~ \Xmat, \thetab \sim \mathcal{N}(\Xmat \thetab, \sigma^2 \id_n),$ i.e.,
%
\begin{align*}
%	
	p(\yv | \Xmat, \thetab) 
%	
	&\propto	\exp\biggl[-\frac{1}{2\sigma^2}(\yv - \Xmat\thetab)^\top(\yv - \Xmat\thetab)\biggr] \\
%	
	&= \exp\biggl[-\frac{\| \yv - \Xmat\thetab \|_2^2}{2\sigma^2}\biggr] \\
%	
	&= \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2}\biggr].
%	
\end{align*}
%
Moreover, note that the maximum a posteriori estimate of $\thetab,$ which is defined by
%
$$		\thetabh = \argmax_{\thetab} p(\thetab | \Xmat, \yv)	$$
%
can also be defined by 
%
%
$$		\thetabh = \argmax_{\thetab} \log \left( p(\thetab | \Xmat, \yv) \right),	$$
%
since $\log$ is a monotonically increasing function, so the maximizer is the same.
%
\begin{enumerate}
%	
  \item  If the prior distribution is  a uniform distribution over the parameter vectors $\thetab$, i.e.,
%  
	$$  q(\thetab)  \propto 1, $$
%	
	then 
%	
	\begin{eqnarray*}
		p(\thetab | \Xmat, \yv) &\propto& p(\yv | \Xmat, \thetab) q(\thetab) \\
%		
		&\propto& \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2}\biggr].
%		
	\end{eqnarray*}
%  
	With this, 
%	
	\begin{align*}
%				
		\thetabh 
%		
		&= \argmax_{\thetab} \log \left( p(\thetab | \Xmat, \yv) \right) \\
%		
		&= \argmax_{\thetab} -\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} \\
%		
		&= \argmin_{\thetab}  \frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} \\
%		
		&= \argmin_{\thetab}  \sum_{i=1}^n (\yi - \thetab^\top \xi)^2, \tag{$2\sigma^2$ is just a constant scaling}
%		
	\end{align*}
%
	so the  maximum a posteriori estimate coincides with the empirical risk minimizer for the L2-loss (over the linear models).
%
  \item If we choose a Gaussian distribution over the parameter vectors $\thetab$ as the prior belief, i.e.,
  %  
  $$  q(\thetab)  \propto  \exp\biggl[ -\frac{1}{2\tau^2}\thetab^\top\thetab  \biggr], \qquad \tau>0, $$
  %  
  	then 
%  
  \begin{eqnarray*}
%  	
  	p(\thetab | \Xmat, \yv) &\propto& p(\yv | \Xmat, \thetab) q(\thetab) \\
  	%		
  	&\propto& \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{1}{2\tau^2}\thetab^\top\thetab \biggr] \\
%  	
	&=& \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{\|\thetab\|_2^2}{2\tau^2}  \biggr]
  	%		
  \end{eqnarray*}
  %  
  With this, 
  %	
  \begin{align*}
  	%				
  	\thetabh 
  	%		
  	&= \argmax_{\thetab} \log \left( p(\thetab | \Xmat, \yv) \right) \\
  	%		
  	&= \argmax_{\thetab} -\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{\|\thetab\|_2^2}{2\tau^2}   \\
  	%		
  	&= \argmin_{\thetab}  \frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} + \frac{\|\thetab\|_2^2}{2\tau^2}   \\
%  	
  	&= \argmin_{\thetab}   \sum_{i=1}^n (\yi - \thetab^\top \xi)^2  + \frac{\sigma^2}{ \tau^2}  \|\thetab\|_2^2   ,
  	%		
  \end{align*}
  %
  so the  maximum a posteriori estimate coincides for the choice of $\lambda = \frac{\sigma^2}{ \tau^2}>0$ with the regularized empirical risk minimizer for the L2-loss with L2 penalty (over the linear models), i.e., the Ridge regression.
%  
  \item If we choose a Laplace distribution over the parameter vectors $\thetab$ as the prior belief, i.e.,
%  
	$$  q(\thetab)  \propto  \exp\biggl[-\frac{\sum_{i=1}^p |\thetab_i|}{\tau} \biggr], \qquad \tau>0, $$
	%  
	  	then 
%	
	\begin{eqnarray*}
		%  	
		p(\thetab | \Xmat, \yv) &\propto& p(\yv | \Xmat, \thetab) q(\thetab) \\
		%		
		&\propto& \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{\sum_{i=1}^p |\thetab_i|}{\tau}  \biggr] \\
		%  	
		&=& \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{\|\thetab\|_1}{\tau}  \biggr]
		%		
	\end{eqnarray*}
	%  
	With this, 
	%	
	\begin{align*}
		%				
		\thetabh 
		%		
		&= \argmax_{\thetab} \log \left( p(\thetab | \Xmat, \yv) \right) \\
		%		
		&= \argmax_{\thetab} -\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{\|\thetab\|_1}{\tau}   \\
		%		
		&= \argmin_{\thetab}  \frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} + \frac{\|\thetab\|_1}{\tau}  \\
		%  	
		&= \argmin_{\thetab}   \sum_{i=1}^n (\yi - \thetab^\top \xi)^2  + \frac{2\sigma^2}{ \tau}  \|\thetab\|_1   ,
		%		
	\end{align*}
	%
	so the  maximum a posteriori estimate coincides for the specific choice of $\lambda = \frac{2\sigma^2}{ \tau}$ with the regularized empirical risk minimizer for the L2-loss with L1 penalty (over the linear models), i.e., the Lasso regression.
%	
\end{enumerate}
%
}

\loesung{Gaussian Posterior Process}{


\begin{enumerate}
\item Prior distribution (assuming the same notation as in the lecture): $$\bm{f} \sim \mathcal{N}(\bm{m}, \bm{K})$$ with $\bm{m} = m(\bm{x})$ and $\bm{K}$ defined by the entries $\bm{K}_{ij} = k(x_i,x_j)$. NB: Note the (in-)finite Gaussian property of a GP. 
\item Note that the posterior distribution $\bm{f}|\bm{y},\bm{x}$ in this case is different from the one of $\bm{f}_* | \bm{x}_*, \bm{x}, \bm{y}$ and also from the marginal distribution of $\bm{y} \sim \mathcal{N}(\bm{m},\bm{K} + \sigma^2 \bm{I})$! We have: 
\begin{equation}
\begin{split}
p(\bm{f}|\bm{y}) &\propto p(\bm{y}|\bm{f}) \cdot p(\bm{f}) \\
&\propto \exp(-\frac{1}{2} (\bm{y}-\bm{f})^\top (\sigma^{2} \bm{I})^{-1}(\bm{y}-\bm{f})) \cdot \exp(-\frac{1}{2}(\bm{f}-\bm{m})^\top \bm{K}^{-1} (\bm{f}-\bm{m}))\\
&\propto \exp(-\frac{1}{2} \{ \bm{f}^\top \underbrace{((\sigma^{2} \bm{I})^{-1} + \bm{K}^{-1})}_{=: \bm{K}^{-1}_{post}} \bm{f} -2 \bm{f}^\top \underbrace{((\sigma^{2} \bm{I})^{-1} \bm{y} + \bm{K}^{-1} \bm{m})}_{=: \tilde{\bm{f}}} \})\\
&\propto \exp(-\frac{1}{2} \{ \bm{f}^\top \bm{K}^{-1}_{post} \bm{f} - 2 \bm{f}^\top\tilde{\bm{f}}  \})
\end{split}
\end{equation}
by removing all constant factors that do not depend on $\bm{f}$ as we only need to know the density up to a constant of proportionality. By extending the proportionality, we can get a quadratic form in $\bm{f}$:
\begin{equation}
\begin{split}
p(\bm{f}|\bm{y}) &\propto \exp(-\frac{1}{2} \{ \bm{f}^\top \bm{K}^{-1}_{post} \bm{f} - 2 \bm{f}^\top\tilde{\bm{f}}  \})\\
&\propto \exp(-\frac{1}{2} \{ \bm{f}^\top \bm{K}^{-1}_{post} \bm{f} - 2 \bm{f}^\top \bm{K}^{-1}_{post} \underbrace{\bm{K}_{post} \tilde{\bm{f}}}_{:=\bm{f}_{post}}  \}) \\
&\propto \exp(-\frac{1}{2}  (\bm{f}-\bm{f}_{post})^\top \bm{K}^{-1}_{post} (\bm{f}-\bm{f}_{post}))
\end{split}
\end{equation}
which is the so-called \emph{kernel} of a multivariate normal distribution $\mathcal{N}(\bm{f}_{post},\bm{K}_{post} )$, i.e., $\bm{f}|\bm{y} \sim \mathcal{N}(\bm{f}_{post},\bm{K}_{post} )$. 
\item In order to get the posterior predictive distribution for a new sample $x_*$ from the same data-generating process, we could derive $$p(y_* | x_*, \bm{y}, \bm{x}) = \int p(y_*|x_*, \bm{x}, \bm{y}, \bm{f}) \cdot p(\bm{f}|\bm{y},\bm{x}) \,d\bm{f}.$$ This is feasible but cumbersome. Alternatively, we can make use of the fact that the joint distribution of $\bm{y}$ and $y_*$ is known (cf. slides on noisy GP): 
$$\begin{pmatrix} \bm{y} \\ y_* \end{pmatrix} \sim \mathcal{N}\left( \begin{pmatrix} \bm{m} \\ m_* \end{pmatrix}, 
\begin{pmatrix} 
\bm{K} + \sigma^2 \bm{I} & \bm{K}_*\\
\bm{K}^\top_* & K_{**}\\
\end{pmatrix}
\right),$$
with $m_* = m(x_*)$, $\bm{K}_* = k(x_*, \bm{x})$ and $K_{**} = k(x_*,x_*)$.
The conditional distribution can then be derived using the rule of conditioning for Gaussian distributions: $$y_* | x_*, \bm{x}, \bm{y} \sim \mathcal{N}(m_* + \bm{K}^\top_* (\bm{K} + \sigma^2 \bm{I})^{-1}(\bm{y}-\bm{m}), K_{**} - \bm{K}^\top_* (\bm{K} + \sigma^2 \bm{I})^{-1} \bm{K}_*).$$
\item To implement a GP with squared exponential kernel and $\ls = 1$, we need the inverse of $\bm{K}$. $\bm{x}$ being a vector implies that we have only one feature and thus the entries of our matrix $\bm{K}$ are 
$$
\bm{K} = \begin{pmatrix} 1 & \exp(-0.5 (x^{(1)} - x^{(2)})^2) \\ \exp(-0.5 (x^{(2)} - x^{(1)})^2) & 1 \end{pmatrix}.
$$
The inverse of $\bm{K}$ is then given by $$
\frac{1}{1-\exp(-(x^{(1)} - x^{(2)})^2)} \begin{pmatrix} 1 & -\exp(-0.5 (x^{(1)} - x^{(2)})^2) \\ -\exp(-0.5 (x^{(2)} - x^{(1)})^2) & 1 \end{pmatrix}.
$$
If we have a noisy GP, we would have to add $\sigma^2 \bm{I}_2$ to $\bm{K}$ with resulting inverse 

$$
\bm{K}_y^{-1} = \frac{1}{(1+\sigma^2)^2-\exp(-(x^{(1)} - x^{(2)})^2)} \begin{pmatrix} 1+\sigma^2 & -\exp(-0.5 (x^{(1)} - x^{(2)})^2) \\ -\exp(-0.5 (x^{(2)} - x^{(1)})^2) & 1+\sigma^2 \end{pmatrix}.
$$
Assuming a zero mean GP, we can derive $\frac{\partial \bm{K}_y}{\partial \theta}$ with $\theta = \sigma^2$, which gives us the identity matrix. We can thus maximize the marginal likelihood (slide on \emph{Gaussian Process Training}), by finding $\sigma^2$ that yields 
$$\text{tr}\left( \bm{K}_y^{-1} \bm{y} \bm{y}^\top \bm{K}_y^{-1} - \bm{K}_y^{-1} \right) = 0.$$
This can be solved analytically (though quite tedious). We will use a root-finding function for this. For the posterior predictive distribution we can make use of the results from the previous exercise.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(kernlab)}

\hlcom{# set seed, define n, true (unknown) sigma}
\hlkwd{set.seed}\hlstd{(}\hlnum{4212}\hlstd{)}
\hlstd{n} \hlkwb{<-} \hlnum{2}
\hlstd{sigma} \hlkwb{<-} \hlnum{1}

\hlcom{# define kernel with l = 1}
\hlstd{kernel_fun} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)}
  \hlkwd{kernelMatrix}\hlstd{(}\hlkwc{kernel} \hlstd{=} \hlkwd{rbfdot}\hlstd{(}\hlkwc{sigma} \hlstd{=} \hlnum{1}\hlopt{/}\hlnum{2}\hlstd{),}
               \hlkwc{x} \hlstd{= x)}
\hlstd{kernel_fun_pred} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,}\hlkwc{y}\hlstd{)}
  \hlkwd{kernelMatrix}\hlstd{(}\hlkwc{kernel} \hlstd{=} \hlkwd{rbfdot}\hlstd{(}\hlkwc{sigma} \hlstd{=} \hlnum{1}\hlopt{/}\hlnum{2}\hlstd{),}
               \hlkwc{x} \hlstd{= x,} \hlkwc{y} \hlstd{= y)}

\hlcom{# draw data according to the generating process:}
\hlstd{x} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(n)}
\hlstd{K} \hlkwb{<-} \hlkwd{kernel_fun}\hlstd{(x)}
\hlstd{K_y} \hlkwb{<-} \hlstd{K} \hlopt{+} \hlkwd{diag}\hlstd{(}\hlkwd{rep}\hlstd{(sigma}\hlopt{^}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{))}
\hlstd{(y} \hlkwb{<-} \hlkwd{t}\hlstd{(mvtnorm}\hlopt{::}\hlkwd{rmvnorm}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwc{sigma} \hlstd{= K_y)))}
\end{alltt}
\begin{verbatim}
##          [,1]
## [1,] 2.012317
## [2,] 1.866819
\end{verbatim}
\begin{alltt}
\hlcom{# function to find the best sigma^2}
\hlstd{root_fun} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{sigmaSq}\hlstd{)\{}
  \hlstd{K_y_inv} \hlkwb{<-} \hlkwd{solve}\hlstd{(K} \hlopt{+} \hlkwd{diag}\hlstd{(}\hlkwd{rep}\hlstd{(sigmaSq,}\hlnum{2}\hlstd{)))}
  \hlnum{0.5}\hlopt{*}\hlkwd{sum}\hlstd{(}\hlkwd{diag}\hlstd{(K_y_inv}\hlopt{%*%}\hlstd{y}\hlopt{%*%}\hlkwd{t}\hlstd{(y)}\hlopt{%*%}\hlstd{K_y_inv} \hlopt{-} \hlstd{K_y_inv))}
\hlstd{\}}

\hlcom{# get the best sigma}
\hlstd{(bestSigmaSq} \hlkwb{<-} \hlkwd{uniroot}\hlstd{(}\hlkwc{f} \hlstd{= root_fun,} \hlkwc{interval} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{20}\hlstd{)))}\hlopt{$}\hlstd{root}
\end{alltt}
\begin{verbatim}
## [1] 1.943684
\end{verbatim}
\begin{alltt}
\hlcom{# plot the optimization problem and best sigma}
\hlstd{possible_sigvals} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{0.001}\hlstd{,}\hlnum{20}\hlstd{,}\hlkwc{l}\hlstd{=}\hlnum{1000}\hlstd{)}
\hlkwd{plot}\hlstd{(possible_sigvals,} \hlkwd{sapply}\hlstd{(possible_sigvals, root_fun),}
     \hlkwc{xlab} \hlstd{=} \hlkwd{expression}\hlstd{(sigma}\hlopt{^}\hlnum{2}\hlstd{),} \hlkwc{ylab} \hlstd{=} \hlstr{"marginal likelihood derivative"}\hlstd{,}
     \hlkwc{pch} \hlstd{=} \hlnum{20}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwc{h}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{lty}\hlstd{=}\hlnum{2}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwc{v}\hlstd{=bestSigmaSq}\hlopt{$}\hlstd{root,} \hlkwc{lty}\hlstd{=}\hlnum{2}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-3-1} 
\begin{kframe}\begin{alltt}
\hlcom{# function to draw samples from the predictive posterior}
\hlstd{draw_from_pred_posterior} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{number_samples}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{x}\hlstd{,} \hlkwc{xstar}\hlstd{,} \hlkwc{sigmaSq} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{\{}

  \hlcom{# invert noisy K}
  \hlstd{K_y_inv} \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{kernel_fun}\hlstd{(x)} \hlopt{+} \hlkwd{diag}\hlstd{(}\hlkwd{rep}\hlstd{(sigmaSq,}\hlnum{2}\hlstd{)))}
  \hlcom{# get the other K's for new data}
  \hlstd{Kstar} \hlkwb{<-} \hlkwd{kernel_fun_pred}\hlstd{(x,xstar)}
  \hlstd{Kstarstar} \hlkwb{<-} \hlkwd{kernel_fun}\hlstd{(xstar)}
  \hlcom{# draw samples according to Ex. (d)}
  \hlkwd{rnorm}\hlstd{(number_samples,}
        \hlkwc{mean} \hlstd{=} \hlkwd{as.numeric}\hlstd{(}\hlkwd{t}\hlstd{(Kstar)} \hlopt{%*%} \hlstd{K_y_inv} \hlopt{%*%} \hlstd{y),}
        \hlkwc{sd} \hlstd{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{as.numeric}\hlstd{(Kstarstar} \hlopt{-} \hlkwd{t}\hlstd{(Kstar)} \hlopt{%*%} \hlstd{K_y_inv} \hlopt{%*%} \hlstd{Kstar))}
  \hlstd{)}

\hlstd{\}}

\hlcom{# draw enough samples to get a feeling for the distribution}
\hlstd{samples_posterior} \hlkwb{<-}
       \hlkwd{draw_from_pred_posterior}\hlstd{(}\hlkwc{number_samples} \hlstd{=} \hlnum{1000}\hlstd{,} \hlkwc{sigmaSq} \hlstd{= bestSigmaSq}\hlopt{$}\hlstd{root,}
                                \hlkwc{y} \hlstd{= y,} \hlkwc{x} \hlstd{= x,} \hlkwc{xstar} \hlstd{=} \hlnum{0}\hlstd{)}
\hlcom{# plot the distribution}
\hlkwd{hist}\hlstd{(samples_posterior,} \hlkwc{breaks}\hlstd{=}\hlnum{50}\hlstd{,} \hlkwc{xlab}\hlstd{=}\hlkwd{expression}\hlstd{(y[}\hlstr{"*"}\hlstd{]}\hlopt{^}\hlstd{b))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-3-2} 
\end{knitrout}

\end{enumerate}
}
\end{document}
