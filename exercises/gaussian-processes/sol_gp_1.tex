\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}


\input{../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfic}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopficsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfaml}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Advanced Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}


\newcommand{\kopfdive}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Deep Dive\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #1}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}: #1}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}[1]
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}: #1}\\\noindent}
	{\bigskip}

\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-svm.tex}
\input{../../latex-math/ml-gp.tex}
\newcommand{\xvp}{\mathbf{x}^\prime}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\dv}{\mathbf{d}}

\kopfaml{7}{Gaussian Processes}



\loesung{Bayesian Linear Model}{


%
The posterior distribution is obtained by Bayes' rule 
%
$$
\underbrace{p(\thetab | \Xmat, \yv)}_{\text{posterior}} = \frac{\overbrace{p(\yv | \Xmat, \thetab)}^{\text{likelihood}}\overbrace{q(\thetab)}^{\text{prior}}}{\underbrace{p(\yv|\Xmat)}_{\text{marginal}}}. 
$$
% 
In the Bayesian linear model we have a Gaussian likelihood: $\yv ~|~ \Xmat, \thetab \sim \mathcal{N}(\Xmat \thetab, \sigma^2 \id_n),$ i.e.,
%
\begin{align*}
%	
	p(\yv | \Xmat, \thetab) 
%	
	&\propto	\exp\biggl[-\frac{1}{2\sigma^2}(\yv - \Xmat\thetab)^\top(\yv - \Xmat\thetab)\biggr] \\
%	
	&= \exp\biggl[-\frac{\| \yv - \Xmat\thetab \|_2^2}{2\sigma^2}\biggr] \\
%	
	&= \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2}\biggr].
%	
\end{align*}
%
Moreover, note that the maximum a posteriori estimate of $\thetab,$ which is defined by
%
$$		\thetabh = \argmax_{\thetab} p(\thetab | \Xmat, \yv)	$$
%
can also be defined by 
%
%
$$		\thetabh = \argmax_{\thetab} \log \left( p(\thetab | \Xmat, \yv) \right),	$$
%
since $\log$ is a monotonically increasing function, so the maximizer is the same.
%
\begin{enumerate}
%	
  \item  If the prior distribution is  a uniform distribution over the parameter vectors $\thetab$, i.e.,
%  
	$$  q(\thetab)  \propto 1, $$
%	
	then 
%	
	\begin{eqnarray*}
		p(\thetab | \Xmat, \yv) &\propto& p(\yv | \Xmat, \thetab) q(\thetab) \\
%		
		&\propto& \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2}\biggr].
%		
	\end{eqnarray*}
%  
	With this, 
%	
	\begin{align*}
%				
		\thetabh 
%		
		&= \argmax_{\thetab} \log \left( p(\thetab | \Xmat, \yv) \right) \\
%		
		&= \argmax_{\thetab} -\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} \\
%		
		&= \argmin_{\thetab}  \frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} \\
%		
		&= \argmin_{\thetab}  \sum_{i=1}^n (\yi - \thetab^\top \xi)^2, \tag{$2\sigma^2$ is just a constant scaling}
%		
	\end{align*}
%
	so the  maximum a posteriori estimate coincides with the empirical risk minimizer for the L2-loss (over the linear models).
%
  \item If we choose a Gaussian distribution over the parameter vectors $\thetab$ as the prior belief, i.e.,
  %  
  $$  q(\thetab)  \propto  \exp\biggl[ -\frac{1}{2\tau^2}\thetab^\top\thetab  \biggr], \qquad \tau>0, $$
  %  
  	then 
%  
  \begin{eqnarray*}
%  	
  	p(\thetab | \Xmat, \yv) &\propto& p(\yv | \Xmat, \thetab) q(\thetab) \\
  	%		
  	&\propto& \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{1}{2\tau^2}\thetab^\top\thetab \biggr] \\
%  	
	&=& \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{\|\thetab\|_2^2}{2\tau^2}  \biggr]
  	%		
  \end{eqnarray*}
  %  
  With this, 
  %	
  \begin{align*}
  	%				
  	\thetabh 
  	%		
  	&= \argmax_{\thetab} \log \left( p(\thetab | \Xmat, \yv) \right) \\
  	%		
  	&= \argmax_{\thetab} -\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{\|\thetab\|_2^2}{2\tau^2}   \\
  	%		
  	&= \argmin_{\thetab}  \frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} + \frac{\|\thetab\|_2^2}{2\tau^2}   \\
%  	
  	&= \argmin_{\thetab}   \sum_{i=1}^n (\yi - \thetab^\top \xi)^2  + \frac{\sigma^2}{ \tau^2}  \|\thetab\|_2^2   ,
  	%		
  \end{align*}
  %
  so the  maximum a posteriori estimate coincides for the choice of $\lambda = \frac{\sigma^2}{ \tau^2}>0$ with the regularized empirical risk minimizer for the L2-loss with L2 penalty (over the linear models), i.e., the Ridge regression.
%  
  \item If we choose a Laplace distribution over the parameter vectors $\thetab$ as the prior belief, i.e.,
%  
	$$  q(\thetab)  \propto  \exp\biggl[-\frac{\sum_{i=1}^p |\thetab_i|}{\tau} \biggr], \qquad \tau>0, $$
	%  
	  	then 
%	
	\begin{eqnarray*}
		%  	
		p(\thetab | \Xmat, \yv) &\propto& p(\yv | \Xmat, \thetab) q(\thetab) \\
		%		
		&\propto& \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{\sum_{i=1}^p |\thetab_i|}{\tau}  \biggr] \\
		%  	
		&=& \exp\biggl[-\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{\|\thetab\|_1}{\tau}  \biggr]
		%		
	\end{eqnarray*}
	%  
	With this, 
	%	
	\begin{align*}
		%				
		\thetabh 
		%		
		&= \argmax_{\thetab} \log \left( p(\thetab | \Xmat, \yv) \right) \\
		%		
		&= \argmax_{\thetab} -\frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} -\frac{\|\thetab\|_1}{\tau}   \\
		%		
		&= \argmin_{\thetab}  \frac{ \sum_{i=1}^n (\yi - \thetab^\top \xi)^2}{2\sigma^2} + \frac{\|\thetab\|_1}{\tau}  \\
		%  	
		&= \argmin_{\thetab}   \sum_{i=1}^n (\yi - \thetab^\top \xi)^2  + \frac{2\sigma^2}{ \tau}  \|\thetab\|_1   ,
		%		
	\end{align*}
	%
	so the  maximum a posteriori estimate coincides for the specific choice of $\lambda = \frac{2\sigma^2}{ \tau}$ with the regularized empirical risk minimizer for the L2-loss with L1 penalty (over the linear models), i.e., the Lasso regression.
%	
\end{enumerate}
%
}

\loesung{Covariance Functions}{

\begin{enumerate}
    \item Proofs:
    \begin{enumerate}
        \item $k(\xv, \xvp) = \sigma_0^2$ is a valid covariance function since the kernel matrix $\Kmat = \sigma_0^2 \cdot \one \one^T$ is a positive semi-definite matrix. This can be proved as follows: first, $\Kmat = \Kmat^T$ is symmetric; second, for all $\vv = (v_1, \ldots, v_n)^T \in \R^n$, $\vv^T \Kmat \vv = \sigma_0^2 \cdot (\sumin v_i, \ldots, \sumin v_i) \cdot (v_1, \ldots, v_n)^T = \sigma_0^2 \cdot [v_1 \sumin v_i + \ldots + v_n \sumin v_n] = \sigma_0^2 \cdot [(\sumin v_i) \cdot (\sumin v_i)] \geq 0$.
        \item To prove that $k(\xv, \xvp) = \sigma_0^2 + \xv^T \xvp$ is a valid covariance function, we need to notice that $\sigma_0^2$ and $\xv^T \xvp$ are both valid covariance function, and sum operation also yield a valid covariance function.
        \item $k(\xv, \xvp) = (\sigma_0^2 + \xv^T \xvp)^p$ is a valid covariance function since the linear function is a covariance function, and the only polynomial coefficient $1$ is positive.
        \item The squared exponential can be written as $k(\xv, \xvp) = \exp(-\frac{\xv^T \xv}{2 \ls^2}) \cdot \exp(\frac{\xv^T \xvp}{\ls^2}) \cdot \exp(-\frac{{\xvp}^T \xvp}{2\ls^2})$. Note that $\exp(\frac{\xv^T \xvp}{\ls^2})$ is a valid covariance function and can be easily proved using the composition rules. We further define $t(\xv) = \exp(-\frac{\xv^T \xv}{2 \ls^2})$. Therefore, $k(\xv, \xvp) = t(\xv) \cdot \exp(\frac{\xv^T \xvp}{\ls^2}) \cdot t(\xvp)$ is a valid covariance function.
    \end{enumerate}
    \item $k(\cdot, \cdot)$ is called stationary if $k(\xv, \xv + \dv) = k(\zero, \dv)$; $k(\cdot, \cdot)$ is called isotropic if it is a function of $||\xv - \xvp||$.
    \begin{enumerate}
        \item $k(\xv, \xvp) = \sigma_0^2$ is stationary since $k(\xv, \xv + \dv) = k(\zero, \dv) = \sigma_0^2$. It can be written as $k(\xv, \xvp) = \sigma_0^2 ||\xv - \xvp||^0$, so it is isotropic.
        \item $k(\xv, \xvp) = \sigma_0^2 + \xv^T \xvp$ is not stationary since $k(\xv, \xv + \dv) = \sigma_0^2 + \xv^T \xv + \xv^T \dv$, while $k(\zero, \xv + \dv) = \sigma_0^2$. Furthermore, it can not be written as $k(|| \xv - \xvp||)$, so it is not isotropic.
        \item Similar to linear covariance function, the polynomial covariance function is neither stationary nor isotropic.
        \item The squared exponential covariance function is stationary as $k(\xv, \xv + \dv) = k(\zero, \dv) = \exp(-\frac{||d||^2}{2\ls^2})$. It is a function of $||\xv - \xvp||$, so it is isotropic.
        \item Similar to the argument of squared exponential covariance function, the Mat\'ern covariance function is stationary and isotropic.
        \item Similar to the argument of squared exponential covariance function, the exponential covariance function is stationary and isotropic.
    \end{enumerate}
\end{enumerate}
}
\end{document}
