\documentclass[a4paper]{article}
\include{../../style/preamble_ueb.tex}


\begin{document}

\lstset{style=rstyle}


\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopfaml{12}{Multitarget Learning}


\loesung{Conditional Random Fields vs. Structured SVMs}{

%
Similar to probabilistic classifier chains, conditional random fields try to model the conditional distribution $\P(\yv~|~\xv)$ by means of
%
$$		\pi(\xv,\yv) = \frac{\exp(s(\xv,\yv))}{\sum_{\yv' \in \Yspace^m} \exp(s(\xv,\yv'))},	$$
%
where $x\in \Xspace$ and $\yv \in \Yspace$ with $\Yspace$ being a finite set (e.g., multi-label classification), and $s:\Xspace \times \Yspace \to \R$ being a scoring function.
%
Training of a conditional random field is based on (regularized) empirical risk minimization using the negative log-loss:
%
$$ \ell_{log}(\xv,\yv,s) = \log\left(\sum_{\yv' \in \Yspace^m} \exp(s(\xv,\yv'))\right) - s(\xv,\yv).	$$
%
Predictions are then made by means of
%
\begin{align}\label{prediction}
%	
	h(\xv) = \argmax_{\yv \in \Yspace^m} s(\xv,\yv).
%	
\end{align}
%
Structured Support Vector Machines (Structured SVMs) are also using scoring functions for the prediction, but use the structured hinge loss for the (regularized) empirical risk minimization approach:
%
$$	\ell_{shinge}(\xv,\yv,s) = \max_{\yv' \in \Yspace^m} \left( \ell(\yv,\yv') + s(\xv,\yv')	 - s(\xv,\yv) \right),	$$
%
where $\ell:\Yspace^m \times \Yspace^m \to \R$ is some target loss function (e.g., Hamming loss or subset 0/1 loss).

Show that if we use scoring functions $s$ of the form
%
$$	s(\xv,\yv) = \sum_{j=1}^m s_j(\xv,y_j),	$$
%
where $s_j:\Xspace \times \Yspace \to \R$ are scoring functions for the $j$-th target, then
%
\begin{enumerate}
%	
	\item conditional random fields are very well suited to model the case, where the distributions of the targets $y_1,\ldots,y_m$ are conditionally independent,
	In other words, show that $\P(\yv~|~\xv) \approx \prod_{j=1}^m \P(y_j~|~\xv)$. \textit{Hint:} Use the multinomial theorem: $$(z_1 + z_2 + \ldots + z_g)^m = \sum_{k_1 + k_2 + \ldots k_g = m} \binom{m}{k_1, k_2, \ldots, k_g} \prod_{t=1}^g z_t^{k_t}.$$ 

	\textbf{Solution:}
%	
	
	The idea of conditional random fields is to model the joint conditional distribution $\P(\yv~|~\xv)$ by means of $\pi(\xv,\yv).$
%	
	Thus, it should hold $\P(\yv~|~\xv) \approx \pi(\xv,\yv)$ and with this,
%	
	\begin{align*}
%		
		\P(\yv~|~\xv) 
%		
		&\approx \pi(\xv,\yv) \\
%		
		&= \frac{\exp(s(\xv,\yv))}{\sum_{\yv' \in \Yspace^m} \exp(s(\xv,\yv'))} \\
%		
		&=  \frac{\exp\left(\sum_{j=1}^m s_j(\xv,y_j)\right)}{\sum_{\yv' \in \Yspace^m} \exp \left(  \sum_{j=1}^m s_j(\xv,y_j') \right)} \\
%		
		&=  \frac{ \prod_{j=1}^m \exp\left( s_j(\xv,y_j)\right)}{\sum_{\yv' \in \Yspace^m} \prod_{j=1}^m \exp \left(  s_j(\xv,y_j') \right)} \\
%		
		&=  \frac{ \prod_{j=1}^m \exp\left( s_j(\xv,y_j)\right)}{ \prod_{j=1}^m \sum_{y_j' \in \Yspace}  \exp \left(  s_j(\xv,y_j') \right)} \\
%		
		&= \prod_{j=1}^m \underbrace{\frac{  \exp\left( s_j(\xv,y_j)\right)}{ \sum_{y_j' \in \Yspace}  \exp \left(  s_j(\xv,y_j') \right)}}_{ =:\pi_j(\xv,y_j) }. \\
%		
	\end{align*}

Note that we used $\sum_{\yv \in \Yspace^m} \prod_{j=1}^m \exp \left( s_j(\xv,y_j) \right) = \prod_{j=1}^m \sum_{y_j \in \Yspace}  \exp \left(  s_j(\xv,y_j) \right)$. We prove this as follows. For brevity let's assume $|\mathcal{Y}| = g$ and define $S_j = s_j(\xv, y_j)$. The left hand side can be written as 
\begin{align*}
    \sum_{\yv \in \Yspace^m} \prod_{j=1}^m \exp \left( s_j(\xv,y_j) \right) &= \sum_{k_1 + k_2 + \ldots + k_g = m} \binom{m}{k_1, k_2, \ldots, k_g} \prod_{t=1}^g S_t^{k_t}.
\end{align*}
So we enumerate all the possible $\yv$.
By using the binomial theorem, this boils down to 
\begin{align*}
    \sum_{k_1 + k_2 + \ldots + k_g = m} \binom{m}{k_1, k_2, \ldots, k_g} \prod_{t=1}^g S_t^{k_t}
    &= (S_1 + S_2 + \ldots + S_g)^m \\
    &= \prod_{j=1}^m \left(\sum_{t=1}^g S_t \right) \\
    &= \prod_{j=1}^m \left( \sum_{y_j \in \mathcal{Y}} \exp(s_j(\xv, y_j)) \right). \\
\end{align*}
(If you find a problem understanding this part of proof, try a simple example with $|\mathcal{Y}| = 2$ and $m = 3$ and compute the both sides of the equation by hand.)

%
	So, if $\pi_j(\xv,y_j)$ is interpreted as a model for the marginal conditional distribution $\P(y_j~|~\xv),$ we see from above
%	
	$$   \P(\yv~|~\xv) \approx \prod_{j=1}^m \P(y_j~|~\xv),	$$
%	
	i.e., the targets are conditionally independent. 

	

	\item the structured hinge loss corresponds to the multiclass hinge loss for the targets if we use the (non-averaged) Hamming loss for $\ell(\yv,\yv')= \sum_{j=1}^m \mathds{1}_{[y_j \neq y_j']} $, i.e.,
%	
	$$	\ell_{shinge}(\xv,\yv,s) =  \sum_{j=1}^m \max_{y_j' \in \Yspace} \left( \mathds{1}_{[y_j \neq y_j']} + s_j(\xv,y_j')	- s_j(\xv,y_j) \right).	$$
%	

	\textbf{Solution:}
%	
	
	This can be seen immediately from the definition:
%	
	\begin{align*}
%		
		\ell_{shinge}(\xv,\yv,s) 
%		
		&= \max_{\yv' \in \Yspace^m} \left( \ell(\yv,\yv') + s(\xv,\yv')	 - s(\xv,\yv) \right) \\
%		
		&=  \max_{\yv' \in \Yspace^m} \left( \sum_{j=1}^m \mathds{1}_{[y_j \neq y_j']} + s(\xv,\yv')	 - s(\xv,\yv) \right) \\
%		
		&=  \max_{\yv' \in \Yspace^m} \left( \sum_{j=1}^m \mathds{1}_{[y_j \neq y_j']} +  \sum_{j=1}^m s_j(\xv,y_j')	 -  \sum_{j=1}^m s_j(\xv,y_j) \right) \\
%		
		&=  \max_{\yv' \in \Yspace^m} \left( \sum_{j=1}^m \mathds{1}_{[y_j \neq y_j']} +   s_j(\xv,y_j')	 -   s_j(\xv,y_j) \right) \\
%		
		&=  \sum_{j=1}^m \max_{y_j' \in \Yspace} \left( \mathds{1}_{[y_j \neq y_j']} + s_j(\xv,y_j')	- s_j(\xv,y_j) \right). \tag{Summands are independent.}
%		 
%		
	\end{align*}
	
\end{enumerate}

}


\end{document}