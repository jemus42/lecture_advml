\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}


\input{../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfic}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopficsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfaml}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Advanced Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}


\newcommand{\kopfdive}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Deep Dive\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #1}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}: #1}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}[1]
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}: #1}\\\noindent}
	{\bigskip}

\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopfaml{9}{Multi-target Prediction}

\loesung{Multivariate Regression}{


\newcommand{\ba}{\mathbf{a}}

%
	%
	Consider the multivariate regression setting on $\Xspace \subset \R^p$ without target features, i.e., $\Yspace=\R$ and $\mathcal{T} = \{1,\ldots,m\}.$	
%	
	Furthermore, consider the approach of learning a (simple) linear model $f_j(\xv) = \ba_j^\top \xv$ for each target $j$ independently.
%	
	For this purpose, we would face the following optimization problem:
%	
	\begin{equation*}	\label{eq:multiridge}
%		
		\min_A \|Y - \Xmat A \|^2_F,
%		
	\end{equation*}
%
	where  $ \| B \|^2_F  = \sqrt{ \sum_{i=1}^n \sum_{j=1}^m B_{i,j}^2 } $ is the Frobenius norm for a matrix $B \in \R^{n \times m}$ and 
	%		
	\begin{equation*}
		\label{eq:notation}
		\Xmat = \begin{bmatrix} (\xv^{(1)} )^\top \\ \vdots \\ (\xv^{(n)})^\top \end{bmatrix}, \qquad A = [\ba_1 \quad \cdots \quad \ba_m] \,, \qquad Y = \begin{bmatrix} \yv^{(1)}\\ \vdots \\ \yv^{(n)} \end{bmatrix}.
	\end{equation*}
	
	\begin{enumerate}
%	
	\item Show that $\hat A = (\Xmat^\top \Xmat)^{-1} \Xmat^\top Y$ is the optimal solution in this case (provided that $\Xmat^\top \Xmat$ is invertible).
	
	\textbf{Solution:}
	
	Note that for a matrix $B = (\bm{b}_1 \ldots \bm{b}_m) \in \R^{n\times m},$ it holds that
%	
	\begin{align*} 
%		\label{frobenius}
%		
		\| B \|^2_F  
%		 
	 	=  \sum_{i=1}^n \sum_{j=1}^m B_{i,j}^2 
%		 
		=  \sum_{j=1}^m \sum_{i=1}^n B_{i,j}^2 
%		
		&=  \sum_{j=1}^m \bm{b}_j^\top \bm{b}_j. 
%		
	\end{align*}
%
	Thus, the function $f(A)=\|Y - \Xmat A \|^2_F$ we want to minimize can be written as
%	
	\begin{align*}
%		
		\|Y - \Xmat A \|^2_F 
%		
		&= \sum_{j=1}^m (\bm{y}_j -\Xmat \bm{a}_j)^\top(\bm{y}_j -\Xmat \bm{a}_j) \\
%		
		&= \sum_{j=1}^m \bm{y}_j^\top\bm{y}_j - 2 \bm{y}_j^\top \Xmat \bm{a}_j + \bm{a}_j^\top \Xmat^\top  \Xmat \bm{a}_j,
%		
	\end{align*}
%
	where $\bm{y}_j$ is the $j$-th column of $Y.$
%	
	Therefore, we can write $f(A) = \sum_{j=1}^m f_j(\ba_j),$ where 
%	
	$$	f_j(\ba) = 	\bm{y}_j^\top\bm{y}_j - 2 \bm{y}_j^\top \Xmat \bm{a} + \bm{a}^\top \Xmat^\top  \Xmat \bm{a}	$$
%	
	and we can minimize each $f_j$ separately (w.r.t.\ to $\ba_j$).
%	
	We compute the gradient of $f_j$ and set it to $\bm{0}$ and solve w.r.t.\ $\ba:$
%	
	\begin{align*}
%		
		\nabla f_j(\ba) 
%		
		&= - 2 \bm{y}_j^\top \Xmat  + 2 \Xmat^\top  \Xmat \bm{a} \stackrel{!}{=} \bm{0} \\
%		
		&\Leftrightarrow \ba = (\Xmat^\top \Xmat)^{-1} \Xmat^\top \bm{y}_j.
%		
	\end{align*}
%
	We check that this is indeed a minimum, by checking that the Hessian matrix is positive semi-definite:
%	
	The Hessian matrix is
%
		\begin{align*}
		%		
		\nabla \nabla^\top f_j(\ba) 
		%		
		&=   2 \Xmat^\top  \Xmat.
		%		 
		%		
	\end{align*}
%
	It is positive semi-definite, since for any $\bm{z}\in \R^p$ it holds that
%	
	\begin{align*}
%		
		\bm{z}^\top (2 \Xmat^\top  \Xmat) \bm{z} 
%		
		= 2 \bm{z}^\top  \Xmat^\top  \Xmat  \bm{z} 
%		
		= 	2   (\Xmat  \bm{z})^\top \Xmat  \bm{z} 
%		
		= 2 \| \Xmat  \bm{z} \|_2^2 \geq 0.
%		
	\end{align*}
%
	Consequently, the minimizer of $f_j$ is $\hat{\ba}_j= (\Xmat^\top \Xmat)^{-1} \Xmat^\top \bm{y}_j,$ so that the minimizer of $f$ is 
%	
	$$	\hat{A} = (\hat{\ba}_1 ~ \ldots ~ \hat{\ba}_m) = \left(	(\Xmat^\top \Xmat)^{-1} \Xmat^\top \bm{y}_1 ~ \ldots ~ (\Xmat^\top \Xmat)^{-1} \Xmat^\top \bm{y}_m		\right) = (\Xmat^\top \Xmat)^{-1} \Xmat^\top Y.	$$
%	
	In particular, the gradient of $f$ w.r.t.\ matrix $A=(\ba_1 ~ \ldots ~ \ba_m)$ is 
%	
	\begin{align*}
%		
		\nabla f(A) 
%		
		&= ( \nabla f_1(\ba_1) ~ \ldots ~ \nabla f_m(\ba_m) )  \\
%		
		&= \left( - 2 \bm{y}_1^\top \Xmat  + 2 \Xmat^\top  \Xmat \bm{a}_1 \quad \ldots \quad - 2 \bm{y}_m^\top \Xmat  + 2 \Xmat^\top  \Xmat \bm{a}_m\right) \\
%		
		&= -2Y^\top \Xmat  + 2 \Xmat^\top  \Xmat A.
%		
	\end{align*}
%
	Hence, a gradient descent routine with (fixed) step size $\alpha$ for $f$ would iterate as follows:
%	
	\begin{align*}
%		
		\hat A \leftarrow \hat A - 2 \alpha \left( -Y^\top \Xmat  +  \Xmat^\top  \Xmat \hat A \right).
%		
	\end{align*}
%
	\item Assume that the data $(\xi,\yv^{(i)}) \in \Xspace \times \Yspace^m$ is generated\footnote{Of course, in an iid fashion and the $\xv$'s are independent of the $\bm{\eps}$'s.} according to the following statistical model
%	
	$$	(y_1,\ldots,y_m) =	\yv = (\xi)^\top A^* + \bm{\eps}^\top,		$$
%	
	where $A^* \in \R^{p\times m}$ and $\bm{\eps} \sim \normal(\bm{0},\bm{\Sigma}).$
%
	Show that the maximum likelihood estimate for $A^*$ coincides with the estimate in (a). 
	
	\textbf{Solution:}
%	
	
	Under the statistical model it holds that $\yv^{(i)}~|~\xi \sim \normal\left(	(\xi)^\top A^*, \bm{\Sigma}	\right),$ i.e.\footnote{Note that $\yv^{(i)} - (\xi)^\top A^* $ is a row vector.},
%	
	\begin{align}\label{def_distr}
%		
		p(\yv^{(i)}~|~\xi,A^*) 
%		
		&= (2\pi)^{-m/2} |\bm{\Sigma}|^{-1/2} \, \exp\left[	-\frac12 (\yv^{(i)} - (\xi)^\top A^* )	\bm{\Sigma}^{-1} (\yv^{(i)} - (\xi)^\top A^* )^\top	\right] \notag \\
%		
		&\propto \exp\left[	-\frac12 \left(\yv^{(i)} - (\xi)^\top A^* \right)	\bm{\Sigma}^{-1} \left(\yv^{(i)} - (\xi)^\top A^* \right)^\top	\right] .
%		
	\end{align}
%
	So the log-likelihood for $A$ is
%	
	\begin{align*}
%		
		l(A~|~\D) 
%		
		&= \log\left(	\prod_{i=1}^n 		p(\yv^{(i)}~|~\xi,A)	\right) \\
%		
		&\propto \log\left(	 	\exp\left[	-\frac12 \sum_{i=1}^n \left(\yv^{(i)} - (\xi)^\top A \right)	\bm{\Sigma}^{-1} \left(\yv^{(i)} - (\xi)^\top A \right)^\top	\right]	\right) \\
%		
		&= 	-\sum_{i=1}^n  \left(\yv^{(i)} - (\xi)^\top A \right)	\bm{\Sigma}^{-1} \left(\yv^{(i)} - (\xi)^\top A \right)^\top.		 
%		
	\end{align*}
%	
	So, we want to maximize the function 
%	
	\begin{align*}
%		
		g(A) 
%		
		&= 	-\sum_{i=1}^n  \left(\yv^{(i)} - (\xi)^\top A \right)	\bm{\Sigma}^{-1} \left(\yv^{(i)} - (\xi)^\top A \right)^\top \\
%		
		&= 	-\sum_{i=1}^n  \yv^{(i)} \bm{\Sigma}^{-1} (\yv^{(i)})^\top  
		- 2 (\xi)^\top A \bm{\Sigma}^{-1} (\yv^{(i)})^\top 
		+ (\xi)^\top A	\bm{\Sigma}^{-1}  A^\top \xi.	
%		
	\end{align*}
%	
	We compute the gradient of $g$ and set it to $\bm{0}_{p\times m}$ and solve w.r.t.\ $A:$
	%	
	\begin{align*}
		%		
		\nabla g(A) 
		%		
		&= 	\sum_{i=1}^n   2 \xi \yv^{(i)}   \bm{\Sigma}^{-1}
		- 2   \xi	(\xi)^\top  A \bm{\Sigma}^{-1}   \stackrel{!}{=} \bm{0}_{p\times m} \\
		%		
		&\Leftrightarrow 	\Xmat^\top Y  \bm{\Sigma}^{-1}
		-   \Xmat^\top \Xmat  A \bm{\Sigma}^{-1}   \stackrel{!}{=} \bm{0}_{p\times m} \\
		&\Leftrightarrow A = (\Xmat^\top \Xmat)^{-1} \Xmat^\top Y,
		%		
	\end{align*}
%
	where we used for computing the gradient that 
%	
	\begin{align*}
%		
		&\frac{\partial \bm{z}^\top \bm{B} \tilde{\bm{z}} }{\partial  \bm{B} } = \bm{z} \tilde{\bm{z}}^\top, \qquad \forall \bm{z}\in\R^n,\tilde{\bm{z}}\in \R^{m}, \bm{B}\in\R^{n \times m}, \\
%		
		&\frac{\partial \bm{z}^\top \bm{B} \bm{C} \bm{B}^\top \tilde{\bm{z}} }{\partial  \bm{B} } 
%		
		= 2 \bm{z} \tilde{\bm{z}}^\top \bm{B} \bm{C}^\top  , \qquad \forall \bm{z}\in\R^n,\tilde{\bm{z}}\in \R^{m}, \bm{B}\in\R^{n \times m}, \bm{C} \in \R^{n \times n}.
%		
	\end{align*}
%
	Moreover, any matrix product $\Xmat^\top Y$ can be written as the sum of outer products of the column and row vectors: $ \sum_{i=1}^n    \xi \yv^{(i)}.$
	
%
	Thus, the MLE coincides with the OLS in (a).		
	\item Write a function implementing a gradient descent routine for the optimization of this linear model.
	%
	 \item Run a small simulation study by creating 20 data sets as indicated below and test different step sizes $\alpha$ (fixed across iterations) against each other and against the state-of-the-art routine for linear models (in R, using the function \texttt{lm}, in Python, e.g., \texttt{sklearn.linear\_model.LinearRegression}).
 %
  \begin{itemize}
%  	
    \item Compare the difference in the estimated parameter matrices $\hat A$ using the mean squared error, i.e., 
    %
    $$ \frac{1}{m\cdot p} \sum_{i=1}^p \sum_{j=1}^m (\ba^{*}_{i,j}-\hat{\ba}_{i,j})^2$$ 
    %
    and summarize the difference over all 100 simulation repetitions.
%    
    \item Compare the estimation also with the James-Stein estimate of $A^*$, which is given by 
%    
	$$  A^{JS} = \left(  \ba_1^{JS} \ldots \ba_m^{JS}  \right),$$
%	
	where $$  \ba_j^{JS} = \left (1 - \frac{(m-2)\sigma^2}{n\|\hat{\ba}_j - \ba_j^* \|^2_2} \right )  (\hat{\ba}_j - \ba_j^*) + \ba_j^*, \quad j=1,\ldots,m.$$
%	
	and $\hat{\ba}_j$ is the MLE for the $j$th target parameter.
%    
    \end{itemize}
%	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#' @param step_size the step_size in each iteration}
\hlcom{#' @param X the feature input matrix X}
\hlcom{#' @param Y the score matrix Y}
\hlcom{#' @param A the parameter matrix}
\hlcom{#' @param eps a small constant measuring the changes in each update step. }
\hlcom{#' Stop the algorithm if the estimated model parameters do not change}
\hlcom{#' more than \textbackslash{}code\{eps\}.}

\hlcom{#' @return a set of optimal parameter matrix A}
\hlstd{gradient_descent} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{step_size}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{Y}\hlstd{,} \hlkwc{A} \hlstd{=} \hlkwd{matrix}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{0}\hlstd{,}\hlkwd{ncol}\hlstd{(X)}\hlopt{*}\hlstd{m),}\hlkwc{ncol}\hlstd{=m),}
                             \hlkwc{eps} \hlstd{=} \hlnum{1e-8}\hlstd{)\{}

  \hlstd{change} \hlkwb{<-} \hlnum{1} \hlcom{# something larger eps}

  \hlstd{XtX} \hlkwb{<-} \hlkwd{crossprod}\hlstd{(X)}
  \hlstd{XtY} \hlkwb{<-} \hlkwd{crossprod}\hlstd{(X,Y)}

  \hlkwa{while}\hlstd{(}\hlkwd{sum}\hlstd{(}\hlkwd{abs}\hlstd{(change))} \hlopt{>} \hlstd{eps)\{}

    \hlcom{# Use standard gradient descent:}
    \hlstd{change} \hlkwb{<-} \hlopt{+} \hlstd{step_size} \hlopt{*} \hlstd{(XtY} \hlopt{-} \hlstd{XtX}\hlopt{%*%}\hlstd{A)}

    \hlcom{# update A in the end}
    \hlstd{A} \hlkwb{<-} \hlstd{A} \hlopt{+} \hlstd{change}


  \hlstd{\}}

  \hlkwd{return}\hlstd{(A)}

\hlstd{\}}

\hlcom{# make it all reproducible}
\hlkwd{set.seed}\hlstd{(}\hlnum{123}\hlstd{)}

\hlcom{# settings}
\hlstd{n} \hlkwb{<-} \hlnum{10000}
\hlstd{p} \hlkwb{<-} \hlnum{100}
\hlstd{m} \hlkwb{<-} \hlnum{6}
\hlstd{nr_sims} \hlkwb{<-} \hlnum{20}

\hlcom{# define mse}
\hlstd{mse} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,}\hlkwc{y}\hlstd{)} \hlkwd{mean}\hlstd{((x}\hlopt{-}\hlstd{y)}\hlopt{^}\hlnum{2}\hlstd{)}

\hlcom{# create data (only once)}
\hlstd{X} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(n}\hlopt{*}\hlstd{p),} \hlkwc{ncol}\hlstd{=p)}
\hlstd{A_truth} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{runif}\hlstd{(p}\hlopt{*}\hlstd{m,} \hlopt{-}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{),}\hlkwc{ncol}\hlstd{=m)}
\hlstd{f_truth} \hlkwb{<-} \hlstd{X}\hlopt{%*%}\hlstd{A_truth}

\hlcom{# create result object}
\hlstd{result_list} \hlkwb{<-} \hlkwd{vector}\hlstd{(}\hlstr{"list"}\hlstd{, nr_sims)}



\hlstd{js_estimate} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{A}\hlstd{,}\hlkwc{A_truth}\hlstd{)\{}
\hlstd{A_JS} \hlkwb{=} \hlstd{A}

        \hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(A))\{}
                \hlstd{A_JS[,i]}\hlkwb{=} \hlstd{(}\hlnum{1}\hlopt{-}\hlnum{4}\hlopt{*}\hlstd{(m}\hlopt{-}\hlnum{2}\hlstd{)}\hlopt{/}\hlstd{(n}\hlopt{*}\hlkwd{sum}\hlstd{((A[,i]}\hlopt{-}\hlstd{A_truth[,i])}\hlopt{^}\hlnum{2}\hlstd{)))}\hlopt{*}
                  \hlstd{(A[,i]}\hlopt{-}\hlstd{A_truth[,i])}\hlopt{+}\hlstd{A_truth[,i]}
        \hlstd{\}}
\hlstd{A_JS}
\hlstd{\}}

\hlkwa{for}\hlstd{(sim_nr} \hlkwa{in} \hlkwd{seq_len}\hlstd{(nr_sims))}
\hlstd{\{}

  \hlcom{# create response}
  \hlstd{Y} \hlkwb{<-} \hlstd{f_truth} \hlopt{+} \hlkwd{rnorm}\hlstd{(n}\hlopt{*}\hlstd{m,} \hlkwc{sd} \hlstd{=} \hlnum{2}\hlstd{)}

  \hlstd{time_lm} \hlkwb{<-} \hlkwd{system.time}\hlstd{(}
    \hlstd{coef_lm} \hlkwb{<-} \hlkwd{coef}\hlstd{(}\hlkwd{lm}\hlstd{(Y}\hlopt{~-}\hlnum{1}\hlopt{+}\hlstd{X))}
  \hlstd{)[}\hlstr{"elapsed"}\hlstd{]}

  \hlstd{time_js} \hlkwb{<-} \hlkwd{system.time}\hlstd{(}
    \hlstd{coef_js} \hlkwb{<-} \hlkwd{js_estimate}\hlstd{(coef_lm,A_truth)}
  \hlstd{)[}\hlstr{"elapsed"}\hlstd{]}
  \hlstd{time_js} \hlkwb{=} \hlstd{time_js} \hlopt{+} \hlstd{time_lm}

  \hlstd{time_gd_1} \hlkwb{<-} \hlkwd{system.time}\hlstd{(}
    \hlstd{coef_gd_1} \hlkwb{<-} \hlkwd{gradient_descent}\hlstd{(}\hlkwc{step_size} \hlstd{=} \hlnum{0.0001}\hlstd{,} \hlkwc{X} \hlstd{= X,} \hlkwc{Y} \hlstd{= Y)}
  \hlstd{)[}\hlstr{"elapsed"}\hlstd{]}

  \hlstd{time_gd_2} \hlkwb{<-} \hlkwd{system.time}\hlstd{(}
    \hlstd{coef_gd_2} \hlkwb{<-} \hlkwd{gradient_descent}\hlstd{(}\hlkwc{step_size} \hlstd{=} \hlnum{0.00005}\hlstd{,} \hlkwc{X} \hlstd{= X,} \hlkwc{Y} \hlstd{= Y)}
  \hlstd{)[}\hlstr{"elapsed"}\hlstd{]}


  \hlstd{mse_lm} \hlkwb{<-} \hlkwd{mse}\hlstd{(coef_lm, A_truth)}
  \hlstd{mse_js} \hlkwb{<-} \hlkwd{mse}\hlstd{(coef_js, A_truth)}
  \hlstd{mse_gd_1} \hlkwb{<-} \hlkwd{mse}\hlstd{(coef_gd_1, A_truth)}
  \hlstd{mse_gd_2} \hlkwb{<-} \hlkwd{mse}\hlstd{(coef_gd_2, A_truth)}

  \hlcom{# save results in list (performance, time)}
  \hlstd{result_list[[sim_nr]]} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{mse_lm} \hlstd{= mse_lm,}
                                                  \hlkwc{mse_js} \hlstd{= mse_js,}
                                      \hlkwc{mse_gd_1} \hlstd{= mse_gd_1,}
                                      \hlkwc{mse_gd_2} \hlstd{= mse_gd_2,}
                                      \hlkwc{time_lm} \hlstd{= time_lm,}
                                      \hlkwc{time_js} \hlstd{= time_js,}
                                      \hlkwc{time_gd_1} \hlstd{= time_gd_1,}
                                      \hlkwc{time_gd_2} \hlstd{= time_gd_2)}

\hlstd{\}}

\hlkwd{library}\hlstd{(ggplot2)}
\hlkwd{library}\hlstd{(dplyr)}
\hlkwd{library}\hlstd{(tidyr)}

\hlkwd{do.call}\hlstd{(}\hlstr{"rbind"}\hlstd{, result_list)} \hlopt{%>%}
  \hlkwd{gather}\hlstd{()} \hlopt{%>%}
  \hlkwd{mutate}\hlstd{(}\hlkwc{what} \hlstd{=} \hlkwd{ifelse}\hlstd{(}\hlkwd{grepl}\hlstd{(}\hlstr{"mse"}\hlstd{, key),} \hlstr{"MSE"}\hlstd{,} \hlstr{"Time"}\hlstd{),}
         \hlkwc{algorithm} \hlstd{=} \hlkwd{gsub}\hlstd{(}\hlstr{"(mse|time)\textbackslash{}\textbackslash{}_(.*)"}\hlstd{,}\hlstr{"\textbackslash{}\textbackslash{}2"}\hlstd{, key))} \hlopt{%>%}
  \hlkwd{ggplot}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{x} \hlstd{= algorithm,} \hlkwc{y} \hlstd{= value))} \hlopt{+}
  \hlkwd{geom_boxplot}\hlstd{()} \hlopt{+} \hlkwd{theme_bw}\hlstd{()} \hlopt{+}
  \hlkwd{facet_wrap}\hlstd{(}\hlopt{~} \hlstd{what,} \hlkwc{scales} \hlstd{=} \hlstr{"free"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-3-1} 
\end{knitrout}
%

\end{enumerate}
}

\loesung{Conditional Random Fields vs. Structured SVMs}{

%
Similar to probabilistic classifier chains, conditional random fields try to model the conditional distribution $\P(\yv~|~\xv)$ by means of
%
$$		\pi(\xv,\yv) = \frac{\exp(s(\xv,\yv))}{\sum_{\yv' \in \Yspace^m} \exp(s(\xv,\yv'))},	$$
%
where $x\in \Xspace$ and $\yv \in \Yspace$ with $\Yspace$ being a finite set (e.g., multi-label classification), and $s:\Xspace \times \Yspace \to \R$ being a scoring function.
%
Training of a conditional random field is based on (regularized) empirical risk minimization using the negative log-loss:
%
$$ \ell_{log}(\xv,\yv,s) = \log\left(\sum_{\yv' \in \Yspace^m} \exp(s(\xv,\yv'))\right) - s(\xv,\yv).	$$
%
Predictions are then made by means of
%
\begin{align}\label{prediction}
%	
	h(\xv) = \argmax_{\yv \in \Yspace^m} s(\xv,\yv).
%	
\end{align}
%
Structured Support Vector Machines (Structured SVMs) are also using scoring functions for the prediction, but use the structured hinge loss for the (regularized) empirical risk minimization approach:
%
$$	\ell_{shinge}(\xv,\yv,s) = \max_{\yv' \in \Yspace^m} \left( \ell(\yv,\yv') + s(\xv,\yv')	 - s(\xv,\yv) \right),	$$
%
where $\ell:\Yspace^m \times \Yspace^m \to \R$ is some target loss function (e.g., Hamming loss or subset 0/1 loss).

Show that if we use scoring functions $s$ of the form
%
$$	s(\xv,\yv) = \sum_{j=1}^m s_j(\xv,y_j),	$$
%
where $s_j:\Xspace \times \Yspace \to \R$ are scoring functions for the $j$-th target, then
%
\begin{enumerate}
%	
	\item conditional random fields are very well suited to model the case, where the distributions of the targets $y_1,\ldots,y_m$ are conditionally independent,
%	

	\textbf{Solution:}
%	
	
	The idea of conditional random fields is to model the joint conditional distribution $\P(\yv~|~\xv)$ by means of $\pi(\xv,\yv).$
%	
	Thus, it should hold $\P(\yv~|~\xv) \approx \pi(\xv,\yv)$ and with this,
%	
	\begin{align*}
%		
		\P(\yv~|~\xv) 
%		
		&\approx \pi(\xv,\yv) \\
%		
		&= \frac{\exp(s(\xv,\yv))}{\sum_{\yv' \in \Yspace^m} \exp(s(\xv,\yv'))} \\
%		
		&=  \frac{\exp\left(\sum_{j=1}^m s_j(\xv,y_j)\right)}{\sum_{\yv' \in \Yspace^m} \exp \left(  \sum_{j=1}^m s_j(\xv,y_j') \right)} \\
%		
		&=  \frac{ \prod_{j=1}^m \exp\left( s_j(\xv,y_j)\right)}{\sum_{\yv' \in \Yspace^m} \prod_{j=1}^m \exp \left(  s_j(\xv,y_j') \right)} \\
%		
		&=  \frac{ \prod_{j=1}^m \exp\left( s_j(\xv,y_j)\right)}{ \prod_{j=1}^m \sum_{y_j' \in \Yspace}  \exp \left(  s_j(\xv,y_j') \right)} \\
%		
		&= \prod_{j=1}^m \underbrace{\frac{  \exp\left( s_j(\xv,y_j)\right)}{ \sum_{y_j' \in \Yspace}  \exp \left(  s_j(\xv,y_j') \right)}}_{ =:\pi_j(\xv,y_j) }. \\
%		
	\end{align*}
%
	So, if $\pi_j(\xv,y_j)$ is interpreted as a model for the marginal conditional distribution $\P(y_j~|~\xv),$ we see from above
%	
	$$   \P(\yv~|~\xv) \approx \prod_{j=1}^m \P(y_j~|~\xv),	$$
%	
	i.e., the targets are conditionally independent.

	

	\item the structured hinge loss corresponds to the multiclass hinge loss for the targets if we use the (non-averaged) Hamming loss for $\ell(\yv,\yv')= \sum_{j=1}^m \mathds{1}_{[y_j \neq y_j']} $, i.e.,
%	
	$$	\ell_{shinge}(\xv,\yv,s) =  \sum_{j=1}^m \max_{y_j' \in \Yspace} \left( \mathds{1}_{[y_j \neq y_j']} + s_j(\xv,y_j')	- s_j(\xv,y_j) \right).	$$
%	

	\textbf{Solution:}
%	
	
	This can be seen immediately from the definition:
%	
	\begin{align*}
%		
		\ell_{shinge}(\xv,\yv,s) 
%		
		&= \max_{\yv' \in \Yspace^m} \left( \ell(\yv,\yv') + s(\xv,\yv')	 - s(\xv,\yv) \right) \\
%		
		&=  \max_{\yv' \in \Yspace^m} \left( \sum_{j=1}^m \mathds{1}_{[y_j \neq y_j']} + s(\xv,\yv')	 - s(\xv,\yv) \right) \\
%		
		&=  \max_{\yv' \in \Yspace^m} \left( \sum_{j=1}^m \mathds{1}_{[y_j \neq y_j']} +  \sum_{j=1}^m s_j(\xv,y_j')	 -  \sum_{j=1}^m s_j(\xv,y_j) \right) \\
%		
		&=  \max_{\yv' \in \Yspace^m} \left( \sum_{j=1}^m \mathds{1}_{[y_j \neq y_j']} +   s_j(\xv,y_j')	 -   s_j(\xv,y_j) \right) \\
%		
		&=  \sum_{j=1}^m \max_{y_j' \in \Yspace} \left( \mathds{1}_{[y_j \neq y_j']} + s_j(\xv,y_j')	- s_j(\xv,y_j) \right). \tag{Summands are independent.}
%		 
%		
	\end{align*}
	
\end{enumerate} 
}

\end{document}
