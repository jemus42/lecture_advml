
<<setup-child, include = FALSE>>=
library(knitr)
library(mlr)
library(mlbench)
library(ggplot2)
library(gridExtra)
set_parent("../style/preamble.Rnw")
@


\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-mbo}

\lecturechapter{10}{Automatic Parameter Tuning and Architecture Search}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{frame}{Machine Learning}
  \begin{itemize}
    \item Successful, but requires human labor and expertise :
    \begin{itemize}
      \item Pre-process data.
      \item Select/engineer features.
      \item Select a model family.
      \item Optimize hyperparameters (algorithm parameters).
      % \item $\cdots$
    \end{itemize}
    \item Deep learning lets us automatically learn features.
    \begin{itemize}
      \item Automates feature engineering step, with large amount of data
      \item Even more sensitive to architectures, hyperparameters, $\cdots$
    \end{itemize}
  
  \end{itemize}
  \begin{center}
    \scalebox{0.65}{\includegraphics{figure_man/automl1.png}}
  \end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Automatic Machine Learning}
  \begin{itemize}
    \item Can algorithms be trained to automatically build end-to-
end machine learning systems?
  \end{itemize}
  
  Use machine learning to do better machine learning
  
   \begin{itemize}
    \item Can we turn \\
   \textit{Solution = data + manual exploration + computation}
    \item Into \\
    \textit{Solution = data + computation (x100)}
  \end{itemize}
\end{frame}

% \begin{frame}{Automatic Machine Learning}
% \begin{center}
%   \scalebox{0.9}{\includegraphics{figure_man/autodl.png}}
%   \end{center}
% \end{frame}

\begin{frame}{Automatic Machine Learning}
  
  \textbf{Not about automating data scientists}
  \vspace{3mm}
    \begin{itemize}
    \item Efficient exploration of techniques
    \begin{itemize}
    \item Automate the tedious aspects (inner loop)
    \item Make every data scientist a super data scientist
  \end{itemize}
    \item Democratisation
    \begin{itemize}
    \item Allow individuals, small companies to use machine
learning effectively (at lower cost)
\item Open source tools and platforms
  \end{itemize}
    \item Data Science
    \begin{itemize}
    \item Better understand algorithms, develop better ones
    \item Self-learning algorithms
  \end{itemize}

  \end{itemize}

\end{frame}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}{Machine Learning Pipelines}
% \begin{center}
%   \includegraphics[width = \textwidth]{figure_man/automl1.png}
%   \end{center}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}{Automating Machine Learning Pipelines}
% \begin{center}
%   \includegraphics[width = \textwidth]{figure_man/automl2.png}
%   \end{center}
% \end{frame}




% \begin{frame}{Expensive Black-Box Optimization}
%   \begin{minipage}{0.33\linewidth}
%   \includegraphics[width = \linewidth]{figure_man/gears.png}
%   \end{minipage}
%   %
%   \begin{minipage}{0.65\linewidth}
%     \begin{align}
%         y &= f(\boldsymbol{x}) \ , \quad f: \mathbb{X} \rightarrow \mathbb{R} \\
%         \boldsymbol{x}^* &= \argmin\limits_{\boldsymbol{x} \in \mathbb{X}} f(\boldsymbol{x})
%     \end{align}
%     \begin{itemize}
%         \item{$y$}, target value
%         \item{$\boldsymbol{x} \in \mathbb{X} \subset \mathbb{R}^d$}, domain
%         \item{$f(\boldsymbol{x})$} function with considerably long runtime
%         \item{Goal:} Find optimum $\boldsymbol{x}^*$
%     \end{itemize}
%   \end{minipage}
% \end{frame}
% 
% \begin{frame}{Sequential model-based optimization}
%     \begin{itemize}
%       \item Setting: Expensive black-box problem $f: x \rightarrow \mathbb{R} = min!$ 
%       \item Classical problem: Computer simulation with a bunch of control parameters and 
%         performance output; or algorithmic performance on 1 or more problem instances; 
%         we often optimize ML pipelines
%       \item Idea: Let's approximate $f$ via regression!
%     \end{itemize}
%   \begin{block}{Generic MBO Pseudo Code}
%     \begin{itemize}
%       \item \small{Create initial space filling design and evaluate with $f$
%       \item In each iteration:}
%         \begin{itemize}
%           \item \small{Fit regression model on all evaluated points to \\ 
%                 predict \fhx~and uncertainty \shx
%           \item Propose point via infill criterion
%             \[ \operatorname{EI}(x)\uparrow \;\Longleftrightarrow\; \fhx \downarrow \; \wedge \; \shx \uparrow \]
%           \item Evaluate proposed point and add to design
%           \item EGO proposes kriging (aka Gaussian Process) and EI\\
%             \textit{Jones 1998, Efficient Global Opt. of Exp. Black-Box Functions}}
%         \end{itemize}
%     \end{itemize}
%   \end{block}
% \end{frame}

% \begin{frame}{Latin Hypercube Designs}
% \begin{figure}
% \centering
% \includegraphics[height = 4cm]{figure_man/initdes.png}
% \end{figure}
% \begin{itemize}
% \item Initial design to train first regression model
% \item Not too small, not too large
% \item LHS / maximin designs: Min dist between points is maximized
% \item But: Type of design usually has not the largest effect on MBO, and unequal distances between points
%   could even be beneficial
% \end{itemize}
% \end{frame}

% \begin{frame}[fragile]{R package ParamHelpers}

%   \begin{itemize}
%     \item R package ParamHelpers contains mini-language for param spaces
%       \url{https://github.com/berndbischl/ParamHelpers}
%     \item Many param types: num, int, cat, bool, vectors, subordinate
%     \item Can construct a wide variety of different designs, for all param types
%     \item Arbitrary value-trafos to work on e.g. logscale
%     \item Many further operations to program on params and sets  
%   \end{itemize}
 
%   \begin{lstlisting}
%   par.set = makeParamSet(
%     makeDiscreteParam("kernel",
%       values=c("vanilladot", "rbfdot")),
%     makeNumericParam("C", lower=-10, upper=1,
%       trafo=function(x) 2^x),
%     makeNumericParam("sigma", lower=-10, upper=10,
%       trafo=function(x) 2^x,
%       requires=quote(kernel=="rbfdot"))
%   )
%   des = generateDesign(n=10, par.set=par.set)
%   \end{lstlisting}
% \end{frame}


% \begin{frame}{Kriging and local uncertainty prediction}
 
% Model: Zero-mean GP $Y(x)$ with const. trend and cov. kernel $k_\theta(x_1, x_2)$.
% \begin{itemize}
% \item $\vecy = (y_1, \ldots, y_n)^T$, $\matK = (k(\vecx_i, \vecx_j))_{i,j=1,\ldots,n}$
% \item $\kstarx = (k(\vecx_1, \vecx), \ldots, k(\vecx_n, \vecx))^T$
% \item $\muh = \vecone^T \matK^{-1} \vecy / \vecone^T \matK^{-1} \vecone$ (BLUE)
% \item Prediction: $ \fhx = E[ Y(x) | Y(x_i) = y_i, i=1, \ldots, n ] = $ \\
% $\hat{\mu} + \textbf{k}_n(x)^T K^{-1} (\textbf{y} - \hat{\mu} \textbf{1})$
% \item Uncertainty: $\vhx = Var[ Y(x) | Y(x_i) = y_i, i=1, \ldots, n ] = $\\
% $ \sigma^{2} - \textbf{k}^T_n(x) K^{-1} \textbf{k}_n(x) + \frac{(1 - \textbf{1}^T K^{-1} \textbf{k}^T_n(x))^2}{\textbf{1}^T K^{-1} \textbf{1}}$
% \end{itemize}
 
 
% \begin{figure}
% \centering
% \includegraphics[width = 9cm, height = 4cm]{figure_man/Grafik2}
% \end{figure}
 
 
% \end{frame}

% \begin{frame}{Kriging / GP is a spatial model}
%   \begin{itemize}
%     \item Correlation between outcomes $(y_1, y_2)$ depends on dist of $x_1, x_2$\\
%       E.g. Gaussian covar kernel $k(x_1, x_2) = exp(\frac{-||x_1-x_2||}{2\sigma})$
%     \item Useful smoothness assumption for optimization
%     \item Posterior uncertainty at new $x$ increases with dist to design points  
%     \item Allows to enforce exploration
%     \end{itemize}
%     \begin{figure}
%       \centering
%       \includegraphics[height = 5cm]{figure_man/gp.png}
%     \end{figure}
%   \end{frame}

%   \begin{frame}{Infill Criteria: Expected Improvement}
% \begin{itemize}
% \item Define improvement at $x$ over best visited point 
%       with $y=f_{min}$  as random variable
%       $I(x) = |f_{min} - Y(x)|^+ $ 
% \item For kriging $Y(x) \sim N(\fhx, \vhx)$ (given $x=x$)
% \item Now define $EI(x) = E[ I(x)| x = x ]$
% \item Expectation is integral over normal density starting at $f_{min}$
% \item Alternative: Lower confidence bound (LCB) $\fhx - \lambda\shx$
% \end{itemize}
% Result: $ EI(x) = \left( f_{min} - \fhx \right) \Phi \left( \frac{f_{min} - \fhx)}{\shx} \right) +
%          \shx \phi \left( \frac{f_{min} - \fhx}{\shx} \right) $
% %\[
% %EI(\hat{y}, \hat{s}, f_{min}) = \int^{\infty}_{-\infty} I(y, f_{min})\underbrace{\Phi_{(\hat{y},\hat{s})}(y)}_{PDF(y)} dy = 				\int^{\int_{min}}_{-\infty} (f_{min} - y)\Phi_{(\hat{y},\hat{s})}(y) dy
% %\]		
% \begin{figure}[b]
% \includegraphics[width = \textwidth, height = 3.5cm]{figure_man/Grafik3}
% \end{figure}
% \end{frame}

% \begin{frame}{Focussearch}
%     \begin{itemize}
%       \item EI optimization is multimodal and not that simple
%       \item But objective is now cheap to evaluate
%       \item Many different algorithms exist, from gradient-based methods with restarts to 
%         evolutionary algorithms
%       \item We use an iterated, focusing random search coined \enquote{focus search}
%       \item In each iteration a random search is performed
%       \item We then shrink the constraints of the feasible region towards the best point in the current 
%         iteration (focusing) and iterate, to enforce local convergence
%       \item Whole process is restarted a few times
%       \item Works also for categorical and hierarchical params
%     \end{itemize}
% \end{frame}


% \begin{frame}
%   \begin{figure}[H]
%   \centering %page 1,10
%   \only<1>{\includegraphics[page=1, width=\linewidth]{figure_man/mbo-example0-1.pdf}}
%   \only<2>{\includegraphics[page=1, width=\linewidth]{figure_man/mbo-example1-1.pdf}}
%   \only<3>{\includegraphics[page=1, width=\linewidth]{figure_man/mbo-example2-1.pdf}}
%   \only<4>{\includegraphics[page=1, width=\linewidth]{figure_man/mbo-example3-1.pdf}}
%   \only<5>{\includegraphics[page=1, width=\linewidth]{figure_man/mbo-example4-1.pdf}}
%   \only<6>{\includegraphics[page=1, width=\linewidth]{figure_man/mbo-example20-1.pdf}}
%  \end{figure}
% \end{frame}



% \begin{frame}{mlrMBO: Model-Based Optimization Toolbox}
% \begin{minipage}{0.4\linewidth}
%     \begin{itemize}
%       \item \small{Any regression from mlr
%       \item Arbitrary infill
%       \item Single - or multi-crit
%       \item Multi-point proposal
%       \item Via parallelMap and batchtools
%         runs on many parallel backends and clusters
%       \item Algorithm configuration
%       \item Active research}
%     \end{itemize}
% 
% \end{minipage}
% \begin{minipage}{0.55\linewidth}
%     \includegraphics[width = \textwidth]{figure_man/mlrMBO1.pdf}
% \end{minipage}
% \begin{center}
%     \begin{itemize}
%       \item \small{mlr:
%         \url{https://github.com/mlr-org/mlr}
%       \item mlrMBO:
%         \url{https://github.com/mlr-org/mlrMBO}
%       \item mlrMBO Paper on arXiv:
%         \url{https://arxiv.org/abs/1703.03373}}
%     \end{itemize}
% \end{center}
% \end{frame}


% \begin{frame}
% \frametitle{When to use mlrMBO?}
% \setlength\tabcolsep{1pt}
% \begin{columns}
% \begin{column}{0.6\textwidth}
% \begin{tabular}{rl}
% % & \texttt{optim(par, $f(x)$)} is not enough! \\
% Answer: When & $f(\mathbf x)$ is \emph{expensive}. \\
% %& \light{($\rightarrow$ that's why GAs won't work!)} \\
% & $f(\mathbf x)$ is not convex. \\
% & $f(\mathbf x)$ is noisy. \\
% & $\mathbf x$ is not only numeric.
% \end{tabular} \\
% \vspace{1.5cm}
% \begin{tabular}{ll}
% \color{red} \xmark & space filling search methods \\
% \color{red} \xmark & quasi-newton algorithms \\
% \color{red} \xmark & evolutionary  algorithms \\
% \color{green} \cmark & model-based optimization (mlrMBO)
% \end{tabular}
% \end{column}
% \begin{column}{0.39\textwidth}
%   \begin{figure}
% \centering %page 1,10
% \includegraphics[page=1, width=0.75\linewidth, trim =5.5cm 2cm 4cm 1cm]{figure_man/plot_when_to_use_mbo-1.pdf} \\
% \includegraphics[page=1, width=0.75\linewidth, trim =5.5cm 2cm 4cm 1cm]{figure_man/plot_when_to_use_mbo-2.pdf}
% \end{figure}
% \end{column}
% \end{columns}

% %\begin{minipage}{0.7\linewidth}

% %\end{minipage} %
% %\begin{minipage}{0.25\linewidth}

% %\end{minipage}
% \end{frame}

\begin{vbframe}{Hyperparameter Tuning}
  
    
  \begin{itemize}
    \item Many parameters or decisions for an ML algorithm are not decided by the (usually loss-minimizing) fitting procedure.
    \item Our goal is to optimize these w.r.t. the estimated prediction error (often this implies an independent test set), or by cross-validation.
    \item The same applies to preprocessing, feature construction and other model-relevant operations. In general we might be interested in optimizing a machine learning "pipeline".
    \end{itemize}
    
    \begin{center}
      \scalebox{0.7}{\includegraphics{figure_man/automl2.png}}
    \end{center}
  \framebreak
  
  \textbf{Model parameters vs Hyperparameters}:
  
    \begin{itemize}
      \item Model parameters are optimized during training.
      \item In a simple regression model $y = \mathbf{\thetab}^T \xb$, the model parameter $\mathbf{\thetab}$ is learned from the training set.
      \item Hyperparameters are values that must be specified outside of the training phase and need to be set according to the problem.
      \item Simple linear regression doesn't have hyperparameters, but variants do :
        \begin{itemize}
          \item The type of the regularization (e.g. Lasso or Ridge regression)
          \item Regularization parameter $\lambda$ that controls the size of $\mathbf{\thetab}$.
        \end{itemize}
    \end{itemize}
    
  \framebreak
  
  \textbf{What the hyperparameter tuning problem consists of}:
    \begin{itemize}
      \item The learning method (or are there actually several?)
      \item The performance measure, as determined by the application.\\
          More challenging if we have to deviate from standard cases like accuracy or MSE. In general, we could be interested in multiple
          measures at once.
      \item Resampling procedure for measuring the performance. How do we choose it?
      \item The hyperparameters and their regions-of-interest under consideration for tuning.
    \end{itemize}
  \framebreak
  
  \framebreak
  
  \begin{itemize}
    \item For a learning algorithm $\mathcal{A}$ with $n$ hyperparameters, the hyperparameter \textbf{configuration space} is:
      $$\bm{\Lambda}=\Lambda_{1} \times \Lambda_{2} \times \ldots \Lambda_{n}$$
      where $\Lambda_{i}$ is the domain of the $i$-th hyperparameter.
    \item A vector in this configuration space is denoted as $\bm{\lambda} \in \bm{\Lambda}$ and $\mathcal{A}_{\bm{\lambda}}$ is the algorithm with hyperparameters set to $\bm{\lambda}$.
    \item The domain can be continuous, discrete or categorical.
    \item For practical reasons, the domain of a continuous or integer-valued hyperparameter is typically bounded.
    \item Additionally, some hyperparameters may only need to be specified if another hyperparameter (or combination of hyperparameters) takes on a certain value.
    
    \framebreak
    
    
    \item Given a dataset $\mathcal{D}$, the goal of hyperparameter tuning is to find :
      $$
    \bm{\lambda}^{*}=\underset{\bm{\lambda} \in \bm{\Lambda}}{\operatorname{argmin}} \mathbb{E}_{\left(D_{\text {train}}, D_{\text {val}}\right) \sim \mathcal{D}} L_{\text{val}}\left(\mathcal{L}, \mathcal{A}_{\bm{\lambda}}, D_{\text {train}}, D_{\text {val}}\right)
      $$
      where $L_{\text{val}}\left(\mathcal{L}, \mathcal{A}_{\bm{\lambda}}, D_{\text {train}}, D_{\text {val}}\right)$ is the loss of a model learned by $\mathcal{A}_{\bm{\lambda}}$ when trained on $D_{\text{train}}$ and evaluated on $D_{\text{val}}$.
    \item Validation may be performed using hold-out validation or cross-validation for a user-specified loss function $\mathcal{L}$.
   % \item Note that for a given hyperparameter configuration $\bm{\Lambda}$, there is an
    
    \framebreak
    \small{
    \item Hyperparameter tuning can also be framed as a bi-level optimization problem. For hold-out validation, this is :
      \begin{gather*}
        \min_{\bm{\lambda} \in \bm{\Lambda}} L_{\text{val}} (f(\bm{\lambda},\thetab_{\bm{\lambda}}^*), D_{\text {val}}) \\
        s.t.\text{  } f(\bm{\lambda},\thetab_{\bm{\lambda}}^* ) = \min_{\thetab_{\bm{\lambda}}} \in \thetab_{\bm{\lambda}} L_{\text{train}} (f(\bm{\lambda},\thetab_{\bm{\lambda}}), D_{\text {train}})
      \end{gather*}
      where, for a fixed $\bm{\lambda}$, $\thetab_{\bm{\lambda}}$ parametrizes the models $f(\bm{\lambda},\thetab_{\bm{\lambda}})$ in the hypothesis set and $L_{\text{val}}$ and $L_{\text{train}}$ are the losses on the validation and training sets, respectively.
      \item Therefore, the distinction between parameters $\thetab_{\bm{\lambda}}$ and hyperparameters $\bm{\lambda}$ results in a hierarchy of optimization problems.
      \item The first level problem, which corresponds to empirical risk minimization (ERM), can be seen as a subroutine called by the second level problem.
      \item However, aside from the fact that the search space (parameterized by $\bm{\lambda}$ vs $\thetab_{\bm{\lambda}}$), loss function and data used differ, there is no fundamental difference in the type of operation that is performed at the two levels.
      
      }
      
      % \item We are already familiar with ERM. The focus of this chapter is the second-level problem.
      % \item However, aside from the fact that the search space ($\lambda$ vs $\theta$), loss function and data used differ, there is no fundamental difference in the type of operation that is performed at the two levels.
  \end{itemize}

    \framebreak
  
  
  Possible scenarios:
    \begin{itemize} 
      \item Stable performance:\\
    The algorithm is really insensitive w.r.t. to changes of a parameter, so we don't really have to do anything as long as we stay in a broad range of reasonable values.
      \item Constant default:\\
    We can benchmark the algorithm across a broad range of data sets and scenarios and try to find a constant value that works well
in many different situations. Quite optimistic?
      \item Dynamic (heuristic) default:\\
  We can benchmark the algorithm across a broad range of datasets and scenarios and try to find an easily computable function
that sets the parameter in a data dependent way, e.g. \texttt{mtry}= p/3 or defining kernel widths of RBF SVMs w.r.t. the pairwise distance distribution of training data points.\\
How to construct or learn that heuristic function, though?
    
  \framebreak
    \item Ex-post determination: Set the parameter by extracting more info from the fitted model. E.g. early stopping in boosting or ntree for a random forest (does OOB error increase or stagnate?). Some regularized linear models also allow cheap full-path computation for all possible $\lambda$ values.
  \end{itemize}
  
  \framebreak
  
  \begin{itemize}
%    \item Lots of literature exists for models, far less on efficient tuning.
    \item The optimization problem tuning represents is usually derivative-free: we can only determine the quality of selected
      (hyperparameter) configurations (black-box problem), not the direction in which to change them.
    \item Our optimization problem is stochastic in principle. We want to optimize expected performance and use resampling.
    \item Evaluation of our target function will probably take quite some time and effort. Typically, we are cross-validating complex models on large data sets.
    \item Categorical and especially dependent parameters complicate the problem (... a lot).
    \item For difficult problems, parallelize the computation.
  \end{itemize}
  
\end{vbframe}

  \begin{frame}{Automatic Machine Learning: Techniques}
    \begin{itemize}
      \item \textbf{Bayesian Optimization:} Intelligently optimize pipelines/ architectures by iteratively choosing better ones
  \item \textbf{Genetic algorithms:} Evolve pipelines/architectures to work better for a given application
  \item \textbf{Meta-learning:} learn from previous applications to predict useful pipelines/ architectures for new problems
  \item \textbf{Transfer Learning:} train models on one problem, then transfer (parts) of good solutions to solve new problems.
  \item \textbf{Reinforcement Learning:} Train many models, use performance as "reward" for certain approaches
  \item \textbf{Combinations of all of these}
    \end{itemize}
  \end{frame}

\section{Hyperparameter Tuning using SMBO}


\begin{vbframe}{Hyperparameter Tuning using SMBO}
  
  
  \begin{itemize}
    \item Objective function is resampled performance measure
  % \item Chain mlr operations (e.g. feature filter + ML model) 
    % so we can jointly optimize complex systems 
    \item (Hyper-)Parameter space  $\bm{\lambda} \in \bm{\Lambda}$\\
          might be discrete and dependent / hierarchical
    \item No derivative for the objective, black-box
    \item Objective is stochastic / noisy
    \item Objective is expensive to evaluate
    \item In general we face a problem of algorithm configuration:
    \item $\leadsto$ \textcolor{blue}{Usual approaches: racing or model-based / bayesian optimization}
\end{itemize}
  
  \framebreak
    \begin{itemize}
      \item Initial design for configurations: Latin Hypercube principle can be extended, or just use random
      \item Focus search: Can be (easily) extended, as it is based on random search. To zoom in for categorical hyperparameters, for each
hyperparameter we randomly drop a category which is not used in the currently best configuration.
      \item Use surrogate models to approximate expected loss as a function of hyperparameters
      \item Sequential MBO: use surrogate model to propose a new, promising hyperparameter configuration by optimizing a so-called
infill criterion or acquisition function.
      \item New configurations proposed should either have a good expected objective value or high potential to improve the quality of the surrogate model.
    \end{itemize}
\end{vbframe}

% \begin{frame} {Evolutionary algorithms}
%   \begin{figure}
%     \centering
%       \scalebox{0.75}{\includegraphics{figure_man/evo_1.png}}
%   \end{figure}
%   
%   "Breed" new configurations using:
%   \begin{itemize}
%     \item Selection
%     \item Recombination
%     \item Mutation
%   \end{itemize}
% \end{frame}

\begin{frame} {Complex Parameter Space}
\textbf{Types of hyperparameters:}
    \begin{itemize}
    \item Numerical parameters (real valued / integers)
      \begin{itemize}
        \item Cost parameter C of an SVM
        \item Depth or minimal node size of a tree
      \end{itemize}
    \item Categorical parameters:
      \begin{itemize}
        \item Split criteria for trees: Gini? Entropy?
        \item SVM kernels: Gaussian? Polynomial? Laplace? ...?
        \item type of spline basis: B-spline? Thin plate? Radial basis functions?
      \end{itemize}
    \item Dependent or subordinate parameters:
      \begin{itemize}
        \item Kernel parameters (specific to the kernel)
        \item spline basis parameters: degree, penalty order, dimension, ...
      \end{itemize}
    \end{itemize}
    
\end{frame}
\begin{frame}
\frametitle{Complex Parameter Space}
\vspace{-0.5cm}
\begin{figure}[t]
\center 
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick,circ/.style={circle,draw,font=\sffamily\scriptsize},
                    rect/.style={rectangle,draw,font=\sffamily\scriptsize}]
  \node[rect] (20) at (3, 4.5) {Parameter Set};
  \node[circ] (18) at (0, 3.5) {cl.weights};
  \node[circ]  (1) at (6, 3.5) {learner};
  \node[rect] (19) at (-0.5, 2) {$2^{[-7,...,7)}$};
  \node[rect]  (2) at (2, 2) {randomForest};
  \node[rect]  (3) at (4, 2) {L2 LogReg};
  \node[rect]  (4) at(6, 2) {svm};
  \node[circ]  (5) at (0, 0.5) {mtry};
  \node[circ]  (6) at (2, 0.5) {nodesize};
  \node[circ]  (7) at (4, 0.5) {cost};
  \node[circ]  (8) at (6, 0.5) {cost};
  \node[circ]  (9) at(8, 2) {kernel};
  \node[rect] (10) at (8.5, 1){radial};
  \node[rect] (17) at (7, 1){linear};
  \node[circ] (11) at(8, 0) {$\gamma$};
  \node[rect] (12) at (-0.5, -1) {$\{0.1p,..., 0.9p\}$};
  \node[rect] (13) at (2, -1) {$\{1,..., 0.5n\}$};
  \node[rect] (14) at (4, -1) {$2^{[-15, 15]}$};
  \node[rect] (15) at (6, -1) {$2^{[-15, 15]}$};
  \node[rect] (16) at (8, -1) {$2^{[-15, 15]}$};
  \path[every node/.style={font=\sffamily\small}]
    (1) edge node {}(2)
        edge node {}(3)
        edge node {}(4)
    (2) edge node {}(5)
        edge node {}(6)
    (3) edge node {}(7)
    (4) edge node {}(8)
        edge node {}(9)
    (5) edge node {}(12)
    (6) edge node {}(13)
    (7) edge node {}(14)
    (8) edge node {}(15)
    (9) edge node {}(10)
        edge node {}(17)
    (10) edge node {}(11)
    (11) edge node {}(16)
    (18) edge node {}(19)
    (20) edge node {}(1)
         edge node {}(18);
\end{tikzpicture}
\end{figure}
    \small{Problems: Categorical parameters, hierarchical parameter space, cyclical parameter space...}

\end{frame}

\begin{frame} {Complex Parameter Space}

  \begin{figure}
    \centering
      \scalebox{0.93}{\includegraphics{figure_man/complex_dl.png}}
  \end{figure}
  
  \small{Problems: Categorical parameters, hierarchical parameter space, cyclical parameter space...}
  
\end{frame}

% \begin{frame}{Extensions for mixed parameter spaces: Initial design}
% 
% \begin{itemize}
%   \item In theory: Every DoE technique could be used.
%   \item Popular choice: Latin Hypercube Sampling (LHS).
%   \item Unfortunately: LHS is defined for only numerical parameter spaces.
% \end{itemize}
% 
% \vspace{0.5cm}
% 
% \textbf{LHS for categorical parameters}
% \begin{itemize}
%   \item Map the parameter values to uniform intervals in $[0, 1]$.
% \end{itemize}
% 
% 
% \vspace{0.5cm}
% 
% \textbf{LHS for dependent parameters}:\\
% \textbf{Idea}: Oversampling
%   \begin{itemize}
%     \item Generate many more valid point as required. \\
%     \item Remove point with smallest distance until design is small enough. \\
%     \item Gower distance for mixed parameter spaces.
%   \end{itemize}
% \end{frame}

% \begin{frame}{Surrogate models}
% 
% %   \begin{itemize}
% %   \item Initial design: LHS principle can be extended, or just use random
% %   \item Focus search: Can be (easily) extended, as it is based on random search.
% %     To zoom in for categorical parameters we randomly drop a category for each param
% %     which is not present in the currently best configuration.
% % \end{itemize}
% 
%  
%   \begin{itemize}
%   \item Few approaches for GPs with categorical params exist (usually with new covar kernels), not very established
% \item Alternative: Random regression forest (mlrMBO, SMAC)
%   \item Estimate uncertainty / confidence interval for mean response by
%   efficient bootstrap technique\footnote{Sexton et al, \enquote{Standard errors for bagged and random forest estimators, 2009.}}, or jackknife, so we can define $EI(x)$ for the RF
%   \item Dependent params in mlrMBO: Imputation:
%   \begin{itemize}
%   \item categorical parameters: Introduce new class
%   \item numerical parameters: Impute 2 times the maximum
%   \end{itemize}
%   \item Many of the current techniques to handle these problems are (from a theoretical standpoint) somewhat crude
%   \end{itemize}
% \end{frame}

\begin{frame}{Surrogate models}
\begin{itemize}
\item GPs defined for purely numerical spaces.
  \begin{itemize}
    \item Few approaches for GPs with categorical params exist (usually with new covar kernels), not very established.
  \end{itemize}
  \item Random regression forest (mlrMBO, SMAC)
    \begin{itemize}
      \item Estimate uncertainty / confidence interval for mean response by
  efficient bootstrap technique\footnote{Sexton et al, \enquote{Standard errors for bagged and random forest estimators, 2009.}}, or jackknife, so we can define $EI(x)$ for the RF
      \item Inactive dependent parameters in mlrMBO handled as separate class (categorical) or set to twice their maximal value.
    \end{itemize}
  \item Many of the current techniques to handle these problems are (from a theoretical standpoint) somewhat crude.
  \end{itemize}
\end{frame}



\begin{frame}{Hyperparameter Tuning - Example}
  \begin{minipage}{0.33\linewidth}
  \includegraphics[width = \linewidth]{figure_man/gears2.png}
  \end{minipage}
  %
  \begin{minipage}{0.65\linewidth}
  \textbf{mlrMBO} can be used for:
  \begin{itemize}
  \item Expensive black-box optimization
  \item Hyperparameter tuning for machine learning methods
  \item Machine learning pipeline configuration
  \item Algorithm configuration
  \item ...
  \end{itemize}
  \end{minipage}
\end{frame}

\begin{frame}{Hyperparameter Tuning - Example}
 \begin{itemize}
    \item Still common practice: grid search\\
    For a SVM it might look like:
    \begin{itemize}
      \item $C \in (2^{-12}, 2^{-10}, 2^{-8}, \ldots, 2^{8}, 2^{10}, 2^{12})$
      \item $\gamma \in (2^{-12}, 2^{-10}, 2^{-8}, \ldots, 2^{8}, 2^{10}, 2^{12})$
      \item Evaluate all $13^2 = 169$ combinations $C \times \gamma$
    \end{itemize}
    \item Bad because:
    \begin{itemize}
      \item optimum might be "off the grid"
      \item lots of evaluations in bad areas
      \item lots of costly evaluations
    \end{itemize}
    \item How bad?
  \end{itemize}
\end{frame}

\begin{vbframe}{Hyperparameter Tuning - Example}
\begin{center}
\includegraphics[width=0.5\textwidth]{figure_man/grid1.png}
\end{center}
\begin{itemize}
\item Because of budget restrictions grid might even be smaller!
\item Unpromising area quite big!
\item Lots of costly evaluations!
\end{itemize}
With \textbf{mlrMBO} it is not hard to do it better!


%More interesting applications to time-series regression and cost-sensitive classification\footnote{Koch, Bischl et al:\textit{Tuning and evolution of support vector kernels}, EI 2012}

\framebreak

\vfill
\includegraphics[width=\textwidth]{figure_man/res1.png}
\vfill
\end{vbframe}

\begin{frame}{Hyperparameter Tuning - Example}
\vfill
\includegraphics[width=\textwidth]{figure_man/res2.png}
\vfill
\end{frame}



\begin{frame}{HPOlib}
\begin{itemize}
\item HPOlib is a set of standard benchmarks for hyperparameter optimizer
\item Allows comparison with
\begin{itemize}
\item Spearmint
\item SMAC
\item Hyperopt (TPE)
\end{itemize}
\item Benchmarks:
\begin{itemize}
\item Numeric test functions
\item Numeric machine learning problems (lda, SVM, logistic regression)
\item Deep neural networks and deep belief networks with $15$ and $35$ parameters.
\end{itemize}
\item For benchmarks with discrete and dependent parameters (hpnnet, hpdbnet) a random forest with standard error estimation is used.
\end{itemize}
\end{frame}

\begin{frame}{MBO: HPOlib}
\begin{center}
  \scalebox{0.95}{\includegraphics{figure_man/hpolib-1.pdf}}
\end{center}
\end{frame}

\section{Hyperband}

\begin{frame} {Learning curves}
  \begin{itemize}
    \item It is extremely expensive to train complex models such as deep neural networks on large datasets.
    \item In such a scenario, evaluating even a single hyperparameter configuration can take many hours/days.
    \item For many configurations, it might be clear early on that further training is not likely to significantly improve the performance.
    \item More importantly, the relative ordering of configurations (for a given set) can also become evident early on.
    \item Therefore, fully training every single configuration can be quite wasteful.
  \end{itemize}
\end{frame}

\begin{frame} {Learning curves}
    \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{figure_man/iteration.png}}
      \tiny{\\credit : Ameet Talwalkar}
  \end{figure}
    
    \textbf{Main idea} : Terminate training for configurations that aren't promising and instead allocate more resources to training configurations that are likely to perform better.
\end{frame}

\begin{frame} {Successive-Halving}
  \begin{itemize}
    \item To "weed out" poor configurations during training, one simple approach is \textbf{Successive-Halving}.
    \item Given an initial set of configurations to evaluate :
      \begin{itemize}
        \item Train all configurations for a small initial budget.
        \item Remove the half that performed worst and then double the budget.
        \item Continue training the remaining configurations until this new budget is exhausted.
        \item Again, remove the half that performed worst and double the budget.
        \item Repeat until only a single configuration remains...
      \end{itemize}
  \end{itemize}
\end{frame}

% \begin{frame} {Multi-Armed Bandits}
%   \begin{itemize}
%     \item Imagine a gambler in a casino who wants to maximize his profit.
%     \item He is free to choose between $n$ different slot machines and has a fixed budget $B$.
%     \item He begins to spend $b$ units of budget for each machine, such that $b \cdot n \ll B$
%     \item While he loses some of his budget $B$, he also gains a bit of information concerning each machine's return rates.
%     \lz
%     \item How should a gambler allocate his remaining budget on bandits?
%   \end{itemize}
% \end{frame}

\begin{frame} {Successive-halving}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{figure_man/hyperband3.png}}
      \tiny{\\credit: automl.org}
      \caption{\footnotesize{Illustration of successive halving for eight algorithms/configurations. After evaluating all algorithms on 1/8 of the total budget, half of them are dropped and the budget given to the remaining algorithms is doubled.}}
  \end{figure}
\end{frame}

\begin{frame} {Successive-halving - Example}
Budget $B = 1000$ and $n = 100$ configurations/"arms" : 
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{figure_man/hyperband1.png}}
  \end{figure}
\end{frame}

\begin{frame} {The $n$ versus $B/n$ problem}
  \begin{itemize}
    \item A major drawback of Successive-Halving is that the user has to specify the number of configurations, $n$, beforehand.
    \item Ideally, we would like to explore the configuration space and evaluate a large number of configurations.
    \item However, for a fixed budget, as $n$ increases, the budget $B/n$ that can be spent on any single one of those configurations decreases.
    \item Therefore, the user can either :
      \begin{itemize}
        \item train a large number of configuration for a small number of iterations, or,
        \item train a small number of configurations for a large number of iterations.
      \end{itemize}
    \item As $B/n$ decreases, it becomes harder to "pick out" the most promising configurations from a limited number of training iterations.
  \end{itemize}
\end{frame}

\begin{frame} {The $n$ versus $B/n$ problem}
In general, the relative ordering (in terms of validation loss) of two arbitrary neural networks can vary during training:
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{figure_man/val_err.png}}
  \end{figure}
  
In the early stages of training, Model B looks more promising than Model A even though the latter ultimately performs better. Therefore, we must be careful not to prematurely terminate training for Model A.

\end{frame}

\begin{frame} {Hyperband in a nutshell}
  \begin{itemize}
    \item Hyperband tries to tackle the $n$ versus $B/n$ problem by considering \textbf{several} values of $n$ for a fixed $B$.
    \item Each distinct $n$ corresponds to a different "bracket".
    \item In every bracket, a new set of $n$ random configurations is sampled and then solved by a mutation of the Successive Halving algorithm.
    \item Therefore, it represents a "hedging strategy" which tries to \textit{simultaneously} minimize the risks of under-exploration (small $n$) and premature elimination (small $B/n$).
    \item In simple terms, it is a clever way to "have your cake and eat it too".
  \end{itemize}
\end{frame}

\begin{frame} {Hyperband in a nutshell} 
  \begin{itemize}
    \item Hyperband requires the user to specify two parameters :
      \begin{itemize}
        \item $R$ corresponds to the maximum budget that can be spent on any specific configuration.
        \item $\eta$ corresponds to the proportion of configurations that are discarded in each round of Successive Halving.
      \end{itemize}
    \item For example, if $\eta$ = 3, then only a third of the configurations survive a given round.
    \item Based on these two parameters, the number of brackets that are tried is $s_{\text{max}} + 1$, where, $s_{\text{max}} = \lfloor \log_{\eta}R \rfloor$.
  \end{itemize}
\end{frame}

\begin{frame} {Hyperband - Example}
Hyperband for $R = 81$ and $\eta = 3$
  \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{figure_man/hyperband2.png}}
  \end{figure}
  
  \begin{itemize}
    \item $n_i$ and $r_i$ are the number of configurations and the budget-per-configuration, respectively.
    \item The bracket corresponding to $s = 4$ is designed to maximize exploration. 
  \end{itemize}
  
\end{frame}

\begin{frame} {Hyperband - Example}
Hyperband for $R = 81$ and $\eta = 3$
  \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{figure_man/hyperband2.png}}
  \end{figure}
  \begin{itemize}
    \item Within each bracket, Successive Halving is performed on a randomly sampled set of $n$ configurations.
    \item Note : Each bracket uses approximately the same amount of resources.
  \end{itemize}
  
\end{frame}

\begin{frame} {Hyperband - Pseudocode}
  \begin{figure}
    \centering
      \scalebox{1.1}{\includegraphics{figure_man/hyp_pseudo.png}}
      \tiny{\\source : Li et al. (2016)}
  \end{figure}
\end{frame}

\begin{vbframe} {Bayesian Optimization \& Hyperband (BOHB)}
  
  \begin{itemize}
    \item In each bracket of Hyperband, the configurations are sampled completely at random.
    \item Model-based optimization, on the other hand, learns from the performance of previously sampled configurations.
    \item However, a downside of model-based optimization is that evaluations can be very expensive for complex models such as neural networks.
    \item BOHB is an algorithm which combines the two approaches.
    \item The budgets are chosen according to Hyperband but the configurations for a run of successive halving are sampled from a surrogate model.
    \item BOHB outperforms both Bayesian Optimization and Hyperband on a wide range of tasks.
  \end{itemize}
  
  \framebreak
  
  \begin{itemize}
      \item Instead of using a GP as the surrogate model, BOHB uses a non-parametric multi-dimensional Kernel Density Estimator (KDE) which scales better. 
    % \item A GP models $p(y|x)$ where $y$ is the performance amd $x$ is the configuration. By contrast, in the KDE approach, $p(x|y)$ is modeled.
    \item To build such a model, the set of previously evaluated configurations is divided into two groups based on their observed performance $y$. 
    \item A KDE is then fit for each of the two groups:
          \begin{align*}
            l(\lambda) &= p(y < \alpha | \lambda) \;\;\; \text{('good' configurations)} \\
            g(\lambda) &= p(y \geq \alpha | \lambda) \;\;\; \text{('bad' configurations)}
          \end{align*}
          where $\alpha$ is a cutoff value which represents a pre-defined percentile.
    \item It can shown that maximizing Expected Improvement is equivalent to maximizing the ratio
      $\frac {l(\lambda)}{g(\lambda)}$.
  \end{itemize}
  
  \framebreak

  \begin{itemize}
    \item At each iteration of BOHB, the number of configurations to be evaluated is determined according to Hyperband.
    \item Samples are then drawn from the surrogate model and Successive Halving is carried out. 
    \item Note : BOHB draws a constant fraction of the samples uniformly at random to preserve some of the theoretical guarantees of HB.
    \item To sample a configuration from the model, 
      \begin{itemize} 
        \item Step 1 : A fixed number of samples are drawn from $l'(\lambda)$ (which is the same KDE as $l(\lambda)$ but with a different bandwidth designed to encourage exploration).
        \item Step 2 : The sample with the highest ratio $\frac {l(\lambda)}{g(\lambda)}$ is chosen.
      \end{itemize}
    \item Because different configurations may be evaluated on different budgets, BOHB always builds the surrogate model using the largest budget for which sufficiently many observations are available as they tend to be more reliable.
  \end{itemize}
  
  \framebreak
  
  \begin{figure}
    \centering
      \scalebox{0.65}{\includegraphics{figure_man/comparison_1.png}}
      \tiny{\\source: Falkner et al. 2018}
      \caption{\footnotesize{Illustration of typical results obtained for optimizing six hyperparameters of a neural network. The curves give the immediate regret of the best configuration found by 4 methods as a function of time. For small to medium budgets, Hyperband outperforms random search and Bayesian Optimization (BO). For larger budgets, the model-based approach of BO seems to outperform Hyperband. BOHB achieves the best of both worlds.}}
  \end{figure}

\small{BOHB has strong anytime performance (desirable for smaller budgets) and converges quickly to optimal configurations.}

\end{vbframe}

\section{OpenML}

\begin{vbframe}{What is OpenML?}

OpenML is an online platform for sharing and organizing data, ML tasks, algorithms and experiments. Key elements are:
\begin{itemize}
  \item \textbf{Data sets:}
  Find interesting data sets and use them immediately or share your own.
  \item \textbf{Tasks:}
  View ML problems that people are working on or crowdsource your own ML problems.
  \item \textbf{Flows:}
  Use open-source tools (e.g., R, Python, Java, ...) to solve the tasks with different ML algorithms or complex ML pipelines/workflows. Automate drudge work (with bots).
  \item \textbf{Runs/Results:}
  Reproducible, transparent, reusable results of ML experiments, organized for easy analysis and reusability.
\end{itemize}

%Philosophy: Make the key elements of machine learning experiments reproducible, computer-readable and sharable.

\end{vbframe}

\begin{vbframe}{Overview}
%\centering
\begin{figure}
\includegraphics[width=1\textwidth]{figure_man/oml_overview.pdf}
\end{figure}
\end{vbframe}

\begin{vbframe}{Website}
%\centering
OpenML currently has an increasing amount of data, tasks, flows and runs:
\begin{figure}
\includegraphics[width=1.0\textwidth]{figure_man/oml_website.png}
\end{figure}
\end{vbframe}

\begin{vbframe}{Data sets}
%\centering
\textbf{Data sets} can be uploaded (as ARFF) or referenced (via the URL).

The server automatically versions and analyzes the data, and extracts common meta-data.

\begin{center}
  \includegraphics[width = \textwidth]{figure_man/oml_data.png}
  \end{center}
\end{vbframe}

\begin{vbframe}{Task}
%\centering
\textbf{Tasks} define a ML challenge/problem to be solved.

All solutions/results of tasks are organized online, in a realtime overview.

\begin{center}
  \includegraphics[width = 0.9\textwidth]{figure_man/oml_task.png}
\end{center}
\end{vbframe}

\begin{vbframe}{Flow}
%\centering
\textbf{Flows} (workflows, scripts, complex pipelines) can run anywhere (locally) with many integrated tools + APIs (REST, R, Python, Java, ...).

\begin{center}
  \includegraphics[width = 0.9\textwidth]{figure_man/oml_flows.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Run/Results}
%\centering
\textbf{Runs} are the results of ML experiments.

The results are reproducible, link to their associated data, task and flow and are evaluated online based on many performance measures. %can be shared and analyzed online,
\begin{center}
  \includegraphics[width = 0.9\textwidth]{figure_man/oml_run.png}
\end{center}
\end{vbframe}


\begin{vbframe}{What People Currently Do with OpenML}
%\centering
\begin{center}
  \includegraphics[width = 0.8\textwidth]{figure_man/cool_stuff_text.png}
\end{center}
\end{vbframe}

\endlecture
