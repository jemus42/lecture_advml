%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
library(knitr)
library(mlr)
library(ggplot2)
set_parent("../style/preamble.Rnw")
@

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


% \lecturechapter{8}{ROC: Binary classification performance}
% \lecture{Fortgeschrittene Computerintensive Methoden}


% \begin{vbframe}{Notation}
% \begin{itemize}
% \item $\Xspace$ $p$-dim. input space%, usually we assume $\Xspace = \R^p$, but categorical features can occur, too
% \item $\Yspace$: the target space. In this chapter, binary $\Yspace = \lbrace 0, 1 \rbrace$ or multi class $\Yspace = \gset$.
% \item $x = \xvec \in \Xspace$ observation.
% \item A binary classifier is a function $C: \Xspace \longrightarrow \Yspace = \lbrace 0,1\rbrace$.
% \item A binary scoring classifier (or ranker) is a function $f: \Xspace \longrightarrow \R$ (or $[0,1]$) which outputs a score [probability] for the positive class.
% \item $n$ is the total number of observations.
% \item $\nn$ and $\np$ are the number of negative ($y = 0$) and positive ($y = 1$) observations.
% \item $\rn$ and $\rp$ are the fraction of negative and positive observations, respectively.
% \item $\tp$, $\tn$, $\fap$, $\fan$ are the numbers of true or false positives or negatives, respectively. 
% \end{itemize}
% \end{vbframe}



% \begin{vbframe}{Evaluation of Binary Classifiers}

% \begin{itemize}

% \item  Consider a binary classifier $C(x)$, e.g. for cancer prediction (with true label $y$).
%     \begin{itemize}
%       \item   for $C(x) = 1 = \hat{y}$, we predict cancer
%       \item   for $C(x) = 0 = \hat{y}$, we predict no cancer
%     \end{itemize}
% \item  One possible evaluation measure is the \textit{misclassification error} or \textit{error rate} (i.e. the proportion of patients for which $\hat{y} \neq y$).
% \item  Example: If 10 out of 1000 patients are misclassified, the \textit{error rate} would be 1\%.
% \item  In general, lower \textit{error rates} are better.

% \end{itemize}

% \framebreak

% \begin{itemize}
% \item   Using the \textit{error rate} for imbalanced class labels is not suggested.
% \item   Example: Assume that only 0.5\% of 1000 patients have cancer.
%   \begin{itemize}
%     \item Always returning $C(x) = 0 = \hat{y}$ gives an \textit{error rate} of 0.5\%, which sounds good.
%     \item However, we never predict cancer (which is bad).
%   \end{itemize}
% \end{itemize}

% $\Rightarrow$ We need different evaluation metrics that account for imbalancedness and should not (only) trust the \textit{error rate}.

% \end{vbframe}


% \begin{vbframe}{Confusion Matrix}

% The confusion matrix is a $2 \times 2$ contingency table of predictions $\hat{y}$ and true labels $y$.
% Several evaluation metrics can be derived from a confusion matrix:

% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure_man/roc-confusion_matrix.png}
% \end{center}


% \framebreak

% ... and even more:

% \begin{center}
% %\includegraphics[width=0.95\textwidth]{figure_man/roc-confusion-2.png}
% \end{center}
% \href{https://en.wikipedia.org/wiki/F1_score#Diagnostic_testing}{\beamergotobutton{Clickable version/picture source}} $\phantom{blablabla}$
% \href{https://upload.wikimedia.org/wikipedia/commons/0/0e/DiagnosticTesting_Diagram.svg}{\beamergotobutton{Interactive diagram}}


% % \framebreak
% % Terminology:
% %
% % \begin{itemize}
% %
% % \item   \textbf{True positive (TP):} \newline
% %     We predicted "1" and the true class is "1".
% %
% % \item  \textbf{True negative (TN):} \newline
% %     We predicted "0" and the true class is "0".
% %
% % \item  \textbf{False positive (FP):} \newline
% %     We predicted "1" and the true class is "0" (type I error).
% %
% % \item  \textbf{False negative (FN):} \newline
% %     We predicted "0" and the true class is "1" (type II error).
% %
% % \item  \textbf{Positive (pos):} \newline
% %     Fraction of true class labels with "1". %<!-- \np = \sum_i I(y_i = 1)$ and $\nn = \sum_i I(y_i = 0)$-->
% %
% % \item   \textbf{Negative (neg):} \newline
% %     Fraction of true class labels with "0".
% %
% % \end{itemize}
% % %<!-- (see also [Statistical Tests of Significance \& Type I and Type II Errors](http://0agr.ru/wiki/index.php/Statistical_Tests_of_Significance#Type_I_and_Type_II_Errors "wikilink2")) -->
% %
% %
% % \framebreak
% %
% %
% % The following measures can be obtained from the confusion matrix:
% %
% % \begin{itemize}
% % \item   True positive rate (also known as sensitivity or recall)
% %   \begin{itemize}
% %     \item   Fraction of positive observations correctly classified
% %     \item   $\text{tpr} = \cfrac{\text{\#TP}}{\text{\#TP} + \text{\#FN}}$
% %   \end{itemize}
% % \item   False positive rate (also known as fall-out)
% %   \begin{itemize}
% %     \item   Fraction of negative observations incorrectly classified
% %     \item   $\text{fpr} = \cfrac{\text{\#FP}}{\text{\#FP} + \text{\#TN}} = \text{1 - Specificity}$
% %   \end{itemize}
% % \item   Accuracy
% % \item   Misclassification error (or error rate)
% % \item   Positive predictive value (or precision) %<!--, $P = \cfrac{\text{TP}}{\text{TP} + \text{FP}}$ -->
% % \item   Negative predictive value
% % \item   ...
% % \end{itemize}
% % %<!-- -   Support - fraction of positively classified observations
% % %    -   $\text{sup} = \cfrac{\text{TP} + \text{FP}}{N} =
% % %        \cfrac{\text{predicted pos}}{\text{total}}$ -->
% %
% %

% % \end{vbframe}
% %
% % \begin{vbframe}{Accuracy and Misclassification Error}
% %
% % In practice, these are the most widely used metrics
% % \begin{itemize}
% %   \item   Accuracy: $\text{acc} = \cfrac{\text{\#TP} + \text{\#TN}}{N}$
% %
% %   \begin{itemize}
% %     \item fraction of correctly classified observations
% %   \end{itemize}
% %
% %   \item   Error rate: $\text{error} = \cfrac{\text{\#FN} + \text{\#FP}}{N} = 1 - \text{acc}$
% %
% %   \begin{itemize}
% %     \item Fraction of misclassified observations
% %   \end{itemize}
% %
% % \end{itemize}
% % \end{vbframe}


% %
% %
% % \begin{vbframe}{Precision (P) or Positive Predictive Value (PPV)}
% %   $$P = \cfrac{\text{\#TP}}{\text{\# predicted positives}} = \cfrac{\text{\#TP}}{\text{\#TP} + \text{\#FP}}$$
% %
% % \begin{center}
% %   or
% % \end{center}
% % \vspace{-0.75cm}
% %
% %   $$P = \cfrac{pos \cdot tpr}{pos \cdot tpr + neg \cdot fpr} = \cfrac{tpr}{tpr + \tfrac{\rn}{\rp} \cdot fpr}$$
% %
% % \begin{itemize}
% % \item   Interpretation: For all observations that we predicted $\hat{y} = 1$, what fraction actually has $y = 1$?
% % \item   Higher \textit{precision} is better.
% % \end{itemize}
% % \end{vbframe}
% %
% %
% % \begin{vbframe}{Recall (R)}
% %
% % $$R = \cfrac{\text{\#TP}}{\text{\# actual positives}} = \cfrac{\text{\#TP}}{\text{\#TP + \#FN}} = \text{tpr}$$
% %
% % \begin{itemize}
% % \item   Interpretation: For all observations that actually have $y = 1$, what fraction did we correctly detect as $\hat{y} = 1$?
% % \item   Higher \textit{recall} is better.
% % \item   For a classifier that always returns zero (i.e. $\hat{y} = 0$), the \textit{recall} would be zero.
% % \end{itemize}
% %
% \end{vbframe}


% %<!--
% %-   Out of all the people that do actually have %cancer, how much had been identified?
% %-   We don't fail to spot many people that actually %have cancer.
% % -->

% %<!-- The [F Measure](http://0agr.ru/wiki/index.php%/F_Measure#F_Measure "wikilink3") is a combination %of [Precision and
% %Recall](http://0agr.ru/wiki/index.php/Precision_and_%Recall "wikilink4") -->

% \begin{vbframe}{F-Measure}
% %<!--
% %$P$ and $R$ don't make sense in the isolation from each other

% %-   higher level of $\rho$ may be obtained by lowering $\pi$ and
% %    vice versa

% %Suppose we have a ranking classifier that produces some score (e.g. a probability) for $\mathbf{x}$:

% %-   we decide whether to classify it $\hat{y} = 1$ or $\hat{y} = 0$ based on some threshold parameter $\tau$.
% %-   varying $\tau$, will lead to different values for precision and recall.
% %-   improving recall yields a worse precision and vice versa.
% %-   combine $P$ and $R$ into one measure (also see [ROC Analysis])

% %$F_{\beta} = \cfrac{(\beta^2 + 1) P \, R}{\beta^2 \, %P + R}$

% %-   $\beta$ is the tradeoff between $P$ and $R$
% %-   if $\beta$ is close to 0, then we give more %importance to $P$
% %    -   $F_0 = P$
% %-   if $\beta$ is closer to $+ \infty$, we give more %importance to
% %    $R$

% %When $\beta = 1$ we have $F_1$ score: -->

% It is difficult to achieve high \textit{precision} and high \textit{recall} simultaneously:\\
% A classifier that predicts more "positives" will tend to be more sensitive (higher recall), 
% but it will also tend to give more \emph{false positives} (lower precision).\\
% A classifier that predicts more "negatives" will tend to be more precise (higher PPV), but it will also produce 
% more \emph{false negatives} (lower recall, less sensitive).
% \lz
% A measure that balances the two conflicting goals is the $F$-measure ($F_1$-Score):

% $$F_1 = 2 \cfrac{P \cdot R}{P + R} = \cfrac{2 \tp}{2\tp + \fan + \fap}$$%= \cfrac{2\text{tpr}}{\text{tpr} + \tfrac{\rn}{\rp} \cdot \text{fpr} + 1 }$$
% This is the harmonic mean of precision $P$ and recall $R$.\\
% Note that this measure still doesn't account for the number of true negatives.

% \end{vbframe}

% % \begin{vbframe}{Example}
% %
% % \begin{center}
% % \includegraphics[width=0.7\textwidth]{figure_man/roc-confusion_matrix_example.png}
% % \end{center}
% %
% %
% % \end{vbframe}

% \begin{vbframe}{Multiclass Confusion Matrix}
%   \begin{itemize}
%     \item The performance of a multiclass classifier can also be summarized in a confusion matrix.
%     \item E.g., in a 3-class classification problem with labels A, B and C:
%     \item[] \includegraphics[width=0.9\textwidth]{figure_man/roc-multiclass_confmatrix.png}
%     \item The overall accuracy is calculated similarly to the binary confusion matrix.
%     \item Example:
%           $acc = \cfrac{\tp_A + \tp_B + \tp_C}{n} = \cfrac{30+60+80}{300} \approx 56\%$
%   \end{itemize}

%   \framebreak

%   \begin{itemize}
%     \item[]   \includegraphics[width=0.9\textwidth]{figure_man/roc-multiclass_confmatrix.png}
%     \item Other measures, e.g. tpr, need to be calculated in a one-vs-all manner.
%     \item Example: true positive rate for label A
%     $$\text{tpr}_A = \cfrac{\tp_A}{\tp_A + \fan_A} = \cfrac{30}{30+50+20} = 0.3$$
%     \item Example: precision for label A
%     $$\text{P}_A = \cfrac{\tp_A}{\tp_A + \fap_A} = \cfrac{30}{30+20+10} = 0.5$$
%   \end{itemize}

% \end{vbframe}

% <<echo = FALSE, message = FALSE, warning = FALSE, eval = FALSE>>=
%   library(mlr)
%   set.seed(1)
%   lrn = makeLearner("classif.rpart")
%   iris2 = rbind(iris, iris)
%   iris2$Species = c(rep("A", 100), rep("B", 100), rep("C", 100))
%   iris2.task = makeClassifTask(data = iris2, target = "Species")
%   mod = train(lrn, iris2.task)
%   pred = predict(mod, newdata = iris2)
%   pred$data$response = c(rep("A", 30), rep("B", 50), rep("C", 20),
%                          rep("A", 20), rep("B", 60), rep("C", 20),
%                          rep("A", 10), rep("B", 10), rep("C", 80))
%   cm = calculateConfusionMatrix(pred)
%   t(cm$result)
% @


% %<!--
% %# Visual Analysis

% %Visual ways of evaluating the performance of a classifier

% %-   [ROC Analysis](http://0agr.ru/wiki/index.php/ROC_Analysis "wikilink5") - True Positive Rate vs
%     %False Positive Rate
% %-   [Cumulative Gain Charts](http://0agr.ru/wiki/index.php/Cumulative_Gain_Chart "wikilink6") - True
%   %  Positive Rate vs Predicted Positive Rate


% %# Not Binary Classifiers

% %When we have multi-class classifiers we can use:

% %-   Contingency Table
% %    -   just show misclassified observations side-by-side
% %-   [Cost Matrix](http://0agr.ru/wiki/index.php/Cost_Matrix "wikilink8")
% %    -   we define the cost for each misclassification
% %    -   and calculate the total cost
% %-   some measures can be extended to multiclass classifiers:
% %    -   see Evaluation of Multiclass Classifiers
% %-->

% \begin{vbframe}{Binary Cost-sensitive Classification}
% \begin{itemize}
%   \item Regular classification:
%   \begin{itemize}
%     \item All misclassification errors equally severe.
%     \item Aim: minimize misclassification rate (unbalanced: maximize F1-Score or AUC (...later)).
%   \end{itemize}
%   \item Cost-sensitive classification:
%     \begin{itemize}
%     \item Costs caused by different kinds of errors are not assumed to be equal.
%     \item Aim: minimize expected costs.
%   \end{itemize}
% \end{itemize}

% \framebreak

% Cost matrix:
% \begin{table}[]
% \centering
% \begin{tabular}{|l|c|c|}
% \hline
%                    & Actual positive & Actual negative \\ \hline
% Predicted positive & $c_{11}$              & $c_{10}$              \\
% Predicted negative & $c_{01}$              & $c_{00}$              \\ \hline
% \end{tabular}
% \end{table}
% \begin{itemize}
% \item \textit{Reasonableness} conditions:
%      $$c_{01} > c_{11} \hspace{5pt} \text{ and } \hspace{5pt} c_{10} > c_{00}$$
% \item For a given cost matrix, examples should be classified so that the predictions have minimal expected cost.
%  \item $\P(y|x)$ is the (estimated) probability of class $y$ given features $x$.
%  \item Let $\pix  := \P(y = 1|x)$ and $1 - \pix  = \P(y = 0|x)$.

% % \item There are classification methods that can accomodate misclassification costs directly (e.g. \href{http://www.rdocumentation.org/UKljZ/packages/rpart/functions/rpart.html}{rpart}).
% % \item Alternatively, choose thresholds to turn posterior probabilities into class labels, such that the costs are minimized.
% \end{itemize}

% \framebreak
% Turn estimated class probabilities into predicted labels by 
% choosing thresholds such that the costs are minimized:
% \begin{itemize}
%   \item Predict "1" if \\ expected cost of predicting "1" $\leq$ expected cost of predicting "0"
%   \item[]$\Leftrightarrow$  $(1-\pix )c_{10} + \pix c_{11} \leq (1-\pix ) c_{00} + \pix c_{01}$
%   \item If this inequality is an equality, then predicting either class has the same expected cose
%   \item[] $\Rightarrow$ Choose probability threshold $\pi_C$ such that
%   $$\pi_C = \cfrac{c_{10} - c_{00}}{c_{10}- c_{00} + c_{01} - c_{11}}$$
%   \item[] $\Rightarrow$ define classifier $h(x) = \I(\pix \geq \pi_C)$
%   \item The \textit{Reasonableness} conditions ensure that $\pi_C$ is well defined.
% \end{itemize}

% \end{vbframe}

% \begin{vbframe}{ROC Analysis}
% %<!-- (from Signal Detection Theory) -->
% %<!--
% %\vspace{-0.25cm}
% %-   initially - for distinguishing noise from not %noise
% %-   so it's a way of showing the performance of %Binary Classifiers
% %    -   only two classes - noise vs not noise  -->
% Any binary classifier $\hx$ can be characterized by its
% \begin{itemize}
%   \item true positive rate: $\text{tpr} = \cfrac{\text{\#TP}}{\text{\#TP} + \text{\#FN}}$ (Sensitivity, Recall)
%   \item false positive rate: $\text{fpr} = 1-\text{specificity} = \cfrac{\text{\#FP}}{\text{\#FP} + \text{\#TN}}$ 
% \end{itemize}
% \lz

% For a given scoring classifier $\fx$, define the associated binary classifier $h(x, \theta) = \I(f(x) \geq \theta)$ with decision threshold $\theta$. 
% \lz

% The \textbf{R}eceiver \textbf{O}perating \textbf{C}haracteristic (ROC) curve of a scoring classifier is created by plotting \textit{tpr} vs. \textit{fpr} for all possible decision thresholds $\theta$.\\
% (Any given binary classifier occupies a single point (tpr, fpr)  in \enquote{ROC-Space}.)

% \framebreak

% ROC curves are insensitive to the class distribution in the sense that they are not affected by changes in the ratio $\np/\nn$: 

% \begin{table}[]
% \centering
% \begin{tabular}{|l|c|c|}
%                 \hline
%    $\np/\nn = 1$            & True Positive & True Negative \\ \hline
% Pred. Positive & 40            & 25            \\ \hline
% Pred. Negative & 10            & 25           \\ \hline
% \end{tabular}
% \end{table}
% \begin{table}[]
% \centering
% \begin{tabular}{|l|c|c|}
%                 \hline
%      $\np/\nn = 2$        & True Positive & True Negative \\ \hline
% Pred. Positive & 80            & 25            \\ \hline
% Pred. Negative & 20            & 25           \\ \hline
% \end{tabular}
% \end{table}

% True positive rates ($\text{tpr} = 0.8$) and false positive rates ($\text{fpr} = 0.5$) do not change.


% % \begin{itemize}
% %   \item Note that the class distribution is
% % the relationship of the left column to the right column.
% %   \item Any performance metric that uses values from both
% % columns will be inherently sensitive to class skews (such as accuracy, precision, F measure)
% %   \item ROC curves are based upon tpr and fpr.
% %   \item Each measure uses only values in their respective columns.
% %   \item Changing the class distribution will therefore not change the ROC curve.
% % \end{itemize}

% \end{vbframe}

% \begin{vbframe}{ROC Space Baseline}

% %<!-- We put a random classifier that predicts 1 with some probability, e.g. 3 random classifiers on the baseline: -->
% \begin{itemize}

% \item   The best classifier lies on the top-left corner.
% \item   Example: 3 classifiers that lie on the baseline, i.e. classifiers that
%   \begin{itemize}
%     \item always predict "0" (0\% chance to predict "1"),
%     \item randomly predict "1"  80\% of the time
%     \item always predict "1" (in 100\% cases).
%   \end{itemize}
% \end{itemize}

% \begin{center}
% \includegraphics[width=0.45\textwidth]{figure_man/roc-space.png}
% \end{center}

% \end{vbframe}


% \begin{vbframe}{ROC Space Baseline}

% In practice, we can never obtain a classifier below this line.\\

% Example:
% \begin{itemize}
%   \item   A classifier $h_1$ below the line with $\text{fpr} = 80\%$, and $\text{tpr} = 30\%$
%   \item  Improve it by inverting its predictions:
%     $h_2(x)$: if $h_1(x) = 1$, return 0; if $h_1(x) = 0$, return 1
%   \item   Position of $h_2$ is then $(1 - \text{fpr}, 1 - \text{tpr}) = (20\%, 70\%)$
% \end{itemize}

% \begin{center}
% \includegraphics[width=0.45\textwidth]{figure_man/roc-inv.png}
% \end{center}

% \end{vbframe}


% \begin{vbframe}{ROC Convex Hull}

% Suppose we have 5 classifiers $h_1, h_2, ..., h_5$

% \begin{itemize}
% \item   We calculate $\text{fpr}$ and $\text{tpr}$ for each and plot them on one plot.
% \item   Each classifier is a single point in the ROC space.
% \end{itemize}

% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure_man/roc-plot-classifiers.png}
% \end{center}

% \end{vbframe}


% \begin{vbframe}{ROC Convex Hull}
% We can then try to find classifiers that achieve the best
% $\text{fpr}$ and $\text{tpr}$.

% \begin{itemize}
%   \item  %By the \href{http://0agr.ru/wiki/index.php/Dominance}{dominance} principle, we have the following Pareto frontier (the "ROC convex hull").
%   Classifiers on the convex hull \enquote{dominate} those below it (e.g. $C_3$) and achieve the highest total accuracy acc for some class distribution.
%   \item Classifiers below this hull are suboptimal in this sense: they have either lower tpr \textbf{or} higher fpr than those on it.
%   \item 
% \end{itemize}

% \begin{center}
% \includegraphics[width=0.45\textwidth]{figure_man/roc-convex-hull.png}
% % this example sucks because C3 is not clearly worse (it has better tpr than C2 and C1)
% \end{center}
% \end{vbframe}


% \begin{vbframe}{ISO Accuracy Lines}
% There is a simple relationship between accuracy and $\text{fpr}$, $\text{tpr}$.

% \begin{itemize}
%   \item   Let $n$ be the number of observations,
%   \item   $\nn$ and $\np$ the number of negative and positive observations,
%   \item   $\rn$ and $\rp$ the fraction of negative and positive observations, respectively.
% \end{itemize}
% \begin{align*}
%  \text{acc} &= \cfrac{\text{\#TP} + \text{\#TN}}{n} = \cfrac{\text{\#TP}}{\np} \cdot \cfrac{\np}{n} +
%     \cfrac{\nn - \text{\#FP}}{n} \\
%       &= \cfrac{\text{\#TP}}{\np} \cdot \cfrac{\np}{n} +
%     \cfrac{\nn}{n} - \cfrac{\text{\#FP}}{\nn} \cdot
%     \cfrac{\nn}{n} \\
%     & = \text{tpr} \cdot \rp + \rn - \text{fpr} \cdot \rn
% \end{align*}
% \end{vbframe}


% \begin{vbframe}{ISO Accuracy Lines}
% We can rewrite this and get
% $$\text{tpr} =  \cfrac{\rn}{\rp}
% \cdot \text{fpr} + \cfrac{\text{acc} -
% \rn}{\rp}$$

% Properties:
% \begin{itemize}
%   \item   The ratio $\cfrac{\rn}{\rp}$ is the slope of the line (changing this ratio yields many different slopes).
%   \item   Changing the accuracy yields many parallel lines with the same slope because $acc$ is included in the intercept.
%   \item   "Higher" lines are better w.r.t. $acc$.
% \end{itemize}
% \end{vbframe}


% \begin{vbframe}{ISO Accuracy Lines}
% To calculate the corresponding accuracy, we have to find the intersection point of the accuracy line (red) with the descending diagonal (blue).

% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/iso_lines.png}
% \end{center}
% \end{vbframe}

% \begin{vbframe}{ISO Accuracy Lines}
% Explanation:
% \begin{itemize}
%  \item descending diagonal: $\text{tpr} = 1-\text{fpr} \; \Leftrightarrow \text{fpr} = 1-\text{tpr}$
%  \item ISO accuracy line: $\text{tpr} =  \tfrac{\rn}{\rp}
% \cdot \text{fpr} + \tfrac{\text{acc} -
% \rn}{\rp}$
% \end{itemize}
% At the intersection between these lines we have:
% \begin{align*}
%  \text{tpr} &= \tfrac{\rn}{\rp} \cdot (1-\text{tpr}) + \tfrac{\text{acc} -
%  \rn}{\rp}
%  = \tfrac{\rn}{\rp} - \tfrac{\rn}{\rp} \cdot \text{tpr} + \tfrac{\text{acc} -
%  \rn}{\rp} \\
%  &= \tfrac{\text{acc}}{\rp} - \tfrac{\rn \cdot \text{tpr}}{\rp}
%  = \tfrac{\text{acc} -\rn \cdot \text{tpr}}{\rp}
%  \end{align*}
%   With $\rp + \rn = 1$ this is equivalent to:
% \begin{align*}
%  \rp \cdot \text{tpr} + \rn \cdot \text{tpr} &= \text{acc} \\
%  \Leftrightarrow \hspace{2cm} \text{tpr} &= \text{acc}
%  \end{align*}

% $\Rightarrow$ At the intersection between the descending diagonal and the ISO accuracy line, the achieved accuracy is equal to the true positive rate.
% \end{vbframe}

% \begin{vbframe}{ISO Accuracy Lines vs Convex Hull}
% Recall the convex hull of the ROC plot:

% \begin{center}
% \includegraphics[width=0.65\textwidth]{figure_man/roc-convex-hull.png}
% \end{center}
% \end{vbframe}

% \begin{vbframe}{ISO Accuracy Lines vs Convex Hull}
% Each line segment of the ROC convex hull is an ISO accuracy line
% for a particular class distribution (slope) and accuracy.
% All classifiers on such a line achieve the same accuracy for this distribution:

% \begin{itemize}
%   \item   $\rn / \rp > 1$

%   \begin{itemize}
%     \item   Distribution with more true negative observations.
%     \item   The slope is steep.
%     \item   Classifier to the left (less false positives) is more accurate.
%   \end{itemize}

%   \item   $\rn / \rp < 1$

%   \begin{itemize}
%     \item   Distribution with more positive observations.
%     \item   The slope is flatter.
%     \item   Classifier to the right (more false positives) is more accurate.
%   \end{itemize}

% \end{itemize}
% Each classifier on the convex hull is optimal w.r.t. accuracy for a specific class distribution.
% \end{vbframe}

% \begin{vbframe}{Selecting the Optimal Classifier}
% Find the classifier that achieves the highest accuracy for a given ratio:
% \begin{enumerate}
%   \item Compute the ratio (slope) $\rn / \rp$.
%   \item Find highest accuracy iso-line that touches the ROC convex-hull
%   \item Use classifier on the touch point
% \end{enumerate}
% \end{vbframe}

% \begin{vbframe}{Selecting the Optimal Classifier - Example}
% \begin{center}
% \includegraphics[width=0.55\textwidth]{figure_man/roc-convex-hull-best-acc-1.png}
% \end{center}

% \begin{itemize}
%   \item Distribution: $neg/pos = 1/1$, best classifier: $C_2$, accuracy $\approx 81 \%$
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Selecting the Optimal Classifier - Example}
% \begin{center}
% \includegraphics[width=0.55\textwidth]{figure_man/roc-convex-hull-best-acc-2.png}
% \end{center}

% \begin{itemize}
%   \item Distribution: $neg/pos = 1/4$, best classifier: $C_4$, accuracy $\approx 83 \%$
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Selecting the Optimal Classifier - Example}
% \begin{center}
% \includegraphics[width=0.55\textwidth]{figure_man/roc-convex-hull-best-acc-3.png}
% \end{center}

% \begin{itemize}
%   \item Distribution: $neg/pos = 4/1$, best classifier: $C_2$, accuracy $\approx 81 \%$

% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Scoring Classifiers}
% A scoring classifier (or ranker) is an algorithm that outputs the scores (e.g. a probabilities) for each class instead of one single
% label.

% Do binary classification with a ranker $f$:
% \begin{itemize}
%   \item $f$ outputs a single number
%   \item Set some threshold $\theta$ to transform the ranker into a classifier,
%     e.g. as in logistic regression
%   \begin{itemize}
%       \item Predict $\hat{y} = 1$ (positive class) if $f(x) > \theta$ else predict $\hat{y} = 0$
%   \end{itemize}
%   \item How to set a threshold $\theta$?
%   \begin{itemize}
%     \item Use crossvalidation for finding
%         the best value for $\theta$.
%     \item Draw ROC curves, producing a point in ROC space for \textbf{each possible threshold}.
%   \end{itemize}
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{ROC for Scoring Classifiers}
% \textbf{Naive Method}:

% Given a ranker $f$ and a dataset with $n$ training observations:
% \begin{itemize}
%   \item Consider all possible thresholds ($n-1$ for $n$ observations).
%   \item For each threshold: Calculate $\text{fpr}$ and $\text{tpr}$, and draw this point in ROC space.
%   \item Select the best threshold using the ROC analysis (for the ratio $\rn / \rp$).
% \end{itemize}
% \textbf{Practical Method}:
% \begin{itemize}
%   \item Rank test observations on decreasing score.
%   \item Start in $(0, 0)$, for each observation $x$ (in the decreasing order).
%   \begin{itemize}
%     \item If $x$ is positive, move $1/\rp$ up
%     \item If $x$ is negative, move $1/\rn$ right
%   \end{itemize}
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Example Naive Method}
% \centering
% \includegraphics[width=0.6\textwidth]{figure_man/roc-naive-2.png}

% \href{https://fabian-s.shinyapps.io/roc_shiny/}{\beamergotobutton{Click me for live demo.}}
% \end{vbframe}

% \begin{vbframe}{Example Practical Method}
% Given:
% \begin{itemize}
%   \item 20 observations
% \end{itemize}

% \tiny
% \begin{tabular}{l|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.2cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}}

%   \hspace{-8pt} \#&  1& 2&  3&  4&  5&  6&  7&  8&  9& 10& 11& 12& 13& 14& 15& 16& 17& 18& 19& 20 \\ \hline
%   \hspace{-8pt} C & P& P & N & P & P & P & N & N & P & N & P & N & P & N & N & N & P & N & P & N \\ \hline
%   \hspace{-8pt} Score & .9 & .8 &  .7 & .6 & .55 & .54 & .53 & .52 & .51 & .505 & .4 & .39 & .38 & .37 & .36 & .35 & .34 & .33 & .3 &.1
% \end{tabular}
% \normalsize

% %<!--(https://www.dropbox.com/s/65rdiv42ixe2eac/roc-lift.xlsx) -->
% \begin{itemize}
%   \item $C$ is the actual class of the training observations.
%   \item $\rn / \rp = 1$, i.e. $1/\rp = 1/\rn = 0.1$
% %<!--   [Click here for GIF](https://hsto.org/files/267/36b/ff1/26736bff158a4d82893ff85b2022cc5b.gif "giflink") (Result on next slide) -->
% \end{itemize}

% \begin{itemize}
%   \item Rank test observations on decreasing score.
%   \item Start in $(0, 0)$, for each observation $x$ (in the decreasing order).
%   \begin{itemize}
%     \item If $x$ is positive, move $1/\rp = 0.1$ up
%     \item If $x$ is negative, move $1/\rn = 0.1$ right
%   \end{itemize}
% \end{itemize}

% \framebreak

% \begin{center}
% \includegraphics[page=1]{figure_man/gif.pdf}

%  $x$ is positive, move $1/\rp = 0.1$ up.
% \end{center}

% \framebreak

% \begin{center}
% \includegraphics[page=2]{figure_man/gif.pdf}

%  $x$ is positive, move $1/\rp = 0.1$ up.
% \end{center}

% \framebreak

% \begin{center}
% \includegraphics[page=3]{figure_man/gif.pdf}

%  $x$ is negative, move $1/\rn = 0.1$ right.
% \end{center}

% \framebreak

% \begin{center}
% \includegraphics[page=4]{figure_man/gif.pdf}

%  $x$ is positive, move $1/\rp = 0.1$ up.
% \end{center}

% \framebreak

% \begin{center}
% \includegraphics[page=5]{figure_man/gif.pdf}

%  $x$ is positive, move $1/\rp = 0.1$ up.
% \end{center}

% \framebreak

% \begin{center}
% \includegraphics[page=6]{figure_man/gif.pdf}

%  $x$ is positive, move $1/\rp = 0.1$ up.
% \end{center}

% \framebreak

% \begin{center}
% \includegraphics[page=7]{figure_man/gif.pdf}

%  $x$ is negative, move $1/\rn = 0.1$ right.
% \end{center}

% \framebreak

% \begin{center}
% \includegraphics[page=20]{figure_man/gif.pdf}
% \end{center}

% \framebreak

% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure_man/roc-curve-ex1.png}
% \end{center}

% \begin{itemize}
%   \item Score of the best (6th) classifier is used as the threshold $\theta$.
%   \item Predict positive class for $\theta \geqslant 0.54$ ($\Rightarrow$ accuracy = 0.7).
% \end{itemize}
% \end{vbframe}

% %
% % \begin{vbframe}{Example Practical Method}
% % Given: 20 training observations, 12 negative and 8 positive
% %
% % \vspace{20pt}
% %
% % \tiny
% % \begin{tabular}{l|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}}
% %
% %   \hspace{-8pt} \#&  1& 2&  3&  4&  5&  6&  7&  8&  9& 10& 11& 12& 13& 14& 15& 16& 17& 18& 19& 20 \\ \hline
% %   \hspace{-8pt} C & N & N & N & N & N & N & N & N & N & N & N & N & P & P & P & P & P & P & P & P \\ \hline
% %   \hspace{-8pt} Score & .18 & .24 &  .32 & .33 & .4 & .53 & .58 & .59 & .6 & .7 & .75 & .85 & .52 & .72 & .73 & .79 & .82 & .88 & .9 &.92
% % \end{tabular}
% % \normalsize
% %
% % \vspace{20pt}
% % $\Rightarrow$ sort by score and draw the curves:
% % \vspace{20pt}
% %
% % \tiny
% % \begin{tabular}{l|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}}
% %
% %   \hspace{-8pt} \#&  20& 19&  18&  12&  17&  16&  11&  15&  14& 10& 9& 8& 7& 6& 13& 5& 4& 3& 2& 1 \\ \hline
% %   \hspace{-8pt} C & P & P & P & N & P & P & N & P & P & N & N & N & N & N & P & N & N & N & N & N \\ \hline
% %   \hspace{-8pt} Score & .92 & .9 &  .88 & .85 & .82 & .79 & .75 & .73 & .72 & .7 & .6 & .59 & .58 & .53 & .52 & .4 & .33 & .32 & .24 &.18
% % \end{tabular}
% % \normalsize
% % \end{vbframe}
% %
% %
% % \begin{vbframe}{Example Practical Method}
% % \begin{center}
% % \includegraphics[width=0.75\textwidth]{figure_man/roc-curve-ex2.png}
% % \end{center}
% % \begin{itemize}
% %   \item Best accuracy achieved with observation \# 18.
% %   \item Setting $\theta = 0.88 \Rightarrow$ accuracy of $15/20 \; \hat{=} \; 75 \%$.
% % \end{itemize}
% % \end{vbframe}

% \begin{vbframe}{Other ROC Curve Examples}
% %<!-- (http://www.cs.bris.ac.uk/~flach/ICML04tutorial/ROCtutorialPartI.pdf) -->
% \begin{center}
% \includegraphics[width=0.6\textwidth]{figure_man/roc-curves.png}
% \end{center}
% \end{vbframe}

% \begin{vbframe}{AUC: Area Under ROC Curve}
% The area under the ROC curve (AUC $\in [0, 1]$) is
% \begin{itemize}
%   \item A measure for evaluating the performance of a classifier:
%   \begin{itemize}
%     \item AUC =   1: Perfect classifier, for which all positives are ranked higher than all negatives
%     \item AUC = 0.5: Randomly ordered
%     \item AUC =   0: All negatives are ranked higher than all positives
%     \item Interpretation of AUC: Probability that a classifier $C$ ranks a randomly drawn
%     positive observation "$+$" higher than a randomly drawn negative observation "$-$".
%   \end{itemize}

% %<!-- https://www.quora.com/How-is-statistical-significance-determined-for-ROC-curves-and-AUC-values -->

%   \item Related to the Mann-Whitney-U test, which
%   \begin{itemize}
%     \item Estimates the probability that randomly chosen positives
%       are ranked higher than randomly chosen negatives.
%     \item Relation: $AUC = \cfrac{U}{\np \nn}$
%   \end{itemize}
%   % \item related to the Gini coefficient $= 2 \cdot AUC - 1$ (area above diag.)
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{AUC: Area Under ROC Curve}
% %<!--
% %-   Measure for evaluating the performance of a classifier
% %-   it's the area under the ROC Curve, total area is 100%
% %-->

% \begin{center}
% \includegraphics[width=0.75\textwidth]{figure_man/roc-auc-ex1.png}
% \end{center}
% \end{vbframe}

% \begin{vbframe}{Explanation Mann-Whitney-U Test}
% \begin{itemize}
% \item First we plot the ranks of all the scores as a stack of horizontal bars, and color them by the labels.
% \item Stack the green bars on top of one another, and slide them horizontally as needed to get a nice even stairstep on the right edge (See: practical method example for ROC curves):
% \end{itemize}
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure_man/roc-mannwhitney3.png}
% \end{center}


% \framebreak


% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure_man/roc-mannwhitney2.png}
% \end{center}

% \begin{itemize}
%  \item Definition of the U statistic: $U = R_1 - \cfrac{n_1(n_1 + 1)}{2}$
%  \begin{itemize}
%   \item $R_1$ is the sum of ranks of positive cases (the area of the green bars)
%   \item $n_1$ is the number of positive cases
%  \end{itemize}
%   \item The area of the green bars on the right side is equal to $\cfrac{n_1(n_1 + 1)}{2}$.
% \end{itemize}

% \framebreak
% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure_man/roc-mannwhitney2.png}
% \end{center}

% \begin{itemize}
%  \item $U =$ area of the green bars on left side
%  \item area of dashed rectangle = $n_1 \cdot n_2$
%  \item $AUC$ is $U$ normalized to the unit square,
% \end{itemize}
% $$\Longrightarrow AUC = \cfrac{U}{n_1\cdot n_2}$$
% with $n_1 = \np$ and $n_2 = \nn$.
% \end{vbframe}


% \begin{vbframe}{Partial AUC}
% \begin{itemize}
%   \item Sometimes it can be useful to look at a \href{http://journals.sagepub.com/doi/pdf/10.1177/0272989X8900900307}{specific region under the ROC curve}  $\Rightarrow$ partial AUC (pAUC).
%   \item Let $0 \leq c_1 < c_2 \leq 1$ define a region.
%   \item For example, one could focus on a region with low fpr ($c_1 = 0, c_2 = 0.2$) or a region with high tpr ($c_1 = 0.8, c_2 = 1$):
% \end{itemize}

% <<partial-roc, echo = FALSE, message = FALSE, warning = FALSE, fig.width = 14, fig.height = 7, out.width="0.7\\textwidth">>=
% library(pROC)
% set.seed(1)
% D.ex <- rbinom(200, size = 1, prob = .5)
% M1 <- rnorm(200, mean = D.ex, sd = .65)
% M2 <- rnorm(200, mean = D.ex, sd = 1.5)

% test <- data.frame(D = D.ex, D.str = c("Healthy", "Ill")[D.ex + 1],
%                    M1 = M1, M2 = M2, stringsAsFactors = FALSE)

% rocobj <- pROC::roc(test$D, test$M1)
% par(mfrow = c(1, 2))
% pROC::plot.roc(rocobj, print.auc=TRUE, auc.polygon=TRUE, partial.auc=c(1, 0.8), partial.auc.focus="sp", reuse.auc=FALSE, legacy.axes = TRUE, xlab = "fpr", ylab = "tpr", xlim = c(1, 0), ylim = c(0, 1),  auc.polygon.col="red", auc.polygon.density = 20, auc.polygon.angle = 135, partial.auc.correct = FALSE
% )
% pROC::plot.roc(rocobj, print.auc=TRUE, auc.polygon=TRUE, partial.auc=c(1, 0.8), partial.auc.focus="se", reuse.auc=FALSE, legacy.axes = TRUE, xlab = "fpr", ylab = "tpr", xlim = c(1, 0), ylim = c(0, 1),  auc.polygon.col="red", auc.polygon.density = 20, auc.polygon.angle = 135)
% @

% \framebreak

% \begin{itemize}
%   \item $\text{pAUC} \in [0, c_2 - c_1]$.
%   \item The partial AUC can be corrected (see \href{http://journals.sagepub.com/doi/pdf/10.1177/0272989X8900900307}{McClish}), to have values between $0$ and $1$, where $0.5$ is non discriminant and $1$ is maximal: $$\text{pAUC}_\text{corrected} = \cfrac{1+\cfrac{\text{pAUC} - \text{min}}{\text{max} - \text{min}}}{2} $$
%   \item $\text{min}$ is the
% value of the non-discriminant AUC in the region
%   \item $\text{max}$ is the maximum possible AUC in the region
% \end{itemize}
% \end{vbframe}



% \begin{vbframe}{Multiclass AUC}
% \begin{itemize}
%   \item Consider multiclass classification, where a classifier predicts the probability $p_k$ of belonging to class $k$ for each class.
%   \item Hand and Till (2001) proposed to average the AUC of pairwise comparisons (1 vs. 1) of a multiclass classifier.
%   \begin{itemize}
%     \item estimate $AUC(i,j)$ for each pair of class $i$ and $j$
%     \item $AUC(i,j)$ is the probability that a randomly drawn member of class $i$ has a lower probability of belonging to class $j$
%       than a randomly drawn member of class $j$.
%     \item for $K$ classes, we have ${{K}\choose{2}} = \tfrac{K (K-1)}{2}$ values of $AUC(i,j)$ that are then averaged to compute the Multiclass AUC.
%   \end{itemize}
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Calibration and Discrimination}
% We consider data with a binary outcome $y$.
% \begin{itemize}
%   \item \textbf{Calibration:} When the predicted probabilities closely agree
%     with the observed outcome (for any reasonable grouping).
%   \begin{itemize}
%     \item \textbf{Calibration in the large} is a property of the \textit{full sample}.
%     It compares the observed probability in the full sample  (e.g. proportion of observations for which $y=1$)
%    % <!-- (e.g., 10% if 10 of 100 individuals have the outcome being predicted, e.g. $y=1$) -->
%     with the average predicted probability in the full sample.
%     \item \textbf{Calibration in the small} is a property of \textit{subsets} of the sample.
%     It compares the observed probability in each subset with the average
%     predicted probability in that subset.
%   \end{itemize}
%   \item \textbf{Discrimination:} Ability to perfectly separate the population into $y=0$ and $y=1$.
%     Measures of discrimination are, for example, AUC, sensitivity, specificity.
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Calibration and Discrimination}
% %<!-- http://www.uphs.upenn.edu/dgimhsr/documents/predictionrules.sp12.pdf -->
% A well calibrated  classifier can be poorly discriminating, e.g.

% \begin{table}[]
% \centering
% \begin{tabular}{rrrr}
% \hline
% Obs. Nr. & truth & Pred Rule 1 & Pred Rule 2 \\
% \hline
% 1        & 1     & 1           & 0           \\
% 2        & 1     & 1           & 0           \\
% 3        & 0     & 0           & 1           \\
% 4        & 0     & 0           & 1           \\ \hline
% Avg Prob & 50\%  & 50\%        & 50\%        \\
% \hline
% \end{tabular}
% \end{table}

% \begin{itemize}
%   \item Both prediction rules have identical calibration in the large (50\%), however, rule 1 is better than rule 2.
% \end{itemize}

% <<discrim-calib, eval = FALSE, echo = FALSE>>=
% truth = c(1,1,0,0,0,0)
% pred.rule.1 = c(1,1,0,0,0,0)
% pred.rule.2 = c(0,0,0,0,1,1)
% library(knitr)
% kable(data.frame(truth = truth, "pred rule 1" = pred.rule.1, "pred rule 2" = pred.rule.2))
% @
% \end{vbframe}

% \begin{vbframe}{Calibration and Discrimination}
% A well discriminating classifier can have a bad calibration, e.g.

% \begin{table}[]
% \centering
% \begin{tabular}{rrrr}
% \hline
% Obs. Nr. & truth & Pred Rule 1 & Pred Rule 2 \\
% \hline
% 1        & 1     & 0.9           & 0.9         \\
% 2        & 1     & 0.9           & 0.9           \\
% 3        & 0     & 0.1          & 0.7           \\
% 4        & 0     & 0.1         & 0.7           \\ \hline
% Avg Prob & 50\%  & 50\%        & 80\%        \\
% \hline
% \end{tabular}
% \end{table}

% \begin{itemize}
%   \item Both prediction rules are well discriminating (e.g., setting thresholds $\theta_1 = 0.5$, $\theta_2 = 0.8$)
%   \item Prediction rule 2 is rather poorly calibrated. The proportion of observations for which $y=1$ would be estimated with $80\%$.
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{ROC Analysis in R}
% \begin{itemize}
%   \item \texttt{generateThreshVsPerfData} calculates one or several performance measures for a sequence of decision thresholds from 0 to 1.
%   \item It provides S3 methods for objects of class \texttt{Prediction}, \texttt{ResampleResult}
% and \texttt{BenchmarkResult} (resulting from  \texttt{predict.WrappedModel}, \texttt{resample}
% or \texttt{benchmark}).
%   \item \texttt{plotROCCurves} plots the result of \texttt{generateThreshVsPerfData} using \texttt{ggplot2}.
%   \item More infos \url{http://mlr-org.github.io/mlr-tutorial/release/html/roc_analysis/index.html}
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Example 1: Single predictions}
% \scriptsize
% <<roc-ex-prep, echo=TRUE, message = FALSE>>=
% library(mlr)
% set.seed(1)
% # get train and test indices
% n = getTaskSize(sonar.task)
% train.set = sample(n, size = round(2/3 * n))
% test.set = setdiff(seq_len(n), train.set)

% # fit and predict
% lrn = makeLearner("classif.lda", predict.type = "prob")
% mod = train(lrn, sonar.task, subset = train.set)
% pred = predict(mod, task = sonar.task, subset = test.set)
% @
% \normalsize
% \end{vbframe}

% \begin{vbframe}{Example 1: Single predictions}
% We calculate fpr, tpr and compute error rates:

% \scriptsize
% <<roc-ex-1, echo = TRUE>>=
% df = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
% @
% \normalsize
% \begin{itemize}
%   \item \texttt{generateThreshVsPerfData} returns an object of class \texttt{ThreshVsPerfData},
% which contains the performance values in the \texttt{\$data} slot.
%   \item By default, \texttt{plotROCCurves} plots the performance values of the first two measures passed
% to \texttt{generateThreshVsPerfData}.
%   \item The first is shown on the x-axis, the second on the y-axis.
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Example 1: Single predictions}
% \scriptsize
% <<roc-ex-2, echo = TRUE, fig.align="center", fig.width = 8, fig.height = 5, out.width="0.55\\textwidth">>=
% df = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
% plotROCCurves(df)
% @
% \normalsize

% \framebreak

% The corresponding area under curve auc can be calculated by

% \scriptsize
% <<roc-ex-3, echo = TRUE>>=
% performance(pred, auc)
% @

% \normalsize
% \texttt{plotROCCurves} always requires a pair of performance measures that are plotted against
% each other.

% \framebreak

% If you want to plot individual measures vs. the decision threshold, use

% \scriptsize
% <<roc-ex-4, echo = TRUE, fig.align="center", fig.height = 5, fig.width = 10, out.width="0.9\\textwidth">>=
% plotThreshVsPerf(df)
% @
% \normalsize
% \end{vbframe}


% \begin{vbframe}{Example 2: Benchmark Experiment}
% \scriptsize
% <<>>=
% options(width = 200)
% @
% <<roc-bmr-prep, echo = TRUE>>=
% lrn1 = makeLearner("classif.randomForest", predict.type = "prob")
% lrn2 = makeLearner("classif.rpart", predict.type = "prob")

% cv5 = makeResampleDesc("CV", iters = 5)

% bmr = benchmark(learners = list(lrn1, lrn2), tasks = sonar.task,
%   resampling = cv5, measures = list(auc, mmce), show.info = FALSE)
% bmr
% @
% \normalsize

% Calling \texttt{generateThreshVsPerfData} and \texttt{plotROCCurves} on the \texttt{BenchmarkResult}
% produces a plot with ROC curves for all learners in the experiment.

% \framebreak

% \scriptsize
% <<roc-bmr-1, echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="\\textwidth">>=
% df = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce))
% plotROCCurves(df) + scale_c_d()
% @
% \framebreak

% \scriptsize
% <<roc-bmr-2, echo = TRUE, fig.align="center", fig.height = 5, fig.width = 10, out.width="\\textwidth">>=
% plotThreshVsPerf(df) + theme(legend.position = "bottom") + scale_c_d()
% @
% \end{vbframe}

% \endlecture

\lecturechapter{44}{Imbalanced}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Imbalanced Classification problems}

\textbf{Imbalanced data} refers to a (binary or multiclass) classification problem where classes are not respresented equally. However, the minority class usually represents the most important concept to be learned. 

\lz 

%Most classification data do not have exactly equal number of instances in each class, but a small difference often doesn't matter.

\textbf{Example:}

\begin{itemize}
  \item Fraud prediction
  \item Given dataset contains credit card transactions: $200000$ observations containing $500$ frauds 
  \item Always predicting \enquote{no fraud} yields an \enquote{excellent} accuracy of $99.8\%$
  \item Model performance is bad as you did not even detect one fraud
  \item The accuracy is only reflecting the underlying class distribution (\textbf{accuracy paradoxon})
\end{itemize}


%Example for datasets where classes might be extremely unbalanced:

%\begin{itemize}
%\item Disease prediction
%\item Spam detection
%\end{itemize}
\end{vbframe}

\begin{vbframe}{Dealing with imbalanced data}

% ## Imbalanced Data: Example (I)

%{r, include=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
%ap = adjust_path(paste0(getwd(), "/figure"))
<<include=FALSE, cache=FALSE, warning=FALSE, message=FALSE>>=
library(ggplot2)
library(mlr)
library(stats)
@

% **Example: Credit card fraud detection**
 
%{r, echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=6, include = FALSE}
<<echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=6, include = FALSE>>=
df = rbind(
  data.frame(class = rep("Fraud", 200)),
  data.frame(class = rep("No Fraud", 200000))
  )

p1 = ggplot(data = df) + stat_count(aes(x = class, fill = class)) + ggtitle("Class distribution") + xlab("") + ylim(c(0, 200000)) + theme(legend.position = "none")

p1
@
% <!-- \vspace{-0.7cm}
% 
% - Imagine a dataset containing $200000$ credit card transactions containing $200$ frauds.
% - You trained a classifier that yields an accuracy of $99.8\%$. Not bad, right?
% 
% ## Imbalanced Data: Example (I)
% 
% - But taking a deeper look into the predictions, we see that the classifier does nothing but always predicting "no fraud":
% $$
% \frac{\text{\# of correct predictions}}{\text{\# of observations}} = \frac{(200000 - 200)}{200000} = 99.8\%.
% $$
% - The predictions have no value: We did not even detect one fraud
%  -->



We train a classification tree to predict if an email is spam or not based on features extracted from emails.

Our data is really imbalanced, our training set consists of $2788$ non-spam and only $50$ spam mails.$^{(*)}$ Evaluation is done via holdout ($80\%$ for training, $20\%$ for testing).


%{r, echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5}
<<echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5>>=
# we artificially create an imbalanced setting by throwing away data
types = getTaskData(spam.task)$type
subset = c(which(types == "nonspam"), which(types == "spam")[1:50])
task = subsetTask(spam.task, subset)
n = getTaskSize(task)

set.seed(123456789)
test.set = sample(n, size = 0.2 * n)
train.set = setdiff(1:n, test.set)

task.train = subsetTask(task, train.set)
task.test = subsetTask(task, test.set)
print("classification tree")
lrn = makeLearner("classif.rpart",predict.type = "prob")
mod = train(lrn, task.train)
pred = predict(mod, task.test)
perf = mlr::performance(pred, measures = list(mlr::acc, mlr::tpr, mlr::fpr, mlr::auc))
perf

# ggplotConfusionMatrix(pred)
@

\vfill

\begin{footnotesize}
$^{(*)}$ We use the \texttt{spam} dataset (see \texttt{spam.task} in \texttt{mlr}). We \enquote{pretend} that the dataset is imbalanced by using just a subset of the original data.
\end{footnotesize}

\end{vbframe}

\begin{vbframe}{Dealing with imbalanced data}

Like in this example, appropriate measures like for example precision, recall or F1 score enable an adequate evaluation of models on imbalanced data.

Many learners tend to produce unsatisfactory classifiers when faced with imbalanced datasets. What can be done?

\begin{enumerate}
\item Try different algorithms: trees, for example, often perform well on imbalanced datasets
\end{enumerate}
Trees are able to recursively split feature space into decision regions and therefore, handle imbalanced data often better than other classifiers.

However, the quality of the classifier depends on the data distribution of the minority class.

This includes ensemble based tree methods like Random Forest and Gradient Boosting.
\end{vbframe}

\begin{vbframe}{Threshold Tuning (I)}
\begin{enumerate}
\setcounter{enumi}{1}
\item Do Threshold Tuning
\end{enumerate}
\begin{itemize}
\item If a classifier outputs probabilities $\pi (x)$ (or scores $f(x)$), they are transformed into labels by comparing it to a threshold:

$$
\hat y = 1 \quad \text{if} \quad \pi (x) > \alpha
$$

\item For probabilities, this threshold is often set to $0.5$ per default (or $0$ for scores)
\end{itemize}
\end{vbframe}

\begin{vbframe}{Threshold Tuning (II)}

In this example, a threshold of $0.5$ is a bad choice ($FPR = 0, TPR = 0$).

\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/threshold_tuning11.png}
\includegraphics[width=0.45\textwidth]{figure_man/threshold_tuning12.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Threshold Tuning (III)}

\begin{itemize}
\item However, this threshold can be anything between $0$ and $1$.
\item When adjusting the threshold, we can control the number of examples labeled true
\item Naive way: Calculate labels for every possible threshold, evaluate and choose your favorite threshold
\item Better: Look at the ROC curve, where each point corresponds to the TPR and FPR for one specific threshold and take the one you prefer
\end{itemize}
\end{vbframe}

\begin{vbframe}{Threshold Tuning (IV)}

In this simple artificial example, we can achieve a much better performance (FPR = $0.143$, TPR = $1$) by setting thresholds appropriately.

\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/threshold_tuning21.png}
\includegraphics[width=0.45\textwidth]{figure_man/threshold_tuning22.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Over- and undersampling}

\begin{enumerate}
\setcounter{enumi}{2}
\item Try to improve the class distribution
\begin{itemize} 
\item if possible, collect more data
\item try to modify your data to improve the class distribution via
\begin{itemize}
\item oversampling
\item undersampling
\item SMOTE
\end{itemize}
\end{itemize}
\end{enumerate}

\end{vbframe}

\begin{vbframe}{Oversampling (I)}

One approach to balance the class distribution is \textbf{oversampling}: We randomly add copies from the underrepresented class.

\begin{center}
\includegraphics{figure_man/oversampling.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Oversampling (II)}

The number of copies we add by oversampling is specified by a \textbf{oversampling rate}. If the oversampling rate is $2$, the underrepresented class gets doubled.

%{r, echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5}
<<echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5>>=
library(gridExtra)
idx = c(2, 9, 16, 23, 51:63)
df = iris[idx, c(1, 2, 5)]
levels(df$Species) = c("spam", "no spam", "bla")
colnames(df) = c("x1", "x2", "y")

p1 = ggplot() + geom_histogram(data = df, aes(x = y, fill = y), stat = "count") + theme(legend.position = "none")

min.class = which(df$y == "spam")
resampled = sample(min.class, length(min.class), replace = TRUE)
df2 = rbind(df, df[resampled, ])

p2 = ggplot() + geom_histogram(data = df2, aes(x = y, fill = y), stat = "count") + theme(legend.position = "none")

grid.arrange(p1, p2, ncol = 2)
@
\end{vbframe}

\begin{vbframe}{Oversampling (III)}

\textbf{Pros}:

\begin{itemize}
\item Oversampling is easy to implement
\item All information available is used
\end{itemize}

\textbf{Cons}:
\begin{itemize}
\item The size of the dataset increases which might increase runtime of the training or cause storage problems
\item It increases the likelihood of overfitting since it minority class events are replicated
\end{itemize}

\end{vbframe}

\begin{vbframe}{Undersampling (I)}

Another is \textbf{undersampling}: We randomly \textbf{throw away} observations from the overrepresented class.

\begin{center}
\includegraphics[width = 0.5\linewidth]{figure_man/undersampling.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Undersampling (II)}

Again, the number of majority class observations we delete is specified by a \textbf{undersampling rate}. If the rate is $\frac{1}{2}$, the overrepresented class gets halved.

%{r, echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5}
<<echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5>>=
p1 = ggplot()
p1 = p1 + geom_point(data = df, aes(x = x1, y = x2, colour = y), size = 2) + xlim(c(4,7)) + ylim(c(2,4.5))
p1 = p1 + theme(legend.position = "none")

p2 = ggplot() + geom_histogram(data = df, aes(x = y, fill = y), stat = "count") + theme(legend.position = "none")  + ylim(c(0, 15))

grid.arrange(p1, p2, nrow = 1)
@
\end{vbframe}

\begin{vbframe}{Undersampling (III)}

Again, the number of majority class observations we delete is specified by a \textbf{undersampling rate}. If the rate is $\frac{1}{2}$, the overrepresented class gets halved.


%{r, echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5}
<<echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5>>=
task = makeClassifTask(data = df, target = "y")
task.under = undersample(task, rate = 1 / 2)

p1 = ggplot() + geom_point(data = getTaskData(task.under),
	aes(x = x1, y = x2, colour = y), size = 2) + theme(legend.position = "none")  + xlim(c(4,7)) + ylim(c(2,4.5))

p2 = ggplot() + geom_histogram(data = getTaskData(task.under), aes(x = y, fill = y), stat = "count") + theme(legend.position = "none") + ylim(c(0, 15))

grid.arrange(p1, p2, nrow = 1)
@
\end{vbframe}

\begin{vbframe}{Undersampling (IV)}

\textbf{Pros}:

\begin{itemize}
\item Undersampling is also easy to implement
\item It can help improve runtime and storage problems by reducing the number of training data samples (when the training data set is huge)
\end{itemize}

\textbf{Cons}:

\begin{itemize}
\item We loose information by throwing away data samples
\end{itemize}

% <!-- ## Dealing with imbalanced data (II)
% 
% We can \enquote{modify} the class distribution by
% 
% 	- randomly adding copies of observations from the under-represented class (**oversampling**)
% 	- randomly deleting observations from the over-represented class (**undersampling**) -->

%{r, echo=FALSE, fig.height=3.5, include = FALSE}
<<>>=
data = rbind(
  data.frame(x = rnorm(100, mean = 1), class = "spam"),
  data.frame(x = rnorm(5000, mean = 2), class = "no spam")
	)

task = makeClassifTask(data = data, target = "class")
task.over = oversample(task, rate = 8)
task.under = undersample(task, rate = 1/8)

p1 = ggplot() + stat_count(aes(x = getTaskTargets(task),fill= getTaskTargets(task))) + ggtitle("Original") + xlab("") + ylim(c(0, 5000)) + theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
p2 = ggplot() + stat_count(aes(x = getTaskTargets(task.over),fill= getTaskTargets(task.over))) + ggtitle("Oversampling") + xlab("") + ylim(c(0, 5000)) + theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
p3 = ggplot() + stat_count(aes(x = getTaskTargets(task.under),fill= getTaskTargets(task.under))) + ggtitle("Undersampling") + xlab("") + ylim(c(0, 5000)) + theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
grid.arrange(p1, p2, p3, nrow = 1)
@
\end{vbframe}

\begin{vbframe}{SMOTE (I)}

Both oversampling (duplicating examples) and undersampling (throwing away data) might be problematic. Instead we might prefer to generate \textbf{new synthetic} examples.

We generate new synthetic examples using \textbf{SMOTE} \footnote[frame]{Chawla,Bowyer,Hall,Kegelmeyer (2002), SMOTE: Synthetic Minority Over-sampling Technique} algorithm:

For each original data point $x$, find the $k$ nearest neighbors
\begin{itemize}
\item Then repeat $N - 1$ times:
\begin{itemize} 
\item randomly select one of the $k$ nearest neighbors, called $\tilde x$
\item randomly interpolate $x$ and $\tilde x$ to create a new observation
\end{itemize}
\item The number of artificial data points is defined by the \textbf{rate} $N$ ($N = 2$ means that the minority class gets doubled)
\end{itemize}

\end{vbframe}

\begin{vbframe}{SMOTE (II)}

Connect each minority class point to its $k$ (here: $k = 2$) nearest neighbors.

%{r, echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5}
<<echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5>>=
p1 = ggplot()
p1 = p1 + geom_point(data = df, aes(x = x1, y = x2, colour = y), size = 2)
p1 = p1 + theme(legend.position = "none")

min.class  =which(df$y == "spam")
grid = as.data.frame(t(combn(x = min.class, 2)))
df.segm = cbind(df[grid$V1, 1:2], df[grid$V2, 1:2])
names(df.segm) = c("x", "y", "xend", "yend")

p3 = p1 + geom_segment(data = df.segm[c(1,3), ], aes(x = x, y = y, xend = xend, yend = yend), alpha = 0.5)
p3 = p3 + geom_point(data = df[1, ], aes(x = x1, y = x2, colour = y), size = 4)
p3 = p3 + theme(legend.position = "none")

p2 = ggplot() + geom_histogram(data = df, aes(x = y, fill = y), stat = "count") + theme(legend.position = "none")

grid.arrange(p3, p2, nrow = 1)
@
\end{vbframe}

\begin{vbframe}{SMOTE (III)}

Randomly select a neighbor and generate a new point by selecting a random point lying on the respective connection.

%{r, echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5}
<<echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5>>=
task = makeClassifTask(data = df, target = "y")

set.seed(1234)
task.smote = smote(task, rate = 3, nn = 2)
smote.data = getTaskData(task.smote)

p3 = p3 + geom_point(data = smote.data[20, ],
	aes(x = x1, y = x2, colour = y), size = 3)
p2 = ggplot() + geom_histogram(data = rbind(df, smote.data[20, ]), aes(x = y, fill = y), stat = "count") + theme(legend.position = "none")

grid.arrange(p3, p2, nrow = 1)
@
\end{vbframe}

\begin{vbframe}{SMOTE (IV)}

For each of the minority class points, repeat the above procedure $N - 1$ times.

%{r, echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5}
<<echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5>>=
p1 = p1 + geom_segment(data = df.segm[c(1:3, 5:6), ], aes(x = x, y = y, xend = xend, yend = yend), alpha = 0.5)
p1 = p1 + theme(legend.position = "none")
p1 = p1 + geom_point(data = getTaskData(task.smote),
	aes(x = x1, y = x2, colour = y), size = 2)

p2 = ggplot() + geom_histogram(data = smote.data, aes(x = y, fill = y), stat = "count") + theme(legend.position = "none")

grid.arrange(p1, p2, nrow = 1)
@
\end{vbframe}

\begin{vbframe}{SMOTE (V)}

The original SMOTE algorithm cannot handle categorical features, but the algorithm can be extended:

\begin{itemize}
\item To determine the nearest neighbors, use a distance that can handle categorical variables (e. g. Gower distance)
\item Instead of interpolation between between two factor levels, we randomly take one of them.
\end{itemize}

\textbf{Note} that each time the data is modified (especially for SMOTE, over- and undersampling) hyperparameters of used machine learning algorithms possibly need to be changed!

\end{vbframe}

\begin{vbframe}{SMOTE (VI)}

\textbf{Pros}:

\begin{itemize}
\item Classifiers (like trees) can generalize better than by oversampling with replacement strategy
\item Improves recognition of minority class
\item Several improvements and extensions of SMOTE tackle some of its disadvantages
\end{itemize}

\textbf{Cons}:

\begin{itemize}
\item New samples remain in convex hull
\item Can generate noisy samples
\item Not as powerful in high-dimensional data
\item kNN can increase computational complexity
\end{itemize}

\end{vbframe}

\begin{vbframe}{SMOTE (VI)}

Oversampling by replacement can lead to overfitting of decision regions:
\begin{center}
\includegraphics[width=0.7\textwidth]{figure_man/smote_overs_replacement.png}
\end{center}

\end{vbframe}
\begin{vbframe}{SMOTE (VI)}

In contrast, the synthetic samples generated by SMOTE can force decision regions to be larger and thus prevent overfitting:
\begin{center}
\includegraphics[width=0.7\textwidth]{figure_man/smote_oversampling.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Example (I)}

Let's return to the spam example from the beginning. We try to improve performance by using over- and undersampling as well as the SMOTE algorithm.

For undersampling we use a rate or $\frac{1}{30}$, for oversampling a rate of $30$ and for SMOTE, we also use a rate of $30$ and set the number of nearest neighbors to $k = 3$.

\end{vbframe}

\begin{vbframe}{Example (II)}

Evaluation is again done via holdout ($80\%$ for training, $20\%$ for testing). All of the methods reduce the false positive rate while at the same time the true positive rates worsens.

%{r, echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5}
<<echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5>>=
set.seed(123456789)

types = getTaskData(spam.task)$type
subset = c(which(types == "nonspam"), which(types == "spam")[1:50])
task = subsetTask(spam.task, subset)
n = getTaskSize(task)

test.set = sample(n, size = 0.2 * n)
train.set = setdiff(1:n, test.set)

task.train = subsetTask(task, train.set)
task.test = subsetTask(task, test.set)
task.under = undersample(task.train, rate =  1 / 30)
task.over = oversample(task.train, rate =  30)
task.smote = smote(task.train, rate =  30, nn = 3)
tasks = list("imbalanced" = task.train, "oversampling" = task.over, "undersampling" = task.under, "SMOTE" = task.smote)

lrn = makeLearner("classif.rpart",predict.type = "prob")
perf = data.frame()
print("classification tree")
for (i in 1:length(tasks)) {
	mod = train(lrn, tasks[[i]])
	pred = predict(mod, task.test)
	perf = rbind(perf, mlr::performance(pred, measures = list(mlr::acc, mlr::tpr, mlr::fpr, mlr::auc)))
}
names(perf) = c("acc", "tpr", "fpr","auc")
perf$method = names(tasks)
perf
@

\end{vbframe}

\begin{vbframe}{Benchmark (I)}
The performance of over-, undersampling and SMOTE can be compared for different classifiers. The parameters for sampling are the same as in the example before.

<<echo=FALSE, out.width="0.7\\textwidth", fig.height=2.5, fig.width=5>>=

data <- read.csv(file="phpn1jVwe.csv",header=TRUE)
task = makeClassifTask(data=data,target="class")
task

@

\end{vbframe}
\begin{vbframe}{Benchmark (II)}

For over-, undersampling and SMOTE, the overall accuracy of the classifiers decreases together with the true positive rate:
\includegraphics[width=0.8\textwidth]{figure_man/bmrSummary1.png}

\end{vbframe}
\begin{vbframe}{Benchmark (III)}

For over-, undersampling and SMOTE, the overall accuracy of the classifiers decreases together with the true positive rate:

\includegraphics[width=0.8\textwidth]{figure_man/bmrSummary2.png}

\end{vbframe}
\begin{vbframe}{Benchmark (IV)}

However, all methods decrease the false positive rate and improve the recognition of minority samples for all classifiers:

\includegraphics[width=0.8\textwidth]{figure_man/bmrSummary3.png}

\end{vbframe}

\begin{vbframe}{Outlook}

Over- and undersampling techniques can be combined to hybrid approaches to theoretically outperform over- or undersampling. 

For SMOTE, there exist procedures that additionally perform undersampling. 

SMOTE + Tomek Links:
\begin{itemize}
  \item A pair of nearest neighbors which belong to different classes is called a Tomek Link.
  \item  Undersampling is performed by either removing all Tomek Links or by only removing majority class samples of Tomek Links.
\end{itemize}
SMOTE + ENN (Edited Nearest Neighbor):
\begin{itemize}
  \item Samples are removed if their class label is different to majority vote of their k nearest neighbors.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Imbalanced data: Change perspective}

If none of the above approaches worked, you may have to take on another perspective.

\begin{enumerate}
\setcounter{enumi}{3}
\item Try a different perspective: use \textbf{anomaly} or \textbf{change detection} techniques instead of classification algorithms
\end{enumerate}

\begin{center}
\includegraphics[width=0.8\textwidth]{figure_man/classification-vs-anomaly.png}
\end{center}

\end{vbframe}
\endlecture

\lecturechapter{99}{Lightning talks}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Cost-Sensitive Classification}

Consider again the fraud detection example. Which error is more severe? 

\begin{itemize}
  \item Falsely predicting a credit card fraud \textit{(false positive)}\textsuperscript{1}
  \item Not predicting an actual credit card fraud \textit{(false negative)}\textsuperscript{2}
\end{itemize}

\textbf{Cost-sensitive classification} refers to settings where the costs of errors are not assumed to be equal and the objective is to minimize the expected costs. 

\lz 

For now, consider the 2-class case with 0-1-encoding.

Cost matrix:
\begin{table}[]
\centering
\begin{tabular}{|l|c|c|}
\hline
                   & Actual positive & Actual negative \\ \hline
Predicted positive & $c_{11}$ or TP             & $c_{10}$ or FP\textsuperscript{1}             \\
Predicted negative & $c_{01}$ or FN\textsuperscript{2}              & $c_{00}$ or TN              \\ \hline
\end{tabular}
\end{table}

with $c_{01} > c_{11}$ and  $c_{10} > c_{00}$ (Reasonableness conditions).

\framebreak

How can these classification costs be taken into account? 

\begin{itemize}
\item \enquote{Manipulating} the predictions by \textbf{thresholding}: 
   \begin{itemize}
     \item Posterior probabilities are turned into class labels using cost minimizing threshold, or if dataset is poorly calibrated, optimizing w. r. t. expected costs. 
    \end{itemize}
\item \enquote{Manipulating} the training data set by \textbf{rebalancing}:
  \begin{itemize}
    \item weighting observations 
    \item resampling
  \end{itemize}

\end{itemize}

Note: Thresholding requires the learner to predict posterior probabilities, rebalancing by weighting requires the learner to handle weights. Resampling can turn arbitrary classification learners into cost-sensitive ones. 

\end{vbframe}

\begin{vbframe}{Thresholding}

A cost is denoted as $c_{ij}$, where $i$ stands for the predicted class and $j$ for the actual class. 
Consider the cost matrix ($c_{01} > c_{11}$ and  $c_{10} > c_{00}$):

\begin{table}[]
\centering
\begin{tabular}{|l|c|c|}
\hline
                   & Actual positive & Actual negative \\ \hline
Predicted positive & $c_{11}$ or TP             & $c_{10}$ or FP             \\
Predicted negative & $c_{01}$ or FN              & $c_{00}$ or TN              \\ \hline
\end{tabular}
\end{table}

The expected cost $R$ of classifying sample x as class i:

\begin{table}[]
\centering
$R(i | x) = \sum_{j}^{} P( j | x)c_{ij}$
\end{table}

Then, a sample x is predicted as class $i=1$, iff $R(1 | x) \leq R(0 | x)$:

\begin{enumerate}
  \item $ P(0|x)c_{10} + P(1|x)c_{11} \leq P(0|x)c_{00} + P(1|x)c_{01} $ \label{eq-1}
  \item $ (1-\pi)c_{10} + \pi c_{11} \leq (1-\pi)c_{00} + \pi c_{01} $ \label{eq-2}
\end{enumerate}

Equation \ref{eq-1} can be substituted to obtain \ref{eq-2} by replacing $P(1|x)$ with $\pi$.


\end{vbframe}

\begin{vbframe}{Thresholding}

The optimal threshold $\pi^{*}$ for decision making is the value that turns the inequality \ref{eq-2} into an equality:

\begin{table}[]
\centering
$(1-\pi^{*})c_{10} + \pi^{*}c_{11} = (1-\pi^{*})c_{00} + \pi^{*}c_{01}$
\end{table}

Solving this equation for threshold $\pi^{*}$ leads to the equality:

\vspace*{-0.3cm}

$$
\pi ^* = \cfrac{c_{10} - c_{00}}{c_{10}- c_{00} + c_{01} - c_{11}}
$$

If the classifier is able to handle the posterior probability $P(1|x)$ for a test sample x, the predicted class is $i=1$, iff
$P(1|x) \geq \pi^{*}$, and $i=0$ otherwise. 

If this is not the case, the training set can be rebalanced to achieve an equivalent classifier.

\end{vbframe}

\begin{vbframe}{Rebalancing}

How to do cost-minimizing rebalancing? 

\begin{itemize}
  \item Remember the case where a learner predicts posterior probabilities. The threshold is changed accordingly to the costs
  $$
  \pi ^* = \cfrac{c_{10} - c_{00}}{c_{10}- c_{00} + c_{01} - c_{11}}
  $$
  \item However, even if that's not the case, a classifier \enquote{implicitly} makes decisions based on the probability threshold $\pi_0$ (usually $\pi_0=0.5$).  
  \item To make the probability threshold $\pi^*$ correspond to a given probability threshold $\pi_0$, the number of negative examples in the training set should be \enquote{multiplied} by
  
  $$
  \frac{\pi^*}{1-\pi^*}\frac{1-\pi_0}{\pi_0}
  $$

\end{itemize}

\end{vbframe}

\begin{vbframe}{Rebalancing - Weighting}

Example: If $c_{00} = c_{11} = 0$ and $\pi_{0} = 0.5$, then the number of negative training samples is \enquote{multiplied} by $\frac{\pi^{*}}{(1-\pi^{*})} = \frac{c_{10}}{c_{01}}$. Consider the case of $c_{10}<c_{01}$, then the factor becomes smaller than 1: $\frac{c_{10}}{c_{01}} < 1$.

If the classifier uses class weights for training, the weight for the negative class samples can be set according to the factor $\frac{c_{10}}{c_{01}}$. By adjusting the weights the classifier becomes cost-sensitive.

\end{vbframe}

\begin{vbframe}{Rebalancing - Resampling}

Example: If $c_{00} = c_{11} = 0$ and $\pi_{0} = 0.5$, then the number of negative training samples is \enquote{multiplied} by $\frac{\pi^{*}}{(1-\pi^{*})} = \frac{c_{10}}{c_{01}}$. Consider the case of $c_{10}<c_{01}$, then the factor becomes smaller than 1: $\frac{c_{10}}{c_{01}} < 1$.

Alternatively, if a classifier can't handle weights, the same effect can be achieved by resampling the training set. The negative training samples can then be resampled by the factor $\frac{c_{10}}{c_{01}}$. If the factor is smaller than one undersampling of the majority class is performed. 

\end{vbframe}

\begin{vbframe}{Direct cost-sensitive learning}

Thresholding and Sampling can be considered as wrappers\footnote[frame]{Ling,Sheng (2008), Cost-Sensitive Learning and the Class Imbalance Problem} that turn cost-insenitive classifiers into cost-sensitive ones. On the other hand, direct methods are cost-sensitive by design.

\begin{itemize}
  \item ICET: Inexpensive Classification with Expensive Tests
  \begin{itemize}
    \item ICET incorporates misclassification costs in the fitness function of genetic
algorithms. 
  \end{itemize}
  \item Cost-sensitive decision trees
  \begin{itemize}
    \item Misclassification costs are considered in tree building process. Attributes are selected according to minimization of total misclassification cost. 
  \end{itemize}
\end{itemize}

\end{vbframe}

\begin{vbframe}{Example-dependent misclassification costs}

\begin{itemize}
  \item Coming back to the example of credit card fraud detection, not detecting a fraud might cause high costs for the bank and for the customer
  \item However, these costs could depend on each customer, due to different credit card limits for example
\end{itemize}

This yields the following cost matrix: 

\small

\begin{table}[]
\centering
\begin{tabular}{|l|c|c|}
\hline
Obs. & costs for predicting 0 & costs for predicting 1 \\ \hline
1 & $c_{0}^{(1)}$ & $c_{1}^{(1)}$ \\
2 & $c_{0}^{(2)}$ & $c_{1}^{(2)}$ \\
3 & $c_{0}^{(3)}$ & $c_{1}^{(3)}$ \\ \hline
\end{tabular}
\end{table}

\normalsize
The misclassification error of observation $i$ is defined by $w^{(i)} = \yi c_{0}^{(i)} + (1 - \yi )c_{1}^{(i)}$.

\framebreak 

How can known methods be turned into learners that can deal with example dependent costs? 

\begin{itemize}
  \item Thresholding: as for class-dependent costs, but for each example a different threshold
  \item Rebalancing: 
  \vspace*{-0.1cm}
  \begin{itemize}
    \item weighting the observations by $w^{(i)}$
    \item resampling, e. g. \textbf{cost-proportionate rejection-sampling} 
  \end{itemize}
  \vspace*{-0.3cm}
      \small
        \begin{algorithm}[H]
      \textbf{Input}: $\D$, $w^{(i)}_{i \in \nset}$ misclassification errors, number of iterations $M$\\
      \textbf{Output}: $\tilde\D$ resampled data set 
      \begin{algorithmic}[1]
        \For{$m = 1 \to M$}
          \State Draw one observation $(\xi, \yi) \in \D$  
          \State Add to $\tilde\D$ with probability $\frac{w^{(i)}}{\max_j w^{(j)}}$
        \EndFor
      \end{algorithmic}
    \end{algorithm}
\end{itemize}


\end{vbframe}

\begin{vbframe}{Multiclass cost-sensitive classification}

In the multiclass case (including the case $g=2$) we have a vector of costs for each observation 

\vspace*{-0.3cm}

$$
c^{(i)} = (c^{(i)}_1, c^{(i)}_2, ..., c^{(i)}_g).
$$

$c^{(i)}_g$ is the cost to be paid when observation $i$ is predicted as class $g$. 

The \textbf{cost-sensitive one-vs-one (CS-OVO)} algorithm can perform cost-sensitive multiclass classification with any base binary classifier: 

\small
  \begin{algorithm}[H]
    \textbf{Input}: $\D$, $\{c^{(i)}\}_{i \in \nset}$ cost vectors
      \begin{algorithmic}[1]
        \For {$k, \tilde k \in \{1, ..., g\}$}
          \State Transform $\D$ to
          $
          \tilde\D = \biggl(\xi, \argmin\limits_{l = k \text{ or } \tilde k} c^{(i)}_l, |c^{(i)}_k - c^{(i)}_{\tilde k} |\biggr)_{i \in \nset}
          $
          \State Use a weighted binary classification algorithm on $\tilde D$ to get $\hat h_{k, \tilde k}$
        \EndFor
        \State $\hat h(x)$ is obtained by majority vote.
      \end{algorithmic}
    \end{algorithm}
\end{vbframe}

\endlecture
