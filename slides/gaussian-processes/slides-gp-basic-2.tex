\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\newcommand{\titlefigure}{figure_man/discrete/marginalization-more.png} %not best picture
\newcommand{\learninggoals}{
  \item How to model distributions over discrete functions
  \item The role of the covariance function
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gaussian Processes: Distribution on Functions}
\lecture{Advanced Machine Learning}

\begin{vbframe}{Discrete Functions}

For simplicity, let us consider functions with finite domains first. \\

\lz


Let $\mathcal{X} = \left\{\xv^{(1)}, \dots , \xv^{(n)}\right\}$ be a finite set of elements and $\Hspace$ the set of all functions from $\mathcal{X} \to \R$.

\textbf{Remark}: $\mathcal{X}$ does not mean the training data here but means the ``real'' domain of the functions.

\lz

Since the domain of any $f(.) \in \Hspace$ has only $n$ elements, we can represent the function $f(.)$ compactly as a $n$-dimensional vector $$\bm{f} = \left[f\left(\xv^{(1)}\right), \dots, f\left(\xv^{(n)}\right)\right].$$
\end{vbframe}


\begin{frame}{Discrete Functions}

Some examples $f: \Xspace \to \R$ where $\Xspace$ is univariate and finite:
\lz

\begin{figure}[h]
\foreach \x in{1,2,3} {
  \includegraphics<\x>[width=0.7\linewidth]{figure/discrete/example_2_\x.pdf} \par
}

\end{figure}



\end{frame}

\begin{frame}{Discrete Functions}

\textbf{Example 2:} Let us consider $f: \Xspace \to \R$ where the input space consists of \textbf{five} points $\Xspace = \{0, 0.25, 0.5, 0.75, 1\}$.

\lz

Examples for functions that live in this space:

\begin{figure}[h]
\foreach \x in{1,2,3} {
  \includegraphics<\x>[width=0.7\linewidth]{figure/discrete/example_5_\x.pdf}\par
}
\end{figure}

\end{frame}


\begin{frame}{Discrete Functions}

\textbf{Example 3:} Let us consider $f: \Xspace \to \R$ where the input space consists of \textbf{ten} points.

\lz

Examples for functions that live in this space:

\begin{figure}[h]
\foreach \x in{1,2,3} {
  \includegraphics<\x>[width=0.7\linewidth]{figure/discrete/example_10_\x.pdf}\par
}
\end{figure}

\end{frame}


\begin{vbframe}{Distributions on Discrete Functions}

\vspace*{0.5cm}

One natural way to specify a probability function on a discrete function $f \in \Hspace$ is to use the vector representation
$$
  \bm{f} = \left[f\left(\xi[1]\right), f\left(\xi[2]\right), \dots, f\left(\xi[n]\right)\right]
$$


of the function.

\lz

Let us see $\bm{f}$ as a $n$-dimensional random variable. We will further assume the following normal distribution:

$$
  \bm{f} \sim \mathcal{N}\left(\bm{m}, \bm{K}\right).
$$

\textbf{Note: } For now, we set $\bm{m} = \bm{0}$ and take the covariance matrix $\bm{K}$ as given. We will see later how they are chosen / estimated.

\end{vbframe}

\begin{frame}{Discrete Functions}

\textbf{Example 1 (continued):} Let $f: \Xspace \to \R$ be a function that is defined on \textbf{two} points $\Xspace$. We sample functions by sampling from a two-dimensional normal variable

$$
\bm{f} = [f(1), f(2)] \sim \mathcal{N}(\bm{m}, \bm{K})
$$


%\begin{figure}[H]
\foreach \x in{1,2,3} {
  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_2_\x_a.pdf}   \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_2_\x_b.pdf}
} \par
\begin{footnotesize}
In this example, $m = (0, 0)$ and $K = \begin{pmatrix} 1 & 0.5 \\ 0.5 & 1 \end{pmatrix}$.
\end{footnotesize}
%\end{figure}

\end{frame}


\begin{frame}{Discrete Functions}

\textbf{Example 2 (continued):} Let us consider $f: \Xspace \to \R$ where the input space consists of \textbf{five} points. We sample functions by sampling from a five-dimensional normal variable


$$
\bm{f} = [f(1), f(2), f(3), f(4), f(5)] \sim \mathcal{N}(\bm{m}, \bm{K})
$$

%\begin{figure}[h]
\foreach \x in{1,2,3} {
  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_5_\x_a.pdf}  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_5_\x_b.pdf}
}
%\end{figure}

\end{frame}





% \begin{vbframe}
% \begin{figure}
% 	\centering
% 	\includegraphics{figure_man/discrete/sample2.png} \\
% 	\begin{footnotesize} If we sample again, we get another function.
% 	\end{footnotesize}
% \end{figure}


% However, we are usually interested in functions with infinite domain size.

% \lz

% This idea is extended to infinite domain size via \textbf{Gaussian processes}.

% \end{vbframe}

\begin{frame}{Discrete Functions}

\textbf{Example 3 (continued):} Let us consider $f: \Xspace \to \R$ where the input space consists of \textbf{ten} points. We sample functions by sampling from ten-dimensional normal variable

$$
\bm{f} = [f(1), f(2), \dots, f(10)] \sim \mathcal{N}(\bm{m}, \bm{K})
$$

%\begin{figure}[h]
\foreach \x in{1,2,3} {
  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_10_\x_a.pdf}  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_10_\x_b.pdf}
}
%\end{figure}

\end{frame}


\begin{vbframe}{Role of the Covariance Function}

Note that the covariance controls the \enquote{shape} of the drawn function. Consider four cases of varying correlation size
\vspace{10pt}

\begin{enumerate}
  \item[a)] Correlation almost 1: $\bm{K} = \begin{footnotesize}\begin{pmatrix} 1 & 0.99 & \dots & 0.99 \\
  0.99 & 1 & \dots & 0.99 \\
  0.99 & 0.99 & \ddots & 0.99 \\
  0.99 & \dots & 0.99 & 1 \end{pmatrix}.\end{footnotesize}$
  \vspace{10pt}
  \item[b)] strongly correlated: Off diagonal elements between 0.7-0.9.
  \vspace{10pt}
  \item[c)] moderately correlated: Off diagonal elements between 0.4-0.7.
  \vspace{10pt}
  \item[d)] uncorrelated: $\bm{K} = \id$.
\end{enumerate}

\end{vbframe}

\begin{vbframe}{Role of the Covariance Function}

\begin{figure}
  \includegraphics[width=0.35\linewidth]{figure/discrete/example_extreme_50_1.pdf} \includegraphics[width=0.35\linewidth]{figure/discrete/example_extreme_50_2.pdf}
\end{figure}

\begin{figure}
  \includegraphics[width=0.35\linewidth]{figure/discrete/example_extreme_50_3.pdf} \includegraphics[width=0.35\linewidth]{figure/discrete/example_extreme_50_4.pdf}
\end{figure}


\framebreak

\begin{itemize}
  \item \enquote{Meaningful} functions (on a numeric space $\Xspace$) may be characterized by a spatial property: \vspace*{0.2cm}
  \begin{itemize}
    \item[] If two points $\xi, \xi[j]$ are close in $\Xspace$-space, their function values $f(\xi), f(\xi[j])$ should be close in $\Yspace$-space.
  \end{itemize} \vspace*{0.2cm}
  In other words: If they are close in $\Xspace$-space, their functions values should be \textbf{correlated}! \vspace*{0.4cm}
  \item We can enforce that by choosing a covariance function with
  $$
    \bm{K}_{ij} \text{ high, if } \xi[i], \xi[j] \text{ close.}
  $$

  \framebreak

  \item We can compute the entries of the covariance matrix by a function that is based on the distance between $\xi, \xi[j]$, for example:

  \vspace*{0.2cm}
  \begin{enumerate}
    \item[e)] Spatial correlation: \begin{footnotesize}$K_{ij} = k(\xi[i], \xi[j]) = \exp\left(-\frac{1}{2}\left|\xi - \xi[j]\right|^2\right)$\end{footnotesize}
  \end{enumerate}

\begin{figure}
  \includegraphics[width=0.45\linewidth]{figure/discrete/example_extreme_50_5.pdf}  \includegraphics[width=0.45\linewidth]{figure/discrete/example_extreme_50_6.pdf}
\end{figure}

\end{itemize}

\begin{footnotesize}
\textbf{Note}: $k(\cdot,\cdot)$ is known as the \textbf{covariance function} or \textbf{kernel}. It will be studied in more detail later on.
\end{footnotesize}

\end{vbframe}

\endlecture
\end{document}
