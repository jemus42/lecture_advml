\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\newcommand{\titlefigure}{figure_man/discrete/marginalization-more.png} %not best picture
\newcommand{\learninggoals}{
  \item The difference between weight-space and function-space views
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gaussian Processes: From Weight-space to Function-space}
\lecture{Advanced Machine Learning}


\begin{vbframe}{Weight-Space View}

\begin{itemize}
  \item Until now we considered a hypothesis space $\Hspace$ of parameterized functions $\fxt$ (in particular, the space of linear functions). 
  \item Using Bayesian inference, we derived distributions for $\thetab$ after having observed data $\D$. 
  \item Prior believes about the parameter are expressed via a prior distribution $q(\thetab)$, which is updated according to Bayes' rule 

  $$
  \underbrace{p(\thetab | \Xmat, \yv)}_{\text{posterior}} = \frac{\overbrace{p(\yv | \Xmat, \thetab)}^{\text{likelihood}}\overbrace{q(\thetab)}^{\text{prior}}}{\underbrace{p(\yv|\Xmat)}_{\text{marginal}}}. 
  $$
\end{itemize}

\end{vbframe}


\begin{vbframe}{Function-space View}

Let us change our point of view: 

\begin{itemize}
  \item Instead of \enquote{searching} for a parameter  $\thetab$ in the parameter space, we directly search in a space of \enquote{allowed} functions $\Hspace$.  
  \item We still use Bayesian inference, but instead specifying a prior distribution over a parameter, we specify a prior distribution \textbf{over functions} and update it according to the data points we have observed. 
\end{itemize}

\framebreak 

Intuitively, imagine we could draw a huge number of functions from some prior distribution over functions $^{(*)}$. 

\begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/gp_sample/1_1.pdf}
\end{figure}

\vspace*{-0.5cm}

\begin{footnotesize}
  $^{(*)}$ We will see in a minute how distributions over functions can be specified. 
\end{footnotesize}

\framebreak 

\foreach \x in{1,2,3} {
    After observing some data points, we are only allowed to sample those functions, that are consistent with the data. \\
  \begin{figure}
    \includegraphics[width=0.8\textwidth]{figure/gp_sample/2_\x.pdf}
  \end{figure}
}

\framebreak 

As we observe more and more data points, the variety of functions consistent with the data shrinks. 
  \begin{figure}
    \includegraphics[width=0.8\textwidth]{figure/gp_sample/2_4.pdf}
  \end{figure}

\framebreak 

Inutitively, there is something like \enquote{mean} and a \enquote{variance} of a distribution over functions. 

  \begin{figure}
    \includegraphics[width=0.8\textwidth]{figure/gp_sample/2_4.pdf}
  \end{figure}

\end{vbframe}

\begin{frame}{Weight-space vs. Function-space View}

\begin{table}
  \begin{tabular}{cc}
  \textbf{Weight-Space View} & \textbf{Function-Space View} \vspace{4mm}\\ 
  Parameterize functions & \vspace{1mm}\\
  \footnotesize Example: $\fxt = \thetab^\top \xv$ & \vspace{3mm}\\
  Define distributions on $\thetab$ & Define distributions on $f$ \vspace{4mm}\\
  Inference in parameter space $\Theta$ & Inference in function space $\Hspace$
  \end{tabular}
\end{table}  

\lz

Next, we will see how we can define distributions over functions mathematically. 


\end{frame}









\endlecture
\end{document}
