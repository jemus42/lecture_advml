\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

%\usepackage{algorithm}
%\usepackage{algorithmic}

\newcommand{\sens}{\mathbf{A}} % vector x (bold)
\newcommand{\ba}{\mathbf{a}}
\newcommand{\batilde}{\tilde{\mathbf{a}}}
\newcommand{\Px}{\mathbb{P}_{x}} % P_x
\newcommand{\Pxj}{\mathbb{P}_{x_j}} % P_{x_j}
\newcommand{\indep}{\perp \!\!\! \perp} % independence symbol
% ml - ROC
\newcommand{\np}{n_{+}} % no. of positive instances
\newcommand{\nn}{n_{-}} % no. of negative instances
\newcommand{\rn}{\pi_{-}} % proportion negative instances
\newcommand{\rp}{\pi_{+}} % proportion negative instances
% true/false pos/neg:
\newcommand{\tp}{\# \text{TP}} % true pos
\newcommand{\fap}{\# \text{FP}} % false pos (fp taken for partial derivs)
\newcommand{\tn}{\# \text{TN}} % true neg
\newcommand{\fan}{\# \text{FN}} % false neg
\newcommand{\qv}{\mathbf{q}}  % vector q (bold)
\newcommand{\cv}{\mathbf{c}}    % vector c (bold)

\usepackage{multicol}

\newcommand{\titlefigure}{figure/cost_matrix}
\newcommand{\learninggoals}{
  \item Learn threshold adjusting in cost-sensitive learning
  \item Get to know MetaCost as a general approach to make classifiers cost-sensitve
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Imbalanced Learning via Cost-Sensitive Learning Part 2}
\lecture{Advanced Machine Learning}



\sloppy



\begin{vbframe}{Threshold adjusting: binary case}
    \footnotesize 
    \begin{itemize}
        \footnotesize
        \item Motivation: The threshold provided by Minimum Expected Cost is not always the truly optimal due to e.g. wrong model class, finite data, etc.
        \item Key idea: model the mis-classification cost $M_C$ as a function $M_C = g(T)$ of \textbf{probability threshold} $T \in [0, 1]$.
        \item By adjusting $T$, we can plot a curve of $M_C$ versus $T$ as the following figure (copied from Fig.1 in \href{https://www.aaai.org/Library/AAAI/2006/aaai06-076.php}{\beamergotobutton{Sheng et al. 2006}}):
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{slides/imbalanced-learning/figure/threshold_adjusting.png}
        \end{figure}

        \item The optimal threshold $T^{*}$ corresponds to the point with the smallest $M_C$.

        \item What if two local minima? We prefer the one whose ``valley'' has a wider span (e.g. $T_2$ in the 3rd subfigure). Because it is less sensitive to small changes in $T$.

        \item Search the best $T$ on \textbf{validation} sets, and do $K$-fold cross-validation to avoid overfitting!
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Threshold adjusting: binary case}
    \footnotesize 
    \begin{itemize}
        \scriptsize
        \item Example: German Credit Task, where customers are classified as "good" or "bad".
        \begin{center}
                            \begin{tabular}{cc|cc}
        			& &\multicolumn{2}{c}{True class} \\
        			& & $y=$ good & $y=$ bad  \\
        			\hline
        			\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}} & $\hat y$ = good & 0 & 3 \\
        			& $\hat y$ = bad & 1 & 0\\
                \end{tabular}
        \end{center}
        \item $C(good,bad)/(C(bad,good)+C(good,bad))=3/4=c^{*}$ is the optimal theoretical threshold value.

        \item Train three classifiers with threefold cv. Find best empirical threshold $T$ on validation sets. For XGBoost, the empirical minimum deviates substantially from the theoretical minimum.

                \begin{figure}[h]
            \centering
            \includegraphics[width=0.95\textwidth]{slides/imbalanced-learning/figure_man/Cost_Curves_plot.png}
        \end{figure}

    \end{itemize}
\end{vbframe}


\begin{vbframe}{Threshold Adjusting: Multi-class Case}
    \footnotesize
    \begin{itemize}
        \footnotesize
        \item It is also possible to perform threshold adjusting in multi-class classifcation.
        
        \item Here, we present a simple approach for classifiers that output scores $\pi(\xv) = (\pi(\xv)_1,\ldots,\pi(\xv)_g)^\top$ in multi-class classification:
        
        \begin{itemize}
            \footnotesize
            \item In the standard setting, we predict class $h(\xv) = \argmax_{j} \pi(\xv)_j$.
            \vspace{10pt}
            
            \item Now, we model the miss-classification costs $M_C$ as a function $M_C = g(\mathbf{T})$ where $\mathbf{T} \in \R^{g}$.
            \vspace{10pt}
            
            \item At step 1, we re-scale the scores $\tilde{\pi}(\xv) = (\frac{\pi(\xv)_1}{\mathbf{T}_1},\ldots, \frac{\pi(\xv)_g}{ \mathbf{T}_g})^\top$, and predict class $\tilde{h}(x) = \argmax_j \tilde{\pi}(\xv)_j$.
            \vspace{10pt}

            \item At step 2, we compute $M_C$ using the cost matrix $C$ and $\tilde{h}(x)$.
            \vspace{10pt}

            \item Similar to the binary case, we adjust $\mathbf{T}$ for $g$ classes to achieve the minimal mis-classification cost $M_C$.
            \vspace{10pt}

        \end{itemize}
    \end{itemize}
\end{vbframe}

\begin{vbframe}{MetaCost: Overview}


        \begin{itemize}
            \item Model-agnostic wrapper technique                      
            \item General idea: 
                \begin{enumerate}
                    \item Relabel training examples with their ``optimal'' classes, i.e., with low expected costs;
                   
                    \item Apply the classifier on the relabeled data set.
                \end{enumerate} 
        \end{itemize}

\end{vbframe}


\begin{vbframe}{MetaCost: Algorithm}
	
	\scriptsize{
%		
 	The procedure of MetaCost is divided into three phases:
			%			
		\begin{minipage}{0.53\textwidth} 
%				
				\begin{enumerate}
%					
					\scriptsize
					\item \textcolor{teal}{Bagging --- The underlying classifier is used (trained) $L$ times on different bootstrapped samples of the training data, respectively.}
%					

     				\item \textcolor{blue}{Relabeling --- The index set is the union of the classifier indices for which $\xi$ is OOB.}

                        \item \textcolor{magenta}{ Relabeling --- Compute $\pi_j(\xi)$ by averaging over the predictions of the OOB classifiers.}

                        \item \textcolor{red}{ Relabeling --- Determine new label $\tilde y^{(i)}$ w.r.t. the class that minimzes the sum of the weighted costs over the $g$ classes.}
%					
					\item \textcolor{orange}{Cost-sensitivity ---  The classifier is trained on the relabeled data set, resulting in a cost-sensitive classifier.}
%					
				\end{enumerate}
%

%		
		
		\end{minipage}
		\begin{minipage}{0.45\textwidth} 
			\begin{algorithmic}
				
				\tiny
%				
				\State \textbf{MetaCost}  
				\State \textbf{Input:} 
				$\D = \{(\xi,\yi)\}_{i=1}^n$ training data, \\
				$M \in \N$ number of bagging iterations, \\
				$B \in \N$ bootstrap size, \\
				$f$ probabilistic (black-box) classifier, 
				$\mathbf{C}$ cost matrix, 
%				$g = |\Yspace|$ number of classes
				\State \# 1st Bagging:
				\textcolor{teal}{\For{$m=1,\ldots,M$}
					\State $\D_m  \leftarrow $ BootstrapSample ($\D,B$)
					\State $f_m  \leftarrow $ train $f$ on $\D_m$
				\EndFor}
				\State \# 2nd Relabeling:
				\textcolor{blue}{\For{$i=1,\ldots,n$}
					\State $\tilde M \leftarrow \bigcup_{m: \xi \notin \D_m} \{m\}$
					\EndFor}
					\textcolor{magenta}{\For{$j=1,\ldots,g$} (relabel for binary case)
						\State $\pi_j(\xi)  \leftarrow \frac{1}{|\tilde M| } \sum_{m \in \tilde M}   \pi_j(\xi~|~ f_m) $
					\EndFor}
                        \textcolor{red}{\State $\tilde y^{(i)} \leftarrow \argmin_{i^*} \sum_{j=1}^g \pi_j(\xi) C(i^*,j) $
					\State $\tilde D \leftarrow \tilde D \cup \{(\xi,\tilde y^{(i)})\} $
				\EndFor}
				\State \# 3rd Cost Sensitivity:
%				
				\textcolor{orange}{\State $f_{meta} \leftarrow$ train $f$ on $\tilde D$}
%			
			\end{algorithmic}
		\end{minipage}
		%
	}
	%	
\end{vbframe}

%%%%%



%
\endlecture
\end{document}
