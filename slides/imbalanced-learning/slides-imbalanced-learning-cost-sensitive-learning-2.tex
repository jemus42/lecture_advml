\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

%\usepackage{algorithm}
%\usepackage{algorithmic}

\newcommand{\sens}{\mathbf{A}} % vector x (bold)
\newcommand{\ba}{\mathbf{a}}
\newcommand{\batilde}{\tilde{\mathbf{a}}}
\newcommand{\Px}{\mathbb{P}_{x}} % P_x
\newcommand{\Pxj}{\mathbb{P}_{x_j}} % P_{x_j}
\newcommand{\indep}{\perp \!\!\! \perp} % independence symbol
% ml - ROC
\newcommand{\np}{n_{+}} % no. of positive instances
\newcommand{\nn}{n_{-}} % no. of negative instances
\newcommand{\rn}{\pi_{-}} % proportion negative instances
\newcommand{\rp}{\pi_{+}} % proportion negative instances
% true/false pos/neg:
\newcommand{\tp}{\# \text{TP}} % true pos
\newcommand{\fap}{\# \text{FP}} % false pos (fp taken for partial derivs)
\newcommand{\tn}{\# \text{TN}} % true neg
\newcommand{\fan}{\# \text{FN}} % false neg
\newcommand{\qv}{\mathbf{q}}  % vector q (bold)
\newcommand{\cv}{\mathbf{c}}    % vector c (bold)

\usepackage{multicol}

\newcommand{\titlefigure}{figure/cost_matrix}
\newcommand{\learninggoals}{
  \item Learn threshold adjusting in cost-sensitive learning
  \item Get to know MetaCost as a general approach to make classifiers cost-sensitve
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Imbalanced Learning via Cost-Sensitive Learning Part 2}
\lecture{Advanced Machine Learning}



\sloppy



\begin{vbframe}{Threshold adjusting: binary case}
    \footnotesize 
    \begin{itemize}
        \footnotesize
        \item Motivation: The threshold provided by Minimum Expected Cost is not always the truly optimal due to e.g. wrong model class, finite data, etc.
        \item Key idea: model the mis-classification cost $M_C$ as a function $M_C = g(T)$ of \textbf{probability threshold} $T \in [0, 1]$.
        \item By adjusting $T$, we can plot a curve of $M_C$ versus $T$ as the following figure (copied from Fig.1 in \href{https://www.aaai.org/Library/AAAI/2006/aaai06-076.php}{\beamergotobutton{Sheng et al. 2006}}):
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{slides/imbalanced-learning/figure/threshold_adjusting.png}
        \end{figure}

        \item The optimal threshold $T^{*}$ corresponds to the point with the smallest $M_C$.

        \item What if two local minima? We prefer the one whose ``valley'' has a wider span (e.g. $T_2$ in the 3rd subfigure). Because it is less sensitive to small changes in $T$.

        \item Search the best $T$ on \textbf{validation} sets, and do $K$-fold cross-validation to avoid overfitting!
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Threshold adjusting: binary case}
    \footnotesize 
    \begin{itemize}
        \scriptsize
        \item Example: Consider the German Credit Task, where customers are classified as "good" or "bad". The cost of classifying a bad customer as good is higher than vice versa. Define the following cost matrix:
        \begin{center}
                            \begin{tabular}{cc|cc}
        			& &\multicolumn{2}{c}{True class} \\
        			& & $y=$ good & $y=$ bad  \\
        			\hline
        			\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}} & $\hat y$ = good & 0 & 3 \\
        			& $\hat y$ = bad & 1 & 0\\
                \end{tabular}
        \end{center}
        \item We can compute the optimal theoretical threshold value by $C(good,bad)/(C(bad,good)+C(good,bad))=3/4=c^{*}$.

        \item Now, train three classifiers with threefold cross-validation and find the best empirical threshold $T$ on the validation sets. For XGBoost, the empirical minimum deviates substantially from the theoretical minimum in comparison.

                \begin{figure}[h]
            \centering
            \includegraphics[width=0.7\textwidth]{slides/imbalanced-learning/figure_man/Cost_Curves_plot.png}
        \end{figure}

    \end{itemize}
\end{vbframe}


\begin{vbframe}{Threshold Adjusting: Multi-class Case}
    \footnotesize
    \begin{itemize}
        \footnotesize
        \item It is also possible to perform threshold adjusting in multi-class classifcation.
        
        \item Here, we present a simple approach for classifiers that output scores $\pi(\xv) = (\pi(\xv)_1,\ldots,\pi(\xv)_g)^\top$ in multi-class classification:
        
        \begin{itemize}
            \footnotesize
            \item In the standard setting, we predict class $h(\xv) = \argmax_{j} \pi(\xv)_j$.
            \vspace{10pt}
            
            \item Now, we model the miss-classification costs $M_C$ as a function $M_C = g(\mathbf{T})$ where $\mathbf{T} \in \R^{g}$.
            \vspace{10pt}
            
            \item At step 1, we re-scale the scores $\tilde{\pi}(\xv) = (\frac{\pi(\xv)_1}{\mathbf{T}_1},\ldots, \frac{\pi(\xv)_g}{ \mathbf{T}_g})^\top$, and predict class $\tilde{h}(x) = \argmax_j \tilde{\pi}(\xv)_j$.
            \vspace{10pt}

            \item At step 2, we compute $M_C$ using the cost matrix $C$ and $\tilde{h}(x)$.
            \vspace{10pt}

            \item Similar to the binary case, we adjust $\mathbf{T}$ for $g$ classes to achieve the minimal mis-classification cost $M_C$.
            \vspace{10pt}

        \end{itemize}
    \end{itemize}
\end{vbframe}

\begin{vbframe}{MetaCost: Overview}

	\small{
	\begin{itemize}

		\item Instead of making individual algorithms cost-sensitive, a better solution would be converting cost-insensitive classifiers into cost-sensitive ones.

		\item MetaCost is a wrapper method:
        \begin{itemize}
            \small
            \item can be used for \emph{any} type of classifier to obtain a cost-sensitive classifier.
            \vspace{5pt}
            
            \item treats underlying classifier as a black-box, $\leadsto$ no knowledge about its mechanism is required, and no changes to its mechanism is needed.
            \vspace{5pt}
            
            \item needs only a cost-matrix $\mathbf{C}$, which is used to adapt the decision boundaries. {\scriptsize (Some tuning parameters are also needed.)}
            \vspace{5pt}
            
            \item The rough procedure: 
                \begin{enumerate}
                    \small
                    \item relabel the training examples with their ``optimal'' classes, i.e., the ones with low expected costs;
                    \vspace{5pt}
                    
                    \item apply the classifier on the relabeled data set.
                \end{enumerate} 
        \end{itemize}

	\end{itemize}

	}

\end{vbframe}


\begin{vbframe}{MetaCost: Algorithm}
	
	\scriptsize{
%		
 	The procedure of MetaCost is divided into three phases:
			%			
		\begin{minipage}{0.53\textwidth} 
%				
				\begin{enumerate}
%					
					\scriptsize
					\item \textcolor{teal}{Bagging --- The underlying classifier is used (trained) $L$ times on different bootstrapped samples of the training data, respectively.}
%					

     				\item \textcolor{blue}{Relabeling --- The index set is the union of the classifier indices for which $\xi$ is OOB.}

                        \item \textcolor{magenta}{ Relabeling --- Compute $\pi_j(\xi)$ by averaging over the predictions of the OOB classifiers.}

                        \item \textcolor{red}{ Relabeling --- Determine new label $\tilde y^{(i)}$ w.r.t. the class that minimzes the sum of the weighted costs over the $g$ classes.}
%					
					\item \textcolor{orange}{Cost-sensitivity ---  The classifier is trained on the relabeled data set, resulting in a cost-sensitive classifier.}
%					
				\end{enumerate}
%

%		
		
		\end{minipage}
		\begin{minipage}{0.45\textwidth} 
			\begin{algorithmic}
				
				\tiny
%				
				\State \textbf{MetaCost}  
				\State \textbf{Input:} 
				$\D = \{(\xi,\yi)\}_{i=1}^n$ training data, \\
				$M \in \N$ number of bagging iterations, \\
				$B \in \N$ bootstrap size, \\
				$f$ probabilistic (black-box) classifier, 
				$\mathbf{C}$ cost matrix, 
%				$g = |\Yspace|$ number of classes
				\State \# 1st Bagging:
				\textcolor{teal}{\For{$m=1,\ldots,M$}
					\State $\D_m  \leftarrow $ BootstrapSample ($\D,B$)
					\State $f_m  \leftarrow $ train $f$ on $\D_m$
				\EndFor}
				\State \# 2nd Relabeling:
				\textcolor{blue}{\For{$i=1,\ldots,n$}
					\State $\tilde M \leftarrow \bigcup_{m: \xi \notin \D_m} \{m\}$
					\EndFor}
					\textcolor{magenta}{\For{$j=1,\ldots,g$} (relabel for binary case)
						\State $\pi_j(\xi)  \leftarrow \frac{1}{|\tilde M| } \sum_{m \in \tilde M}   \pi_j(\xi~|~ f_m) $
					\EndFor}
                        \textcolor{red}{\State $\tilde y^{(i)} \leftarrow \argmin_{i^*} \sum_{j=1}^g \pi_j(\xi) C(i^*,j) $
					\State $\tilde D \leftarrow \tilde D \cup \{(\xi,\tilde y^{(i)})\} $
				\EndFor}
				\State \# 3rd Cost Sensitivity:
%				
				\textcolor{orange}{\State $f_{meta} \leftarrow$ train $f$ on $\tilde D$}
%			
			\end{algorithmic}
		\end{minipage}
		%
	}
	%	
\end{vbframe}

%%%%%



%%%
% reference: https://www.csie.ntu.edu.tw/~htlin/talk/doc/csovo.acml14.handout.pdf

\begin{vbframe}{CSOVO with instance-specific costs}
    \footnotesize
    \begin{itemize}
        \footnotesize
        \item $\Dn = \{(\xv_i, y_i, \cv_i)\}_i^n$, where $(\xv_i, y_i, \cv_i) \in \mathbb{R}^d \times \{1, 2, \ldots, g\} \times \mathbb{R}^g$.
        \vspace{5pt}
        
        \item Previous discussed cost-sensitive learning is a special case: $\cv_i$ same for all $\xv_i$.
        \vspace{5pt}
        
        \item Cost vector: $\cv_i[y_i] = 0$, i.e. zero-cost for correct prediction. Example:
        \begin{equation*}
            \begin{aligned}
                \mat{
                    \xv_1 & 1 & \mat{0 & 2 & 3 & 1} \\
                    \xv_2 & 2 & \mat{1 & 0 & 1 & 2} \\
                    \xv_3 & 4 & \mat{2 & 1 & 1 & 0} \\
                }
            \end{aligned}
        \end{equation*}
        \vspace{5pt}
        \item Now: How to perform cost-sensitive learning when $g > 2$?
        \vspace{5pt}
        
        \item Idea: we have learnt cost-sensitive learning for the binary case. $\leadsto$ Convert the problem to one-versus-one (OVO).
        \vspace{5pt}
        
        \item But for class $j$ vs. $k$, consider two problems:
        \begin{itemize}
            \footnotesize
            \item How to deal with the label $y_i$? $y_i$ can be neither $j$ nor $k$.
            \vspace{5pt}
            
            \item How to deal with the costs $\cv_i[j]$ and $\cv_i[k]$?
        \end{itemize}
            
    \end{itemize}
\end{vbframe}

\begin{vbframe}{CSOVO with instance-specific costs}
    \footnotesize
    \begin{itemize}
        \item When training a binary classifier $f^{(j, k)}$ for class $j$ vs. $k$,
        \begin{itemize}
            \footnotesize
            \item It is natural to select $\argmin_{l \in \{j, k\}} \cv_i[l]$ as the ground truth class.
            \vspace{5pt}
            
            \item The costs $\cv_i[j]$ and $\cv_i[k]$ can be transformed to a \textbf{sample-wise} weight $|\cv_i[j] - \cv_i[k]|$. Please refer to \href{https://proceedings.mlr.press/v39/lin14.pdf}{\beamergotobutton{Lin et al. 2014}} for details.
            \vspace{5pt}

            \item After cost transformation, the misclassification costs across classes are equal to 1. $\leadsto$ Sample-wise weighted binary classification problem.
            \vspace{5pt}
        \end{itemize}
        
        \item Example continued: Consider instance $(\xv_1, y_1, \cv_1) =(\xv_1, 1, \mat{0 & 2 & 3 & 1})$.
        \begin{itemize}
        \footnotesize
            \item For class 1 vs. 2, $\argmin_{l \in \{1, 2\}} \cv_1[l] = 1$ remains as the ground truth class. The sample-wise weight is $|\cv_1[1] - \cv_1[2]| = 2$.
            \vspace{5pt}
            
            \item Calculate new costs: $\tilde \cv_1[1] = \cv_1[1]/(|\cv_1[1] - \cv_1[2]|) = 0$ and $\tilde \cv_1[2] = 1$.
            \vspace{5pt}

            \item For class 2 vs. 3, $\argmin_{l \in \{2, 3\}} \cv_1[l] = 2$ becomes the new ground truth class. The sample-wise weight is $|\cv_1[2] - \cv_1[3]| = 3$.
            \vspace{5pt}
            
            \item Calculate new costs: $\tilde \cv_1[2] = \cv_1[2]/(|\cv_1[2] - \cv_1[3]|) = 0$ and $\tilde \cv_1[3] = 1$.
            \vspace{5pt}           

        \end{itemize}

    \end{itemize}
\end{vbframe}


\begin{vbframe}{CSOVO with instance-specific costs}
    \footnotesize
    \begin{itemize}

    \item Wrap everything up:
    \begin{enumerate}
        \footnotesize
        \item For class $j$ vs. $k$, transform all $(\xv_i, y_i, \cv_i)$ to $(\xv_i, \argmin_{l \in \{j, k\}} \cv_i[l])$ with sample-wise weight $|\cv_i[j] - \cv_i[k]|$.
        \vspace{5pt}
        
        \item Train a binary classifier $f^{(j, k)}$ using the transformed samples.
        \vspace{5pt}
        
        \item Repeat step 1 and 2 for different $(i, j)$.
        \vspace{5pt}
        
        \item Predict using the votes from all $f^{(i, j)}$.
    \end{enumerate}
    \vspace{5pt}

    \item Theoretical guarantee: test of final classifier $\leq 2\sum_{j < k}$ test cost of $f^{(i, j)}$. 
    \end{itemize}
\end{vbframe}


%\begin{vbframe}{MetaCost: Example}
%	%	
%	\small{
%		\begin{itemize}
%			%		
%			\item We compare C4.5 (decision tree) with MetaCost using C4.5 for the \href{http://staffwww.itn.liu.se/~aidvi/courses/06/dm/
%				labs/heart-c.arff}{heart data set}  in \href{ http://www.cs.waikato.ac.nz/ml/weka/}{Weka}. 
%			%		
%			\item The cost matrix $\mathbf{C}$ is 
%			
%			\begin{center}
%				\begin{tabular}{cc|cc}
%					& &\multicolumn{2}{c}{True class} \\
%					& & $y=1$ & $y=-1$  \\
%					\hline
%					\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $0$                & $ 1 $\\
%					& $\hat y$ = -1 & $ 4 $              &  $0$   \\
%				\end{tabular}
%			\end{center}
%		
%			\item The resulting confusion matrices are 
%			
%						
%			\begin{center}
%				\begin{tabular}{cc|cc}
%					& &\multicolumn{2}{c}{True class} \\
%					& MetaCost & $y=1$ & $y=-1$  \\
%					\hline
%					\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $104$                & $ 21 $\\
%					& $\hat y$ = -1 & $ 61 $              &  $117$   \\
%				\end{tabular}
%							\begin{tabular}{cc|cc}
%				& &\multicolumn{2}{c}{True class} \\
%				& C4.5 & $y=1$ & $y=-1$  \\
%				\hline
%				\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $138$                & $ 40 $\\
%				& $\hat y$ = -1 & $ 27$              &  $98$   \\
%			\end{tabular}
%			\end{center}
%		
%		
%			%		
%			\item The total cost of MetaCost is 145, while C4.5 has total costs of 187. However, MetaCost has $0.729$ correct classifications and C4.5 has $0.779.$ 
%			
%			%		
%		\end{itemize}
%		%
%	}
%	%	
%\end{vbframe}



%\begin{vbframe}{Cost-Sensitive Decision Trees}
%	%	
%	\small{
%		\begin{itemize}
%			%		
%			\item 
%			%		
%		\end{itemize}
%		%
%	}
%	%	
%\end{vbframe}


%
\endlecture
\end{document}
