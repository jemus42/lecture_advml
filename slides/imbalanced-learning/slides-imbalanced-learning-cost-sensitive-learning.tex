\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

%\usepackage{algorithm}
%\usepackage{algorithmic}

\newcommand{\sens}{\mathbf{A}} % vector x (bold)
\newcommand{\ba}{\mathbf{a}}
\newcommand{\batilde}{\tilde{\mathbf{a}}}
\newcommand{\Px}{\mathbb{P}_{x}} % P_x
\newcommand{\Pxj}{\mathbb{P}_{x_j}} % P_{x_j}
\newcommand{\indep}{\perp \!\!\! \perp} % independence symbol
% ml - ROC
\newcommand{\np}{n_{+}} % no. of positive instances
\newcommand{\nn}{n_{-}} % no. of negative instances
\newcommand{\rn}{\pi_{-}} % proportion negative instances
\newcommand{\rp}{\pi_{+}} % proportion negative instances
% true/false pos/neg:
\newcommand{\tp}{\# \text{TP}} % true pos
\newcommand{\fap}{\# \text{FP}} % false pos (fp taken for partial derivs)
\newcommand{\tn}{\# \text{TN}} % true neg
\newcommand{\fan}{\# \text{FN}} % false neg
\newcommand{\qv}{\mathbf{q}}  % vector q (bold)
\newcommand{\cv}{\mathbf{c}}    % vector c (bold)

\usepackage{multicol}

\newcommand{\titlefigure}{figure/cost_matrix}
\newcommand{\learninggoals}{
  \item Learn cost matrix modelling in cost-sensitive learning
  \item Understand Minimum Expected Cost Principle
  \item Learn threshold adjusting in cost-sensitive learning
  \item Get to know MetaCost as a general approach to make classifiers cost-sensitve
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Imbalanced Learning via Cost-Sensitive Learning}
\lecture{Advanced Machine Learning}



\sloppy


\begin{vbframe}{Cost-Sensitive learning: In a Nutshell}
	%	
	\scriptsize{

		\begin{itemize}
		
    		\item Cost-sensitive learning: 
                \begin{itemize}
                    \scriptsize
                    \item Different (mis-)classification costs are taken into consideration.
                    \item The learner seeks to minimize the total costs in expectation.
                    \item Classical learning assumes that the data sets are balanced, and all errors have the same cost.
                \end{itemize}
    		
    		\item Real-world applications, where different costs between misclassifications are present:
      
    		\begin{itemize}
    			\scriptsize	
    			\item Medicine --- Misdiagnosing a cancer patient as healthy vs.\ misdiagnosing a healthy patient as having cancer (and then check again).
       
    			\item (Extreme) Weather prediction ---  Incorrectly predicting that no hurricane occurs vs.\ predicting a strong wind as a hurricane.
    	
    			\item Credit granting --- Lending to a risky client vs. not lending to a trustworthy client.
    		\end{itemize}
         
		
		\end{itemize}
        \vspace{15pt}

        \begin{minipage}{0.49\textwidth}
            \begin{table}[]
                \centering
                \begin{tabular}{p{1cm}c|cc}
                    & &\multicolumn{2}{c}{Truth} \\
                    & & With Tumor & No Tumor  \\
                    \hline
                    \multirow{2}{*}{\parbox{1cm}{Diagnosis}} & Malignent & 0 & $ 1 $\\
                    & Benign & $10000$ & $0$   \\
                \end{tabular}
            \end{table}
        \end{minipage}
        \hfill
        \begin{minipage}{0.49\textwidth}
            \begin{itemize}
                \scriptsize
                \item In these examples, \textbf{the costs of a false negative is much higher than the costs of a false positive}.
                \vspace{15pt}
                
                \item In some applications, the costs are \textbf{unknown} $\leadsto$ need to be specified by experts, or be learnt.
            \end{itemize}   
        \end{minipage}
		
	}
\end{vbframe}


\begin{vbframe}{Cost matrix}
%	
\scriptsize{
%	
	\begin{itemize}
%		
		\item In cost-sensitive learning we are provided with a cost matrix $\mathbf{C}$ of the form
%		
	\end{itemize}
	%	
	\begin{center}
		\tiny
		\begin{tabular}{cc|>{\centering\arraybackslash}p{8em}>{\centering\arraybackslash}p{8em}>{\centering\arraybackslash}p{5em}>{\centering\arraybackslash}p{8em}}
			& & \multicolumn{4}{c}{\bfseries True Class $y$} \\
			&  & $1$ & $2$ & $\ldots$ & $g$  \\
			\hline
			\bfseries Classification     & $1$ & $C(1,1)$  &  $C(1,2)$  & $\ldots$ &  $C(1,g)$ \\
			& $2$ &  $C(2,1)$  &  $C(2,2)$  & $\ldots$ & $C(2,g)$  \\
            $\yh$ & & & & & \\
			& $\vdots$ & $\vdots$ & $\vdots$ & $\ldots$ & $\vdots$ \\
			& $g$ & $C(g,1)$ & $C(g,2)$  & $\ldots$ &  $C(g,g)$\\
		\end{tabular}
	\end{center}
	%	
	\begin{itemize}
		%		
		\item $C(i,j)$ is the cost of classifying $j$ as $i,$ which in the cost-insensitive learning case is simply $C(i,j) = \mathds{1}_{[ i \neq j ]},$ i.e., each misclassification has the same cost of 1.
		\vspace{10pt}
		
		\item Cost matrix is usually designed by experts with domain knowledge.
        \vspace{10pt}
		\begin{enumerate}
			\scriptsize	
            \item Too low costs: might not change the decision boundaries significantly, leading to (still) costly predictions.
            \vspace{10pt}
            \item Too high costs: might harm the generalization capability of the classifier on costly classes.
            \vspace{10pt}
		\end{enumerate}
		
	\end{itemize}

}
\end{vbframe}


\begin{vbframe}{Cost matrix for Imbalanced Learning}
	\footnotesize{
		\begin{itemize}			
	 
			\item A common heuristic for imbalanced data sets is to use 
	
			\begin{itemize}
				\footnotesize 
                \item $C(i,j) = \frac{n_i}{n_j}$ for classes $i$ and $j$ such that $n_j  \ll n_i$, i.e. for misclassifying a minority class $j$ as a majority class $i$.
                \vspace{10pt}
		
				\item $C(i,j) = 1$ for classes $i$ and $j$ such that $n_i \ll n_j$, i.e. for misclassifying a majority class $j$ as a minority class $i$.
                \vspace{10pt}
			
				\item 0 for a correct classification (a theoretical reason will be explained later).
                \vspace{10pt}
			
			\end{itemize}

    		\item In an imbalanced binary classification we get: \\
            \begin{table}[]
                \centering
                \begin{tabular}{cc|cc}
                    & &\multicolumn{2}{c}{True class} \\
                    & & $y=1$ & $y=-1$  \\
                    \hline
                    \multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $0$                & $ 1 $\\
                    & $\hat y$ = -1 & $ \frac{n_-}{n_+} $              &  $0$   \\
                \end{tabular}
            \end{table}
    		

            \item Thus, this heuristic is consistent with the general real case that the cost of false negatives is much higher than the cost of false positives.	
        \end{itemize}
		
	}
\end{vbframe}


\begin{vbframe}{Minimum expected Cost Principle}

	\footnotesize{


		\begin{itemize}\footnotesize
		
			\item Suppose we have:
            \begin{itemize}
                \footnotesize
                \item a cost matrix $\mathbf{C}$,
                \vspace{10pt}
                
                \item knowledge of the true posterior distribution $p(\cdot ~|~ \xv).$
                \vspace{10pt}
            \end{itemize}

            \item Idea for classifying a given feature $\xv$: 
            \begin{itemize}
                \footnotesize
                \item Compute the cost for classifying $\xv$ as $i$. (infeasible to directly compute.)
                \vspace{10pt}
                
                \item $\leadsto$ Marginalize over ``true'' class $j$.
                \vspace{10pt}
            \end{itemize}

			\item \emph{Minimum expected cost principle}: Use the class for prediction with the smallest expected costs, where the expected costs of a class $i\in\{1,\ldots,g\}$ is
	
			$$ 	\E_{J \sim p(\cdot ~|~ \xv)}( C(i,J) ) = \sum_{j=1}^g 	p(j ~|~ \xv) C(i,j).	$$

			
				
		\end{itemize}
	}
\end{vbframe}

\begin{vbframe}{Minimum expected cost principle}
    \footnotesize
    \begin{itemize}
        \item Thus, if we have a classifier $f$ which uses a probabilistic score function $\pi:\Xspace \to [0,1]^g$ with $\pi(\xv) = (\pi(\xv)_1,\ldots,\pi(\xv)_g)^\top$ and $\sum_{j=1}^g \pi(\xv)_j = 1$ for the classification, then one can easily modify $h$ to take the expected costs into account:
		$$  \tilde h(\xv) = \argmin_{i=1,\ldots,g} \sum_{j=1}^g 	\pi(\xv)_j C(i,j). $$
        \vspace{10pt}

        \item For $\xv$, making prediction $i$ means \textbf{acting as if $i$ is the true class of $\xv$.}
        \vspace{10pt}
        
        \item The essence of cost-sensitive decision-making: it can be optimal to act as if class $i$ is true even when $j$ is more probable. (\href{https://dl.acm.org/doi/10.5555/1642194.1642224}{\beamergotobutton{Elkan et. al. 2001}})
        \vspace{10pt}

        \item Now consider: 
        \begin{itemize}
            \footnotesize
            \item If we learnt a binary classifier: $p(1 ~|~ \xv) = \pi(\xv)_1$ and $p(-1 ~|~ \xv) = \pi(\xv)_2$,
            \item Under what condition should we predict $\xv$ as class $1$?
        \end{itemize}
    \end{itemize}
\end{vbframe}


\begin{vbframe}{Minimum expected Cost Principle: Binary Case}
	%	
	\footnotesize{
		%		
		\begin{itemize}
            \footnotesize
            \item Preliminary knowledge: the optimal decisions do NOT change if 
            \begin{itemize}
                \footnotesize
                \item $\mathbf{C}$ is multipled by a positive constant.
                \item $\mathbf{C}$ is added with a constant shift.
            \end{itemize}

            \item Hence, we scale and shift $\mathbf{C}$ to eliminate some entries, obtaining a simpler $\mathbf{C}^\prime$: 
            \begin{table}[]
                \centering
                    \begin{tabular}{cc|cc}
        			& &\multicolumn{2}{c}{True class} \\
        			& & $y=1$ & $y=-1$  \\
        			\hline
        			\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}} & $\hat y$ = 1 & $C^\prime(1,1)$ & $1$ \\
        			& $\hat y$ = -1 & $C^\prime(-1, 1)$ & 0\\
                \end{tabular}
            \end{table}
            where 
            \begin{itemize}
                \footnotesize
                \item $C^\prime (-1, 1) = [C(-1, 1) - C(-1, -1)] / [C(1, -1) - C(-1, -1)]$,
                \item $C^\prime (1, 1) = [C(1, 1) - C(-1, -1)] / [C(1, -1) - C(-1,-1)]$.
            \end{itemize}

            \item We predict $\xv$ as class $1$ if \\
            \begin{align*}
                \E_{J \sim p(\cdot ~|~ \xv)}( C^\prime(1,J) )  \leq \E_{J \sim p(\cdot ~|~ \xv)}( C^\prime(-1,J) ) \\
            \end{align*}

			\item Plugging $\mathbf{C}^\prime$ into the inequality above, we obtain:	
			\begin{align*}	
                & p(-1 ~|~ \xv ) C^\prime(1,-1)  + 	p(1 ~|~ \xv ) C^\prime(1,1) \leq  p(-1 ~|~ \xv ) C^\prime(-1,-1)  + 	p(1 ~|~ \xv ) C^\prime(-1,1)  \\ 
                &\Rightarrow [1 - p(1 ~|~ \xv)] \cdot 1 + p(1 ~|~ \xv) C^\prime(1, 1) \leq p(1 ~|~ \xv) C^\prime(1, -1) \\
                &\Rightarrow p(1 ~|~ \xv) \geq \frac{1}{C^\prime(-1, 1) - C^\prime(1, 1) + 1} \\
                &\Rightarrow p(1 ~|~ \xv) \geq \frac{C(1, -1) - C(-1, -1)}{C(-1, 1) - C(1, 1) + C(1, -1) - C(-1, -1)}
			\end{align*}
		
            \item By further assuming that $C(1, 1) = C(-1, -1) = 0 $, this can by simplied to 
            \begin{align*}
                p(1 ~|~ \xv) \geq \frac{C(1, -1)}{C(-1, 1) + C(1, -1)} = c^{*}
            \end{align*}	
            
            \item This yields the optimal threshold value $c^*$ for probabilistic score classifiers, so that any probabilistic classifier $h$ using a probabilistic score $\pi:\Xspace \to [0,1]$ can be modified to 
            
            $$   \tilde h(\xv) = 2 \cdot \mathds{1}_{[ \pi(\xv) \geq c^*]} -1. $$
							
		\end{itemize}
	}
\end{vbframe}

\begin{vbframe}{Threshold adjusting for imbalanced learning: binary case}
    \footnotesize 
    \begin{itemize}
        \footnotesize
        \item Motivation: The threshold provided by Minimum Expected Cost is not always the truly optimal due to e.g. wrong model class, finite data, etc.
        \item Key idea: model the mis-classification cost $M_C$ as a function $M_C = g(T)$ of \textbf{probability threshold} $T \in [0, 1]$.
        \item By adjusting $T$, we can plot a curve of $M_C$ versus $T$ as the following figure (copied from Fig.1 in \href{https://www.aaai.org/Library/AAAI/2006/aaai06-076.php}{\beamergotobutton{Sheng et al. 2006}}):
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{slides/imbalanced-learning/figure/threshold_adjusting.png}
        \end{figure}

        \item The optimal threshold $T^{*}$ corresponds to the point with the smallest $M_C$.

        \item What if two local minima? We prefer the one whose ``valley'' has a wider span (e.g. $T_2$ in the 3rd subfigure). Because it is less sensitive to small changes in $T$.

        \item Search the best $T$ on \textbf{validation} sets, and do $K$-fold cross-validation to avoid overfitting!
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Threshold Adjusting for Imbalanced Learning: Multi-class Case}
    \footnotesize
    \begin{itemize}
        \footnotesize
        \item It is also possible perform threshold adjusting in multi-class classifcation.
        
        \item Here, we present a simple approach for classifiers that output scores $\pi(\xv) = (\pi(\xv)_1,\ldots,\pi(\xv)_g)^\top$ in multi-class classification:
        
        \begin{itemize}
            \footnotesize
            \item In the standard setting, we predict class $h(\xv) = \argmax_{j} \pi(\xv)_j$.
            \vspace{10pt}
            
            \item Now, we model the miss-classification costs $M_C$ as a function $M_C = g(\mathbf{T})$ where $\mathbf{T} \in \R^{g}$.
            \vspace{10pt}
            
            \item At step 1, we re-scale the scores $\tilde{\pi}(\xv) = (\frac{\pi(\xv)_1}{\mathbf{T}_1},\ldots, \frac{\pi(\xv)_g}{ \mathbf{T}_g})^\top$, and predict class $\tilde{h}(x) = \argmax_j \tilde{\pi}(\xv)_j$.
            \vspace{10pt}

            \item At step 2, we compute $M_C$ using the cost matrix $C$ and $\tilde{h}(x)$.
            \vspace{10pt}

            \item Similar to the binary case, we adjust $\mathbf{T}$ for $g$ classes to achieve the minimal mis-classification cost $M_C$.
            \vspace{10pt}

        \end{itemize}
    \end{itemize}
\end{vbframe}

\begin{vbframe}{MetaCost: Overview}

	\small{
	\begin{itemize}

		\item In stead of making individual algorithms cost-sensitive, a better solution would be converting cost-insensitive classifiers into cost-sensitive ones.

		\item MetaCost is a wrapper method:
        \begin{itemize}
            \small
            \item can be used for \emph{any} type of classifier to obtain a cost-sensitive classifier.
            \vspace{5pt}
            
            \item treats underlying classifier as a black-box, $\leadsto$ no knowledge about its mechanism is required, and no changes to its mechanism is needed.
            \vspace{5pt}
            
            \item needs only a cost-matrix $\mathbf{C}$, which is used to adapt the decision boundaries. {\scriptsize (Some tuning parameters are also needed.)}
            \vspace{5pt}
            
            \item The rough procedure: 
                \begin{enumerate}
                    \small
                    \item relabel the training examples with their ``optimal'' classes, i.e., the ones with low expected costs;
                    \vspace{5pt}
                    
                    \item apply the classifier on the relabeled data set.
                \end{enumerate} 
        \end{itemize}

	\end{itemize}

	}

\end{vbframe}


\begin{vbframe}{MetaCost: Algorithm}
	
	\scriptsize{
%		
 	The procedure of MetaCost is divided into three phases:
			%			
		\begin{minipage}{0.53\textwidth} 
%				
				\begin{enumerate}
%					
					\scriptsize
					\item Bagging --- The underlying classifier is used (trained) $L$ times on different bootstrapped samples of the training data, respectively.
%					
					\item Relabeling --- These $L$ trained classifiers are used to relabel the original training data set by taking the cost-matrix into account.
%					
					\item Cost-sensitivity ---  The classifier is trained on the relabeled data set resulting in a cost-sensitive classifier.
%					
				\end{enumerate}
%
	\scriptsize
	\lz 
			Predictions of the classifier $f$ are converted (if necessary) into probabilistic prediction:
			\begin{center}
				\tiny
							ProbPrediction$(j,f,\xv) = \begin{cases}
					(f(\xv))_j & \mbox{$f$ is a prob.\ classifier,} \\
					\mbox{One-hot$(f(\xv))$}& \mbox{else,}
				\end{cases}$
			\end{center}
%		
		\scriptsize
		where One-hot$(f(\xv))$ uses a one-hot-encoding of the prediction to make it a probability. 
		
		\end{minipage}
		\begin{minipage}{0.45\textwidth} 
			\begin{algorithmic}
				
				\tiny
%				
				\State \textbf{MetaCost}  
				\State \textbf{Input:} 
				$\D = \{(\xi,\yi)\}_{i=1}^n$ training data, \\
				$L \in \N$ number of bagging iterations, \\
				$B \in \N$ bootstrap size, \\
				$f$ (black-box) classifier, 
				$\mathbf{C}$ cost matrix, 
%				$g = |\Yspace|$ number of classes
				\State \# 1st phase:
				\For{$l=1,\ldots,L$}
					\State $\D_l  \leftarrow $ BootstrapSample ($\D,B$)
					\State $f_l  \leftarrow $ train $f$ on $\D_l$
				\EndFor
				\State \# 2nd phase:
				\For{$i=1,\ldots,n$}
					\If{$\xi \in \D_l$ for all $l=1,\ldots,L$}
						\State $\tilde L \leftarrow \{1,\ldots,L\}$
					\Else
						\State $\tilde L \leftarrow \bigcup_{l: \xi \notin \D_l} \{l\}$
					\EndIf
					\For{$j=1,\ldots,g$} (relabel for binary case)
						\State $p_j(\xi)  \leftarrow \frac{1}{|\tilde L| } \sum_{l \in \tilde L}   p_j(\xi~|~ f_l) $
						\State $p_j(\xi~|~ f_l) = $ ProbPrediction$(j,f_l,\xi)$
						\State $\tilde y^{(i)} \leftarrow \argmin_{i^*} \sum_{j=1}^g p_j(\xi) C(i^*,j) $
					\EndFor
					\State $\tilde D \leftarrow \tilde D \cup \{(\xi,\tilde y^{(i)})\} $
				\EndFor
				\State \# 3rd phase:
%				
				\State $f_{meta} \leftarrow$ train $f$ on $\tilde D$
%			
			\end{algorithmic}
		\end{minipage}
		%
	}
	%	
\end{vbframe}

%%%%%



%%%
% reference: https://www.csie.ntu.edu.tw/~htlin/talk/doc/csovo.acml14.handout.pdf

\begin{vbframe}{CSOVO with instance-specific costs}
    \footnotesize
    \begin{itemize}
        \footnotesize
        \item $\Dn = \{(\xv_i, y_i, \cv_i)\}_i^n$, where $(\xv_i, y_i, \cv_i) \in \mathbb{R}^d \times \{1, 2, \ldots, g\} \times \mathbb{R}^g$.
        \vspace{5pt}
        
        \item Previous discussed cost-sensitive learning is a special case: $\cv_i$ same for all $\xv_i$.
        \vspace{5pt}
        
        \item Cost vector: $\cv_i[y_i] = 0$, i.e. zero-cost for correct prediction. Example:
        \begin{equation*}
            \begin{aligned}
                \mat{
                    \xv_1 & 1 & \mat{0 & 2 & 3 & 1} \\
                    \xv_2 & 2 & \mat{1 & 0 & 1 & 2} \\
                    \xv_3 & 4 & \mat{2 & 1 & 1 & 0} \\
                }
            \end{aligned}
        \end{equation*}
        \vspace{5pt}
        \item Now: How to perform cost-sensitive learning when $g > 2$?
        \vspace{5pt}
        
        \item Idea: we have learnt cost-sensitive learning for the binary case. $\leadsto$ Convert the problem to one-versus-one (OVO).
        \vspace{5pt}
        
        \item But for class $j$ vs. $k$, consider two problems:
        \begin{itemize}
            \footnotesize
            \item How to deal with the label $y_i$? $y_i$ can be neither $j$ nor $k$.
            \vspace{5pt}
            
            \item How to deal with the costs $\cv_i[j]$ and $\cv_i[k]$?
        \end{itemize}
            
    \end{itemize}
\end{vbframe}

\begin{vbframe}{CSOVO with instance-specific costs}
    \footnotesize
    \begin{itemize}
        \item When training a binary classifier $f^{(j, k)}$ for class $j$ vs. $k$,
        \begin{itemize}
            \footnotesize
            \item It is natural to select $\argmin_{l \in \{j, k\}} \cv_i[l]$ as the ground truth class.
            \vspace{5pt}
            
            \item The costs $\cv_i[j]$ and $\cv_i[k]$ can be transformed to a \textbf{sample-wise} weight $|\cv_i[j] - \cv_i[k]|$. Please refer to \href{https://proceedings.mlr.press/v39/lin14.pdf}{\beamergotobutton{Lin et al. 2014}} for details.
            \vspace{5pt}

            \item After cost transformation, the misclassification costs across classes are equal to 1. $\leadsto$ Sample-wise weighted binary classification problem.
            \vspace{5pt}
        \end{itemize}

    \item Wrap everything up:
    \begin{enumerate}
        \footnotesize
        \item For class $j$ vs. $k$, transform all $(\xv_i, y_i, \cv_i)$ to $(\xv_i, \argmin_{l \in \{j, k\}} \cv_i[l])$ with sample-wise weight $|\cv_i[j] - \cv_i[k]|$.
        \vspace{5pt}
        
        \item Train a binary classifier $f^{(j, k)}$ using the transformed samples.
        \vspace{5pt}
        
        \item Repeat step 1 and 2 for different $(i, j)$.
        \vspace{5pt}
        
        \item Predict using the votes from all $f^{(i, j)}$.
    \end{enumerate}
    \vspace{5pt}

    \item Theoretical guarantee: test of final classifier $\leq 2\sum_{j < k}$ test cost of $f^{(i, j)}$. 
    \end{itemize}
\end{vbframe}


%\begin{vbframe}{MetaCost: Example}
%	%	
%	\small{
%		\begin{itemize}
%			%		
%			\item We compare C4.5 (decision tree) with MetaCost using C4.5 for the \href{http://staffwww.itn.liu.se/~aidvi/courses/06/dm/
%				labs/heart-c.arff}{heart data set}  in \href{ http://www.cs.waikato.ac.nz/ml/weka/}{Weka}. 
%			%		
%			\item The cost matrix $\mathbf{C}$ is 
%			
%			\begin{center}
%				\begin{tabular}{cc|cc}
%					& &\multicolumn{2}{c}{True class} \\
%					& & $y=1$ & $y=-1$  \\
%					\hline
%					\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $0$                & $ 1 $\\
%					& $\hat y$ = -1 & $ 4 $              &  $0$   \\
%				\end{tabular}
%			\end{center}
%		
%			\item The resulting confusion matrices are 
%			
%						
%			\begin{center}
%				\begin{tabular}{cc|cc}
%					& &\multicolumn{2}{c}{True class} \\
%					& MetaCost & $y=1$ & $y=-1$  \\
%					\hline
%					\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $104$                & $ 21 $\\
%					& $\hat y$ = -1 & $ 61 $              &  $117$   \\
%				\end{tabular}
%							\begin{tabular}{cc|cc}
%				& &\multicolumn{2}{c}{True class} \\
%				& C4.5 & $y=1$ & $y=-1$  \\
%				\hline
%				\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $138$                & $ 40 $\\
%				& $\hat y$ = -1 & $ 27$              &  $98$   \\
%			\end{tabular}
%			\end{center}
%		
%		
%			%		
%			\item The total cost of MetaCost is 145, while C4.5 has total costs of 187. However, MetaCost has $0.729$ correct classifications and C4.5 has $0.779.$ 
%			
%			%		
%		\end{itemize}
%		%
%	}
%	%	
%\end{vbframe}



%\begin{vbframe}{Cost-Sensitive Decision Trees}
%	%	
%	\small{
%		\begin{itemize}
%			%		
%			\item 
%			%		
%		\end{itemize}
%		%
%	}
%	%	
%\end{vbframe}


%
\endlecture
\end{document}
