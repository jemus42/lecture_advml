\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\sens}{\mathbf{A}} % vector x (bold)
\newcommand{\ba}{\mathbf{a}}
\newcommand{\batilde}{\tilde{\mathbf{a}}}
\newcommand{\Px}{\mathbb{P}_{x}} % P_x
\newcommand{\Pxj}{\mathbb{P}_{x_j}} % P_{x_j}
\newcommand{\indep}{\perp \!\!\! \perp} % independence symbol
% ml - ROC
\newcommand{\np}{n_{+}} % no. of positive instances
\newcommand{\nn}{n_{-}} % no. of negative instances
\newcommand{\rn}{\pi_{-}} % proportion negative instances
\newcommand{\rp}{\pi_{+}} % proportion negative instances
% true/false pos/neg:
\newcommand{\tp}{\# \text{TP}} % true pos
\newcommand{\fap}{\# \text{FP}} % false pos (fp taken for partial derivs)
\newcommand{\tn}{\# \text{TN}} % true neg
\newcommand{\fan}{\# \text{FN}} % false neg

\usepackage{multicol}

\newcommand{\titlefigure}{figure/f1_score_plot}
\newcommand{\learninggoals}{
  \item Get to know alternative performance measures for accuracy
  \item See their advantages over accuracy for imbalanced data sets
  \item Understand the extensions of these measures for multiclass settings
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Imbalanced Learning: Performance Measures}
\lecture{Advanced Machine Learning}



\sloppy

\begin{vbframe}{Recap: performance measures for binary classification}
    \footnotesize{
        \begin{itemize}
            \item We encourage readers to first go throuth \href{https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-08-measures-classification/}{\beamergotobutton{Chapter 04.08 in I2ML }}.
            \item In binary classification  (i.e., $\Yspace = \{-1,+1\}$):

		\end{itemize}
		
		\begin{center}
		\tiny
		\renewcommand{\arraystretch}{1.1}
		\begin{tabular}{cc||cc|c}
			& & \multicolumn{2}{c|}{\bfseries True Class $y$} & \\
			& & $+$ & $-$ & \\ 
			\hline \hline
			\bfseries Classification     & $+$ & TP & FP & $\rho_{PPV} = \frac{\text{TP}}{\text{TP} + \text{FP}}$\\
			$\yh$ & $-$ & FN & TN & $\rho_{NPV} = \frac{\text{TN}}{\text{FN} + \text{TN}}$\\
			\hline
			& & $\rho_{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}$ & $\rho_{TNR} = \frac{\text{TN}}{\text{FP} + \text{TN}}$ & $\rho_{ACC} = \frac{\text{TP}+ \text{TN}}{\text{TOTAL}}$
		\end{tabular}
		\renewcommand{\arraystretch}{1}
        \end{center}

        \begin{itemize}
            \item $F_1$ score balances Recall ($\rho_{TPR}$) and Precision ($\rho_{PPV}$):
            $$\rho_{F_1} = 2 \cdot \cfrac{\rho_{PPV} \cdot \rho_{TPR}}{\rho_{PPV} + \rho_{TPR}}$$
            
            \item Note $\rho_{F_1}$ does not account for the number of true negatives.
            
            \item Question: does $F_1$ score suffer from data imbalance as accuracy?
        \end{itemize}
            
    }
    
\end{vbframe}


\begin{vbframe}{$F_1$ Score in Binary classification}
	\footnotesize
	
	\begin{minipage}[c]{0.5\textwidth}
		\footnotesize
		$F_1$ score for different combinations of $\rho_{PPV}$ \& $\rho_{TPR}$. \\
		$\rightarrow$ Tends more towards the \textbf{lower} of the two combined values.
	\end{minipage}%
	\begin{minipage}[c]{0.5\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{figure/f1_score_plot.pdf}
	\end{minipage}
	
	\begin{itemize}
		\item A model with $\rho_{TPR} = 0$ (no positive instance predicted as 
		positive) or 
		$\rho_{PPV} = 0$ (no true positives among the predicted) has $\rho_{F_1} = 0$.
  
		\item Always predicting \enquote{negative}: $\rho_{F_1} = 0$
  
		\item Always predicting \enquote{positive}: $\rho_{F_1} = 2 \cdot \rho_{PPV} / 
		(\rho_{PPV} + 1) = 2 \cdot \np / (\np + n)$,\\ 
		$\leadsto$ small when $\np$ is small.

        \item Hence, $F_1$ score is more robust to data imbalance than accuracy.
  
	\end{itemize}
	
\end{vbframe}


\begin{vbframe}{G Score and G Mean}
	\footnotesize
	
	\begin{minipage}[c]{0.5\textwidth}
	\footnotesize
	\begin{itemize}
		\item G score uses geometric mean: 
		%	
		$$\rho_{G} = \sqrt{\rho_{PPV} \cdot \rho_{TPR}}$$
		%	
		\item It tends more towards the \textbf{lower} of the two combined values.
		%	
	\end{itemize}
\end{minipage}%
\begin{minipage}[c]{0.5\textwidth}
	\centering
	\includegraphics[width=0.8\textwidth]{figure/g_score_plot.pdf}
\end{minipage}

\begin{itemize}
	\item Closely related is the G mean:

	$$\rho_{Gm} = \sqrt{\rho_{TNR} \cdot \rho_{TPR}}.$$

	It also considers the \textbf{true negatives}.
	
	\item Always predicting \enquote{negative}: $\rho_{G} = \rho_{Gm}  = 0 \leadsto$ Robust to data imbalance!

\end{itemize}
	
\end{vbframe}



\begin{vbframe}{Balanced Accuracy}
	\footnotesize
	
	\begin{minipage}[c]{0.5\textwidth}
		\footnotesize
		\begin{itemize}
			\item Similar to $G$ mean, Balanced accuracy (BAC) balances $\rho_{TNR}$ and $\rho_{TPR}$: 
			%	
			$$\rho_{BAC} = \frac{\rho_{TNR} + \rho_{TPR}}{2}$$
			%	
			\item  It tends more towards the \textbf{higher} of the two combined values.
			%	
		\end{itemize}
	\end{minipage}%
	\begin{minipage}[c]{0.5\textwidth}
		\centering
		\includegraphics[width=0.7\textwidth]{figure/bac_plot.pdf}
	\end{minipage}
	%
	\begin{itemize}
		\item If a classifier attains high accuracy on both classes or the data set is almost balanced, then $\rho_{BAC} \approx \rho_{ACC}$.
        \vspace{20pt}
        
		\item However, if a classifier always predicts ``negative'' for an imbalanced data set, i.e. $n_+  \ll n_-,$ then $\rho_{BAC} \ll \rho_{ACC}$. It also considers true negatives.

	\end{itemize}
	
\end{vbframe}


\begin{vbframe}{Matthews Correlation Coefficient}
	%	
	\footnotesize{
	\begin{itemize}
		%
		\item MCC (aka Phi coefficient) is a measure for the correlation between two binary random variables. 
        \vspace{20pt}
  
        \item In classification, the ``predicted'' classes and the ``true'' classes are the two discrete random variables:
	
		$$   \rho_{MCC} = \frac{TP\cdot TN - FP \cdot FN}{\sqrt{(TP+FN)(TP+FP)(TN+FN)(TN+FP)}}$$
        \vspace{20pt}

        \item In contrast to other metrics: 
        \begin{itemize}
            \footnotesize
            \item MCC uses all entries of the confusion matrix;
            \item MCC has value in $[-1,1]$.
        \end{itemize}
		
    \end{itemize}
	
	}
\end{vbframe}

\begin{vbframe}{Matthews Correlation Coefficient}
    \footnotesize{
        $$   \rho_{MCC} = \frac{TP\cdot TN - FP \cdot FN}{\sqrt{(TP+FN)(TP+FP)(TN+FN)(TN+FP)}}$$

        \begin{itemize}
            \item $\rho_{MCC} \approx 1$ $\leadsto$ good classification, i.e., strong correlation between predicted and true classes.
            \vspace{10pt}
    	
            \item $\rho_{MCC} \approx 0$ $\leadsto$ no correlation, i.e., the classification is no better than random guessing.
            \vspace{10pt}
    	
            \item $\rho_{MCC} \approx -1$ $\leadsto$ reversed classification, i.e., the classifier is essentially switching the labels.
            \vspace{10pt}
    
            \item For the previous measures, it is important which class is the positive one (especially in light of imbalanced data). But MCC does not depend on which class is the positive one.
            
        \end{itemize}
         
    }
    
\end{vbframe}


\begin{vbframe}{Performance Measures for Multiclass Classification}
	%	
	\footnotesize{
		\begin{center}
			\tiny
			\begin{tabular}{cc|>{\centering\arraybackslash}p{8em}>{\centering\arraybackslash}p{8em}>{\centering\arraybackslash}p{5em}>{\centering\arraybackslash}p{8em}}
				& & \multicolumn{4}{c}{\bfseries True Class $y$} \\
				& & $1$ & $2$ & $\ldots$ & $g$  \\
				\hline
				\bfseries Classification     & $1$ & $n(1,1)$  &  $n(1,2)$  & $\ldots$ &  $n(1,g)$ \\
				& & (True 1's) & (False 1's for 2's) & $\ldots$ &  (False 1's for $g$'s)  \\
				& $2$ &  $n(2,1)$  &  $n(2,2)$  & $\ldots$ & $n(2,g)$  \\
				$\yh$ & & (False 2's for 1's) & (True 2's) & $\ldots$ &  (False 2's for $g$'s)  \\
				& $\vdots$ & $\vdots$ & $\vdots$ & $\ldots$ & $\vdots$ \\
				& $g$ & $n(g,1)$ & $n(g,2)$  & $\ldots$ &  $n(g,g)$\\
				& & (False $g$'s for 1's) & (False $g$'s for 2's) & $\ldots$ &  (True $g$'s)  \\
			\end{tabular}
		\end{center}
		
		\begin{itemize}
			\item $n(i,j)$: the number of $j$ instances classified as $i$.
            \vspace{5pt}

            \item $n_i = \sum_{j=1}^g n(j,i)$ the total number of $i$ instances.
            \vspace{5pt}

            \item \textbf{Class-specific} metrics:
            \begin{itemize}
                \footnotesize 
                
                \item True positive rate (\textbf{Recall}): $\rho_{TPR_i} = \frac{n(i,i)}{n_i}$
                \vspace{5pt}

                \item True negative rate $\rho_{TNR_i} = \frac{\sum_{j\neq i}n(j,j)}{n-n_i}$ 
                \vspace{5pt}

                \item Positive predictive value (\textbf{Precision}) $\rho_{PPR_i} = \frac{n(i,i)}{\sum_{j=1}^g n(i,j)}$.
                
            \end{itemize}

		\end{itemize}
	}
\end{vbframe}


\begin{vbframe}{Macro $F_1$ Score}
	
	\footnotesize{

	\begin{itemize}

		\item To obtain a single metric value, we can average the class-specific metrics over classes:
        $$\rho_{mMETRIC} = \frac{1}{g}\sum_{i=1}^g \rho_{METRIC_i},$$
        where $METRIC_i$ is a class-specific metric such as $PPV_i$, $TPR_i$ of class $i$.
		
	
		\item With this, one can simply define a \textbf{macro} $F_1$ score:
	
		$$\rho_{mF_1} = 2 \cdot \cfrac{\rho_{mPPV} \cdot \rho_{mTPR}}{\rho_{mPPV} + 
			\rho_{mTPR}}$$

		\item Problem: each class gets an equal weight $\leadsto$ the class sizes are not considered.

        \item How about applying different weights to the class-specific metrics?
%
	\end{itemize}
	}
\end{vbframe}

\begin{vbframe}{Weighted Macro $F_1$ Score}

	\footnotesize{

	\begin{itemize}

		\item For imbalanced data sets, give \textbf{more weights} to \textbf{minority} classes.
        \vspace{10pt}
  
        \item $w_1,\ldots,w_g \in[0,1]$  such that $w_i > w_j$ iff $n_i < n_j$ and $\sum_{i=1}^g w_i = 1.$

        $$\rho_{wmMETRIC} = \frac{1}{g}\sum_{i=1}^g \rho_{METRIC_i} w_i,$$
        where $METRIC_i$ is a class-specific metric such as $PPV_i$, $TPR_i$ of class $i$.
        \vspace{10pt}

	 
		\item Example: $w_i = \frac{n - n_i}{(g-1)n}$ are suitable weights.
        \vspace{10pt}

		\item This leads to the weighted macro $F_1$ score:
			
		$\rho_{wmF_1} = 2 \cdot \cfrac{\rho_{wmPPV} \cdot \rho_{wmTPR}}{\rho_{wmPPV} + 
			\rho_{wmTPR}}$
		\vspace{10pt}
  
		\item Following this idea, we also obtain a weighted macro G score or weighted BAC.
        \vspace{10pt}
	
		\item \textbf{Usually} the weighted $F_1$ score is used with weights $w_i = n_i/n$. However, for imbalanced data sets this would \textbf{overweights} majority classes.

	\end{itemize}
	}
\end{vbframe}


\begin{vbframe}{Other Performance Measures}

	\footnotesize{

		\begin{itemize}
		
			\item ``Micro'' versions of metrics for the multiclass setting: e.g. the micro $TPR$ is $\frac{\sum_{i=1}^g TP_i}{\sum_{i=1}^g TP_i + FN_i}$ 
            \vspace{10pt}
		
			\item Moreover, the MCC can be extended to:
			
			$$   \rho_{MCC} = \frac{ n  \sum_{i=1}^g n(i,i) -  \sum_{i=1}^g \hat n_i n_i}{\sqrt{ (n^2 - \sum_{i=1}^g \hat n_i^2)(n^2 - \sum_{i=1}^g n_i^2)  }},$$

			where $\hat n_i = \sum_{j=1}^g n(i,j)$ is the total number of instances classified as $i.$
            \vspace{10pt}
        
		
			\item Other performance measures which treat the "predicted" classes and the "true" classes as two discrete random variables, e.g.\ Cohen's Kappa or Cross Entropy (see Grandini et al.\ (2021)).
		
		\end{itemize}
	}
\end{vbframe}


\begin{vbframe}{Which Performance Measure to use?}

	\small{

		\begin{itemize}

            \item Since different measures focus on different characteristics $\leadsto$ No golden answer to this question.
            \vspace{20pt}
	
			\item Depends on the application, which characteristic is more important than another.
            \vspace{20pt}

			\item However, it is clear that the usage of accuracy is inappropriate if the data set is imbalanced. $\leadsto$ Use altenative ones.
            \vspace{20pt}

			\item Be careful with comparing the absolute values of the different measures, as these can be on different ``scales'', e.g.\ MCC and BAC. 
            \vspace{20pt}
	
		\end{itemize}
	}
\end{vbframe}



%
\endlecture
\end{document}
