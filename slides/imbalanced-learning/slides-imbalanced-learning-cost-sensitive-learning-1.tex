\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

%\usepackage{algorithm}
%\usepackage{algorithmic}

\newcommand{\sens}{\mathbf{A}} % vector x (bold)
\newcommand{\ba}{\mathbf{a}}
\newcommand{\batilde}{\tilde{\mathbf{a}}}
\newcommand{\Px}{\mathbb{P}_{x}} % P_x
\newcommand{\Pxj}{\mathbb{P}_{x_j}} % P_{x_j}
\newcommand{\indep}{\perp \!\!\! \perp} % independence symbol
% ml - ROC
\newcommand{\np}{n_{+}} % no. of positive instances
\newcommand{\nn}{n_{-}} % no. of negative instances
\newcommand{\rn}{\pi_{-}} % proportion negative instances
\newcommand{\rp}{\pi_{+}} % proportion negative instances
% true/false pos/neg:
\newcommand{\tp}{\# \text{TP}} % true pos
\newcommand{\fap}{\# \text{FP}} % false pos (fp taken for partial derivs)
\newcommand{\tn}{\# \text{TN}} % true neg
\newcommand{\fan}{\# \text{FN}} % false neg
\newcommand{\qv}{\mathbf{q}}  % vector q (bold)
\newcommand{\cv}{\mathbf{c}}    % vector c (bold)

\usepackage{multicol}

\newcommand{\titlefigure}{figure/cost_matrix}
\newcommand{\learninggoals}{
  \item Learn cost matrix modelling in cost-sensitive learning
  \item Understand Minimum Expected Cost Principle
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Imbalanced Learning via Cost-Sensitive Learning Part 1}
\lecture{Advanced Machine Learning}



\sloppy


\begin{vbframe}{Cost-Sensitive learning: In a Nutshell}
	%	
	\scriptsize{

		\begin{itemize}
		
    		\item Cost-sensitive learning: 
                \begin{itemize}
                    \scriptsize
                    \item Different (mis-)classification costs are taken into consideration.
                    \item The learner seeks to minimize the total costs in expectation.
                    \item Classical learning assumes that the data sets are balanced, and all errors have the same cost.
                \end{itemize}
    		
    		\item Real-world applications, where different costs between misclassifications are present:
      
    		\begin{itemize}
    			\scriptsize	
    			\item Medicine --- Misdiagnosing a cancer patient as healthy vs.\ misdiagnosing a healthy patient as having cancer (and then check again).
       
    			\item (Extreme) Weather prediction ---  Incorrectly predicting that no hurricane occurs vs.\ predicting a strong wind as a hurricane.
    	
    			\item Credit granting --- Lending to a risky client vs. not lending to a trustworthy client.
    		\end{itemize}
         
		
		\end{itemize}
        \vspace{15pt}

        \begin{minipage}{0.49\textwidth}
            \begin{table}[]
                \centering
                \begin{tabular}{p{1cm}c|cc}
                    & &\multicolumn{2}{c}{Truth} \\
                    & & With Tumor & No Tumor  \\
                    \hline
                    \multirow{2}{*}{\parbox{1cm}{Diagnosis}} & Malignent & 0 & $ 1 $\\
                    & Benign & $10000$ & $0$   \\
                \end{tabular}
            \end{table}
        \end{minipage}
        \hfill
        \begin{minipage}{0.49\textwidth}
            \begin{itemize}
                \scriptsize
                \item In these examples, \textbf{the costs of a false negative is much higher than the costs of a false positive}.
                \vspace{15pt}
                
                \item In some applications, the costs are \textbf{unknown} $\leadsto$ need to be specified by experts, or be learnt.
            \end{itemize}   
        \end{minipage}
		
	}
\end{vbframe}


\begin{vbframe}{Cost matrix}
%	
\scriptsize{
%	
	\begin{itemize}
%		
		\item In cost-sensitive learning we are provided with a cost matrix $\mathbf{C}$ of the form
%		
	\end{itemize}
	%	
	\begin{center}
		\tiny
		\begin{tabular}{cc|>{\centering\arraybackslash}p{8em}>{\centering\arraybackslash}p{8em}>{\centering\arraybackslash}p{5em}>{\centering\arraybackslash}p{8em}}
			& & \multicolumn{4}{c}{\bfseries True Class $y$} \\
			&  & $1$ & $2$ & $\ldots$ & $g$  \\
			\hline
			\bfseries Classification     & $1$ & $C(1,1)$  &  $C(1,2)$  & $\ldots$ &  $C(1,g)$ \\
			& $2$ &  $C(2,1)$  &  $C(2,2)$  & $\ldots$ & $C(2,g)$  \\
            $\yh$ & & & & & \\
			& $\vdots$ & $\vdots$ & $\vdots$ & $\ldots$ & $\vdots$ \\
			& $g$ & $C(g,1)$ & $C(g,2)$  & $\ldots$ &  $C(g,g)$\\
		\end{tabular}
	\end{center}
	%	
	\begin{itemize}
		%		
		\item $C(i,j)$ is the cost of classifying $j$ as $i,$ which in the cost-insensitive learning case is simply $C(i,j) = \mathds{1}_{[ i \neq j ]},$ i.e., each misclassification has the same cost of 1.
		\vspace{10pt}
		
		\item Cost matrix is usually designed by experts with domain knowledge.
        \vspace{10pt}
		\begin{enumerate}
			\scriptsize	
            \item Too low costs: might not change the decision boundaries significantly, leading to (still) costly predictions.
            \vspace{10pt}
            \item Too high costs: might harm the generalization capability of the classifier on costly classes.
            \vspace{10pt}
		\end{enumerate}
		
	\end{itemize}

}
\end{vbframe}


\begin{vbframe}{Cost matrix for Imbalanced Learning}
	\footnotesize{
		\begin{itemize}			
	 
			\item A common heuristic for imbalanced data sets is to use 
	
			\begin{itemize}
				\footnotesize 
                \item $C(i,j) = \frac{n_i}{n_j}$ for classes $i$ and $j$ such that $n_j  \ll n_i$, i.e. for misclassifying a minority class $j$ as a majority class $i$.
                \vspace{10pt}
		
				\item $C(i,j) = 1$ for classes $i$ and $j$ such that $n_i \ll n_j$, i.e. for misclassifying a majority class $j$ as a minority class $i$.
                \vspace{10pt}
			
				\item 0 for a correct classification (a theoretical reason will be explained later).
                \vspace{10pt}
			
			\end{itemize}

    		\item In an imbalanced binary classification we get: \\
            \begin{table}[]
                \centering
                \begin{tabular}{cc|cc}
                    & &\multicolumn{2}{c}{True class} \\
                    & & $y=1$ & $y=-1$  \\
                    \hline
                    \multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $0$                & $ 1 $\\
                    & $\hat y$ = -1 & $ \frac{n_-}{n_+} $              &  $0$   \\
                \end{tabular}
            \end{table}
    		

            \item Thus, this heuristic is consistent with the general real case that the cost of false negatives is much higher than the cost of false positives.	
        \end{itemize}
		
	}
\end{vbframe}


\begin{vbframe}{Minimum expected Cost Principle (MECP)}

	\footnotesize{


		\begin{itemize}\footnotesize
		
			\item Suppose we have:
            \begin{itemize}
                \footnotesize
                \item a cost matrix $\mathbf{C}$,
                \vspace{10pt}
                
                \item knowledge of the true posterior distribution $p(\cdot ~|~ \xv).$
                \vspace{10pt}
            \end{itemize}

            \item Idea for classifying a given feature $\xv$: 
            \begin{itemize}
                \footnotesize
                \item Compute the cost for classifying $\xv$ as $i$. (infeasible to directly compute.)
                \vspace{10pt}
                
                \item $\leadsto$ Marginalize over ``true'' class $j$.
                \vspace{10pt}
            \end{itemize}

			\item \emph{Minimum expected cost principle}: Use the class for prediction with the smallest expected costs, where the expected costs of a class $i\in\{1,\ldots,g\}$ is
	
			$$ 	\E_{J \sim p(\cdot ~|~ \xv)}( C(i,J) ) = \sum_{j=1}^g 	p(j ~|~ \xv) C(i,j).	$$

			
				
		\end{itemize}
	}
\end{vbframe}

\begin{vbframe}{MECP}
    \footnotesize
    \begin{itemize}
        \item Thus, if we have a classifier $f$ which uses a probabilistic score function $\pi:\Xspace \to [0,1]^g$ with $\pi(\xv) = (\pi(\xv)_1,\ldots,\pi(\xv)_g)^\top$ and $\sum_{j=1}^g \pi(\xv)_j = 1$ for the classification, then one can easily modify $h$ to take the expected costs into account:
		$$  \tilde h(\xv) = \argmin_{i=1,\ldots,g} \sum_{j=1}^g 	\pi(\xv)_j C(i,j). $$
        \vspace{10pt}

        \item For $\xv$, making prediction $i$ means \textbf{acting as if $i$ is the true class of $\xv$.}
        \vspace{10pt}
        
        \item The essence of cost-sensitive decision-making: it can be optimal to act as if class $i$ is true even when $j$ is more probable. (\href{https://dl.acm.org/doi/10.5555/1642194.1642224}{\beamergotobutton{Elkan et. al. 2001}})
        \vspace{10pt}

        \item Now consider: 
        \begin{itemize}
            \footnotesize
            \item If we learnt a binary classifier: $p(1 ~|~ \xv) = \pi(\xv)_1$ and $p(-1 ~|~ \xv) = \pi(\xv)_2$,
            \item Under what condition should we predict $\xv$ as class $1$?
        \end{itemize}
    \end{itemize}
\end{vbframe}


\begin{vbframe}{MECP: Binary Case}
	%	
	\footnotesize{
		%		
		\begin{itemize}
            \footnotesize
            \item Preliminary knowledge: the optimal decisions do NOT change if 
            \begin{itemize}
                \footnotesize
                \item $\mathbf{C}$ is multipled by a positive constant.
                \item $\mathbf{C}$ is added with a constant shift.
            \end{itemize}

            \item Hence, we scale and shift $\mathbf{C}$ to eliminate some entries, obtaining a simpler $\mathbf{C}^\prime$: 
            \begin{table}[]
                \centering
                    \begin{tabular}{cc|cc}
        			& &\multicolumn{2}{c}{True class} \\
        			& & $y=1$ & $y=-1$  \\
        			\hline
        			\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}} & $\hat y$ = 1 & $C^\prime(1,1)$ & $1$ \\
        			& $\hat y$ = -1 & $C^\prime(-1, 1)$ & 0\\
                \end{tabular}
            \end{table}
            where 
            \begin{itemize}
                \footnotesize
                \item $C^\prime (-1, 1) = [C(-1, 1) - C(-1, -1)] / [C(1, -1) - C(-1, -1)]$,
                \item $C^\prime (1, 1) = [C(1, 1) - C(-1, -1)] / [C(1, -1) - C(-1,-1)]$.
            \end{itemize}

            \item We predict $\xv$ as class $1$ if \\
            \begin{align*}
                \E_{J \sim p(\cdot ~|~ \xv)}( C^\prime(1,J) )  \leq \E_{J \sim p(\cdot ~|~ \xv)}( C^\prime(-1,J) ) \\
            \end{align*}

			\item Plugging $\mathbf{C}^\prime$ into the inequality above, we obtain:	
			\begin{align*}	
                & p(-1 ~|~ \xv ) C^\prime(1,-1)  + 	p(1 ~|~ \xv ) C^\prime(1,1) \leq  p(-1 ~|~ \xv ) C^\prime(-1,-1)  + 	p(1 ~|~ \xv ) C^\prime(-1,1)  \\ 
                &\Rightarrow [1 - p(1 ~|~ \xv)] \cdot 1 + p(1 ~|~ \xv) C^\prime(1, 1) \leq p(1 ~|~ \xv) C^\prime(-1, 1) \\
                &\Rightarrow p(1 ~|~ \xv) \geq \frac{1}{C^\prime(-1, 1) - C^\prime(1, 1) + 1} \\
                &\Rightarrow p(1 ~|~ \xv) \geq \frac{C(1, -1) - C(-1, -1)}{C(-1, 1) - C(1, 1) + C(1, -1) - C(-1, -1)}
			\end{align*}
		
            \item By further assuming that $C(1, 1) = C(-1, -1) = 0 $, this can by simplied to 
            \begin{align*}
                p(1 ~|~ \xv) \geq \frac{C(1, -1)}{C(-1, 1) + C(1, -1)} = c^{*}
            \end{align*}	
            
            \item This yields the optimal threshold value $c^*$ for probabilistic score classifiers, so that any probabilistic classifier $h$ using a probabilistic score $\pi:\Xspace \to [0,1]$ can be modified to 
            
            $$   \tilde h(\xv) = 2 \cdot \mathds{1}_{[ \pi(\xv) \geq c^*]} -1. $$
							
		\end{itemize}
	}
\end{vbframe}



%\begin{vbframe}{MetaCost: Example}
%	%	
%	\small{
%		\begin{itemize}
%			%		
%			\item We compare C4.5 (decision tree) with MetaCost using C4.5 for the \href{http://staffwww.itn.liu.se/~aidvi/courses/06/dm/
%				labs/heart-c.arff}{heart data set}  in \href{ http://www.cs.waikato.ac.nz/ml/weka/}{Weka}. 
%			%		
%			\item The cost matrix $\mathbf{C}$ is 
%			
%			\begin{center}
%				\begin{tabular}{cc|cc}
%					& &\multicolumn{2}{c}{True class} \\
%					& & $y=1$ & $y=-1$  \\
%					\hline
%					\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $0$                & $ 1 $\\
%					& $\hat y$ = -1 & $ 4 $              &  $0$   \\
%				\end{tabular}
%			\end{center}
%		
%			\item The resulting confusion matrices are 
%			
%						
%			\begin{center}
%				\begin{tabular}{cc|cc}
%					& &\multicolumn{2}{c}{True class} \\
%					& MetaCost & $y=1$ & $y=-1$  \\
%					\hline
%					\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $104$                & $ 21 $\\
%					& $\hat y$ = -1 & $ 61 $              &  $117$   \\
%				\end{tabular}
%							\begin{tabular}{cc|cc}
%				& &\multicolumn{2}{c}{True class} \\
%				& C4.5 & $y=1$ & $y=-1$  \\
%				\hline
%				\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $138$                & $ 40 $\\
%				& $\hat y$ = -1 & $ 27$              &  $98$   \\
%			\end{tabular}
%			\end{center}
%		
%		
%			%		
%			\item The total cost of MetaCost is 145, while C4.5 has total costs of 187. However, MetaCost has $0.729$ correct classifications and C4.5 has $0.779.$ 
%			
%			%		
%		\end{itemize}
%		%
%	}
%	%	
%\end{vbframe}



%\begin{vbframe}{Cost-Sensitive Decision Trees}
%	%	
%	\small{
%		\begin{itemize}
%			%		
%			\item 
%			%		
%		\end{itemize}
%		%
%	}
%	%	
%\end{vbframe}


%
\endlecture
\end{document}
