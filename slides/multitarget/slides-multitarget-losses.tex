\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\sens}{\mathbf{A}} % vector x (bold)
\newcommand{\ba}{\mathbf{a}}
\newcommand{\batilde}{\tilde{\mathbf{a}}}
\newcommand{\Px}{\mathbb{P}_{x}} % P_x
\newcommand{\Pxj}{\mathbb{P}_{x_j}} % P_{x_j}
\newcommand{\indep}{\perp \!\!\! \perp} % independence symbol
% ml - ROC
\newcommand{\np}{n_{+}} % no. of positive instances
\newcommand{\nn}{n_{-}} % no. of negative instances
\newcommand{\rn}{\pi_{-}} % proportion negative instances
\newcommand{\rp}{\pi_{+}} % proportion negative instances
% true/false pos/neg:
\newcommand{\tp}{\# \text{TP}} % true pos
\newcommand{\fap}{\# \text{FP}} % false pos (fp taken for partial derivs)
\newcommand{\tn}{\# \text{TN}} % true neg
\newcommand{\fan}{\# \text{FN}} % false neg

\newcommand{\Tspace}{\mathcal{T}}
\newcommand{\tv}{\mathbf{t}}
\newcommand{\tj}{\mathbf{t}_j}

\usepackage{multicol}
\usepackage{color,colortbl} 
\definecolor{putblue}{RGB}{0,0,124}
\definecolor{putred}{RGB}{204,33,69}

\newcommand{\titlefigure}{figure/fmeasure}
\newcommand{\learninggoals}{
  \item Get to know loss functions for multi-target prediction problems
  \item Know the Bayes predictor for Hamming loss and subset $0/1$ loss
  \item Understand the difference between macro-, micro-, and instance-wise-losses 
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Loss Functions for Multi-Target Prediction}
\lecture{Advanced Machine Learning}



\sloppy


\begin{frame}
	\frametitle{Multivariate Loss Functions}
	\begin{itemize}
%		
		\small
%		
		\item In multi-target prediction: For a feature vector $\xv$, predict a vector of scores $f(\xv) = (f(x)_1, f(x)_2, \ldots, f(x)_l)^\top$ for $l$ tasks with a function (hypothesis) $f: \Xspace \rightarrow \R^{g_1} \times \cdots \times \R^{g_l} $.
        \vspace{15pt}
%		
		\item If we want to follow the machine learning paradigm based on loss minimization, we need a \emph{multivariate loss functions} 
		$$
		L: \, (\Yspace_1 \times \cdots \times \Yspace_l) \times (\R^{g_1} \times \cdots \times \R^{g_l}) \rightarrow \mathbb{R}.
		$$ 
        \vspace{15pt}

        \item In multi-target regression: $\Yspace_1 = \ldots = \Yspace_l = \R$, and $g_1 = \ldots = g_l = 1$.
        \vspace{15pt}

        \item In multi-label classification: $\Yspace_1 =  \ldots = \Yspace_l = \{0, 1\}$, and $g_1 = \ldots = g_l = 1$. \\ $\leadsto$ I.e., each task is a binary classification.

%				
%		\item Can we achieve this goal through simple reduction, i.e., by training one model for each target independently? Or can we do better with more sophisticated methods?  
%		
%		\item There are \emph{two views:} the individual target and joint target view.
%
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Multivariate loss functions}
	\small
	\begin{itemize}
%		
%		\item 
%
		\item A loss $L$ (on test data) is decomposable over examples if it can be written in the form
		$$
		L = \sum_{i=1}^n L(\yv^{(i)}, f(\xi)) 
		%\quad \text{vs.} \quad L \ne \sum_{i=1}^n \ell(\yv^{(i)}, f(\xi)) 
		\, ,
		$$
		i.e., as a sum of losses over all (test) examples. 
		
		
		\item A multivariate loss $L$ is decomposable over targets if it can be written as
		$$
		L(\yv, f) = \sum_{m=1}^l L_m(y_m, f(\xv)_m) 
		$$
		with suitable single-target losses $L_m$. 
		
%		

		\begin{minipage}{0.45\textwidth}
%			
			\item  In general, we distinguish between two categories of losses: micro- and instance-wise-losses. 
%			
		\end{minipage}
%	
		\begin{minipage}{0.45\textwidth}
			\begin{center}
				\includegraphics[width=0.9\textwidth]{figure/fmeasure}
			\end{center}
		\end{minipage}
%		
	\end{itemize}
%
\end{frame}




\begin{frame}
	\frametitle{Micro Losses}
	\begin{itemize}
		

            \item Micro-losses: The overall loss corresponds to aggregating the pointwise losses over the targets and the instances.
                    \vspace{5pt}

            \item Thus, we have	
	$$
	L =  \frac{1}{n \cdot l} \sum_{i,m} L(y_m^{(i)} , f(\xv)_m^{(i)}),
	$$
	where $L: \Yspace_m \times \R^{g_l} \to \R$ in this case.
                    \vspace{5pt}            
		\item Example \emph{Squared error loss} (i.e. used in multivariate regression):
		$$
		L(\yv, f) = \sum_{m=1}^l (y_m - f(\xv)_m)^2.
		$$
            \item Can also be used for cases with missing entries.

		
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Instance-wise Losses}
	\begin{itemize}
		
            \item Inputs one label vector $\yv$ and computes the loss with respect to the instance-wise predictions.
                    \vspace{5pt}            

		\item The \emph{Hamming loss} averages over mistakes on individual scores:    
		$$
		\displaystyle L_H(\yv, f) = \frac{1}{l}  \, \sum_{m=1}^l \, \mathds{1}_{[y_m \neq h_m(x)]},
		$$
        where $h_m(x) := [f(\xv)_m \geq c_m]$ is the threshold function for task $m$ with threshold $c_m$.
        \vspace{5pt}
		
		\item The \emph{subset 0/1 loss} simply checks for entire correctness:  
		\begin{align*}
			\displaystyle L_{0/1}(\yv, f) & = \mathds{1}_{[ \yv \neq \mathbf{h}(x) ]}  =  \max_m \, \mathds{1}_{[y_m \neq  h_m(x)]}
		\end{align*}
		
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Hamming vs.\ subset 0/1 loss}
	\begin{itemize}
		\item The risk minimizer for the Hamming loss is the  \emph{marginal mode}:
		$$
		f^*(\xv)_m = \arg \max_{y_m \in \{0,1\}} \Pr( y_j  ~|~ \xv )\,, \quad m = 1,\ldots,l,
		$$
		%\item 
		while for the subset 0/1 loss it is the \emph{joint mode}:
		$$
		f^*(\xv) = \arg \max_{\yv} \Pr(\yv ~|~ \xv) \,.
		$$
		\item Marginal mode vs. joint mode:\\[6pt]
		\begin{center}
			\begin{tabular}{@{}cc@{}}
				\toprule
				$\yv$ & $\Pr(\yv)$ \\
				\hline
				$0~0~0~0$ & $0.30$ \\
				$0~1~1~1$ & $0.17$ \\
				$1~0~1~1$ & $0.18$ \\
				$1~1~0~1$ & $0.17$ \\
				$1~1~1~0$ & $0.18$ \\
				\toprule
			\end{tabular}
			$\qquad$
			\footnotesize{
				\begin{tabular}{lr}
					Marginal mode: & $1~1~1~1$ \\
					Joint mode: & $0~0~0~0$ \\
				\end{tabular}
			}
		\end{center}
	\end{itemize}
\end{frame}



%\begin{frame}
%	\frametitle{The individual target view}
%	
%	\begin{itemize} 
%		\item How can we improve the predictive accuracy of a single label \yv exploiting information about other labels?
%		\item Goal: predict a value of $y_i$ using $\xv$ and any available information on other targets $y_j$.
%		\item The problem is usually defined through univariate losses $\ell_i(y_i, \hat{y}_i)$.
%		%\item The problem is usually decomposable over the targets.
%		\item  Domain of $y_i$ is either continuous or nominal.
%		\item  Independent models vs.\ regularized (shrunken) models.
%%		\item James-Stein paradox (to be discussed later).
%		
%	\end{itemize}
%	
%\end{frame}
%
%
%
%\begin{frame}
%	\frametitle{The joint target view}
%	
%	\begin{itemize}
%		\item The problem is defined through multivariate losses $\ell(\yv, \yh)$.
%		%\item What are examples of such loss functions ?
%		\item Is reduction to single-target prediction (decomposition over targets) still possible, and even if so, can we improve over such strategies \yv using more expressive models?
%		%\item What are the relations between the losses?
%		%\item Goal: predict a vector $\yv$ using $\xv$.
%		\item Important: \emph{Structure of loss} $\ell(\cdot, \cdot)$, possible \emph{dependencies between targets}, multivariate distribution of $\yv$.
%		
%		%\item The problem might not be easily decomposable over the targets.
%		%\item Domain of $\yv$ is usually finite, but contains a large number of elements.
%		%\item Independent models vs.\ more expressive models.
%		%\item Loss function perspective.
%		
%	\end{itemize}
%\end{frame}



%
\endlecture
\end{document}
