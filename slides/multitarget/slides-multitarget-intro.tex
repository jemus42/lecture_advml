\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\sens}{\mathbf{A}} % vector x (bold)
\newcommand{\ba}{\mathbf{a}}
\newcommand{\batilde}{\tilde{\mathbf{a}}}
\newcommand{\Px}{\mathbb{P}_{x}} % P_x
\newcommand{\Pxj}{\mathbb{P}_{x_j}} % P_{x_j}
\newcommand{\indep}{\perp \!\!\! \perp} % independence symbol
% ml - ROC
\newcommand{\np}{n_{+}} % no. of positive instances
\newcommand{\nn}{n_{-}} % no. of negative instances
\newcommand{\rn}{\pi_{-}} % proportion negative instances
\newcommand{\rp}{\pi_{+}} % proportion negative instances
% true/false pos/neg:
\newcommand{\tp}{\# \text{TP}} % true pos
\newcommand{\fap}{\# \text{FP}} % false pos (fp taken for partial derivs)
\newcommand{\tn}{\# \text{TN}} % true neg
\newcommand{\fan}{\# \text{FN}} % false neg

\newcommand{\Tspace}{\mathcal{T}}
\newcommand{\tv}{\mathbf{t}}
\newcommand{\tij}{\mathbf{t}^{(i)}_j}

\usepackage{multicol}

\newcommand{\titlefigure}{figure/Slide2}
\newcommand{\learninggoals}{
  \item Understand the practical relevance of multi-target prediction problems
  \item Know the relevant special cases of multi-target prediction
  \item Understand the difference between inductive and transductive learning problems
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Introduction to Multi-Target Prediction}
\lecture{Advanced Machine Learning}



\sloppy

\begin{vbframe}{Multi-target prediction: Motivation}
\scriptsize{
    \begin{itemize}    
        \item Conventional supervised learning: Label/Outcome space $\Yspace$ is one-dimensional.	
        \begin{minipage}{0.45\textwidth}    

            \item Multi-target prediction (MTP) problems: multiple target variables of possibly different type (binary, nominal, ordinal, real-valued).
            \vspace{10pt}
            
            \item Why not just \emph{reducing} an MTP problem to learning one model per target, independently of the other targets? 
		\end{minipage}
        \hfill
		\begin{minipage}{0.45\textwidth}    
			\begin{center}

                \includegraphics[width=0.9\textwidth]{figure/image_annotation} 		\tiny
                \\ Iliadis et al. (2021), Multi-target prediction for dummies using
                two-branch neural networks (\href{https://arxiv.org/abs/2104.09967}{\underline{URL}}).
   	
			\end{center}
		\end{minipage}

		\item $\leadsto$ The targets can be more or less \emph{similar} to each other and exhibit \emph{statistical dependencies}.
	    \vspace{10pt}

        \item TODO add visualization example.
        \vspace{10pt}
	
		\item Consequently, there is hope to achieve better performance by tackling the targets \emph{simultaneously}.

    \end{itemize}}

\end{vbframe}

\begin{frame}{Multi-target prediction: Characteristics}

	\small
		A multi-target prediction setting is characterized by instances $\xv \in \Xspace$ and targets $\tv \in \Tspace$ with the following properties: 

		\begin{itemize} \small
	
			\item A training data set $\D$ consists of $(\xi,\tij,y_{ij})$, where $y_{ij} \in \Yspace$ denotes a score that characterizes the relationship between the instance $\xi \in \Xspace$ and the target $\tij \in \Tspace$.  
			
			\item $n$ instances and $l$ targets are observed during training. Thus, the scores $y_{ij}$ can be arranged in an $n \times l$ matrix $Y$. Note $Y$ may have missing values.
	
			\item The score set $\Yspace$ is one-dimensional and can be nominal, ordinal or real-valued.  
		
			\item The goal is to predict scores for any pair $(\xv,\tv) \in \Xspace \times \Tspace$.  
            \vspace{10pt}
		
		\end{itemize}
% 
%
In the conventional MTP setting there is no side information for targets available. 
%	
%Note that we abuse here the notation, as we use $\Yspace$ to denote the score set. 
%
\end{frame}

\section{Special Cases of Multi-target Prediction}

\begin{frame}{Multivariate regression}
	\small
%		
		\begin{enumerate}\small
%			
			\item $|\Tspace|=l$ $\leadsto$ all targets are observed during training. 
	
			\begin{minipage}{0.45\textwidth}    
			
    			\item The score set is $\Yspace = \mathbb{R}$. 
                \vspace{10pt}
    
                \item No side information is available for targets. Without loss of generality, we can hence assign the numbers $1$ to $l$ as identifiers to targets, such that the target space is $\Tspace = \{1,...,l\}$. 
            \end{minipage}
            \hfill
			\begin{minipage}{0.45\textwidth}    
				\begin{center} 	
				\includegraphics[width=0.99\textwidth,trim = 0 0 100 100,clip]{figure/Slide1} 	\tiny
				\\ Waegeman et al. (2019), Multi-target prediction:
				A unifying view on problems and methods (\href{https://arxiv.org/pdf/1809.02352.pdf}{\underline{URL}}).
  	
				\end{center}
			\end{minipage}
		\end{enumerate}
  
    \vspace{10pt}
	Example: Predict whether a protein (rows) will bind to a set of experimentally developed small molecules (columns).
%
\end{frame}



\begin{frame}{Multi-label classification}
	\small
	\begin{itemize}
		\small 

        \begin{minipage}{0.45\textwidth}  
            \item $|\Tspace|=l$ $\leadsto$ all targets are observed during training.
            \vspace{10pt}
            
            \item No side information is available for targets. Again, we can identify targets with natural numbers, such that the target space is $\Tspace = \{1,...,l\}$. 
            \vspace{10pt}
            
			\item The score matrix $Y$ has no missing values. 	
            \vspace{10pt}
			\item The score set is $\Yspace = \{0,1\}$.	
		\end{minipage}
        \hfill
		\begin{minipage}{0.45\textwidth}    
		\begin{center}
			\includegraphics[width=0.9\textwidth,trim = 0 0 100 100,clip]{figure/Slide2} \tiny
			\\ Waegeman et al. (2019), Multi-target prediction:
			A unifying view on problems and methods (\href{https://arxiv.org/pdf/1809.02352.pdf}{\underline{URL}}).
		\end{center}
		\end{minipage}
	\end{itemize}	

    \vspace{10pt}
	Example: Assign for documents (rows) the appropriate category tags (columns).

\end{frame}


\begin{frame}{Label ranking}
	\small
    In \emph{label ranking}, each instance is associated with a ranking of the targets. 

		\begin{enumerate}\small
			
			\begin{minipage}{0.45\textwidth}    
                \item $|\Tspace| = l$ $\leadsto$ all targets are observed during training. 
                \vspace{10pt}

    			\item No side information is available for targets. Again, we identify targets with natural numbers, such that the target space is $\Tspace = \{1,...,l\}$. 		
                \vspace{10pt}
                
    			\item $Y$ has no missing values. 
                \vspace{10pt}
                
    			\item The score set $\Yspace = \{1, \ldots , l\}$, and the scores (i.e. ranks) $y_{ij} \neq y_{ik}$ for all $1 \leq j,k \neq l$. 
                \vspace{10pt}
			\end{minipage}
            \hfill
			\begin{minipage}{0.45\textwidth}    
			\begin{center}
                \includegraphics[width=0.9\textwidth,trim = 0 0 100 0,clip]{figure/labelranking} \tiny
				\\ Waegeman et al. (2019), Multi-target prediction:
				A unifying view on problems and methods (\href{https://arxiv.org/pdf/1809.02352.pdf}{\underline{URL}}).
 	
			\end{center}
		\end{minipage}
		\end{enumerate}
  
    \vspace{10pt}
	Example: Predict for users (rows) their preferences over specific activities (columns).
	
\end{frame}


\begin{frame}{Multi-task learning}

		\begin{itemize}\small
			
			\begin{minipage}{0.45\textwidth}  

                \item \textbf{Not all targets are relavent for all instances}. E.g. a student may only attend one school, other labels are unknown and irrelavent.
                \vspace{10pt}
                
                \item $|\Tspace|= l$ $\leadsto$ all targets are observed during training.
                \vspace{10pt}

			    \item No side information is available for targets. Again, the target space can hence be taken as $\Tspace = \{1,...,l\}$. 
                \vspace{10pt}
       
				\item The score set is homogenous across columns of $Y$, e.g., $\Yspace = \{0,1\}$ or $\Yspace = \mathbb{R}$.
                \vspace{10pt}

			\end{minipage}
            \hfill
			\begin{minipage}{0.45\textwidth}    
				\begin{center} 	
					\includegraphics[width=0.99\textwidth,trim = 0 0 100 100,clip]{figure/Slide3} \tiny
					\\ Waegeman et al. (2019), Multi-target prediction:
					A unifying view on problems and methods (\href{https://arxiv.org/pdf/1809.02352.pdf}{\underline{URL}}).	
				\end{center}
			\end{minipage}
		\end{itemize}
    \vspace{10pt}
	Example: Predict for students (rows) the final grades for a specific high-school course (columns).


\end{frame}


\section{Learning with Side Information on Targets}


\begin{vbframe}{Side information on targets}
    \small
    \begin{itemize}
        \item In some practical applications additional side information about the target space is available.
        \item Examples: 
        \begin{itemize} \small
            \begin{minipage}{0.45\textwidth}    
                \item Extra representation for the target molecules in drug design application (\emph{structured representation}).
            \end{minipage}
            \hfill
            \begin{minipage}{0.4\textwidth}    
                \begin{center}
                    \includegraphics[width=0.9\textwidth,trim = 0 0 50 80,clip]{figure/Slide4}	
                \end{center}
            \end{minipage}
            
            \begin{minipage}{0.45\textwidth}   
                \item Taxonomy on document categories (\emph{hierarchy}).
            \end{minipage}
            \hfill
            \begin{minipage}{0.4\textwidth}    
                \begin{center}  	
                    \includegraphics[width=0.9\textwidth,trim = 0 0 100 20,clip]{figure/Slide5}	
                \end{center}
            \end{minipage}
            
            \begin{minipage}{0.45\textwidth}    
                \item Information about schools and courses (geographical location, reputation of the school, etc.) in student mark forecasting application (\emph{feature representation}).
            \end{minipage}
            \hfill
            \begin{minipage}{0.4\textwidth}    
                \begin{center}	                                     
                    \includegraphics[width=0.9\textwidth,trim = 0 0 100 20,clip]{figure/Slide6} \tiny
                    \\ Waegeman et al. (2019), Multi-target prediction: A unifying view on problems and methods (\href{https://arxiv.org/pdf/1809.02352.pdf}{\underline{URL}}).	
                \end{center}
            \end{minipage}
        \end{itemize}
        
        \footnotesize
        
        \item Such problems are often referred to as dyadic prediction, link prediction, etc.
        
        \item Generally speaking, such settings cover problems that obey the four properties listed in the MTP definition.
        
        \item Scores $y_{ij}$ can be arranged in a matrix $Y$, which is often sparse. 
        
        \item Thus, \emph{dyadic prediction} can be seen as \emph{multi-target prediction with target features}. 
    \end{itemize}
\end{vbframe}


\begin{frame}{Inductive vs.\ Transductive Learning}
	\footnotesize
	\begin{itemize} 
		\item In all of the previous problems, 
		\begin{enumerate} \footnotesize
			\item predictions need be be generated for novel instances, 
            \vspace{10pt}
			\item targets are known beforehand and observed during training.
		\end{enumerate}
		\item These problems are \emph{inductive} w.r.t.\ instances (1) and \emph{transductive} w.r.t.\ targets (2).
        \vspace{20pt}


		\begin{minipage}{0.45\textwidth}    
			\item 	\emph{Side information} is important for generalizing to novel targets that are unobserved during training e.g.
            \begin{itemize}
                \footnotesize
                \item a novel target molecule in the drug design,
                \item a novel tag in the document annotation,
                \item a novel school in the student grading.
            \end{itemize} 
		\end{minipage}
        \hfill
		\begin{minipage}{0.45\textwidth}    
			\begin{center}
			
				\tiny{$$\qquad g(.,.): \mbox{target similarity}$$}
				\includegraphics[width=0.9\textwidth,trim = 0 0 0 50,clip]{figure/Slide7}  \tiny
				\\ Waegeman et al. (2019), Multi-target prediction:
				A unifying view on problems and methods (\href{https://arxiv.org/pdf/1809.02352.pdf}{\underline{URL}}).
	
			\end{center}
		\end{minipage}
	\end{itemize}
\end{frame}

\begin{frame}{Subdivision of different learning settings}

	\footnotesize
	One can distinguish between four different learning settings:

	\begin{itemize} \footnotesize
	
		\item Setting A --- transductive w.r.t. targets and the instances. The goal is to predict missing values of the score matrix (\emph{matrix completion problem}).
        \vspace{10pt}

		\item Setting B --- transductive w.r.t.\ the targets and inductive w.r.t. the instances.
        \vspace{10pt}

	\end{itemize}

	\begin{minipage}{0.55\textwidth}
		\begin{itemize} \footnotesize
			\item Setting C --- inductive w.r.t.\ the targets and transductive w.r.t. the instances. $\leadsto$ Some targets are not observed during training but may appear at prediction time.
            \vspace{10pt}

			\item Setting D --- inductive w.r.t.\ both the targets and the instances (\emph{zero-shot learning}).
            \vspace{10pt}
            
		\end{itemize}
	\end{minipage}
    \hfill
	\begin{minipage}{0.4\textwidth}
	   \center
	   \includegraphics[width=0.99\textwidth,trim = 0 0 50 0,clip]{figure/Slide16}  \tiny
	   \\ Waegeman et al. (2019), Multi-target prediction: A unifying view on problems and methods (\href{https://arxiv.org/pdf/1809.02352.pdf}{\underline{URL}}).
	\end{minipage}
\end{frame}



%
\endlecture
\end{document}
